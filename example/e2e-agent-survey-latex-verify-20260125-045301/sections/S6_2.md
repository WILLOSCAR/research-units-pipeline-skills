Tool use turns language models into systems that can act on external resources, which expands the threat model from "unsafe text" to end-to-end tool-chain behavior. A central tension is that interface contracts therefore define both the attack surface and the monitoring leverage available to defenders [@Zhang2025Security; @Fu2025Eval].

A practical entry point is a threat taxonomy. Benchmarks that systematize attack categories (e.g., prompt injection embedded in tool descriptions, out-of-scope parameter requests, tool-transfer exploits) illustrate that many failures arise from mismatched assumptions about who controls tool metadata, what inputs are trusted, and how tool errors are handled [@Zhang2025Security; @Zhang2024Agent].

This taxonomy lens also clarifies where defenses should intervene. Some failures are model-centric (e.g., susceptibility to instruction hijacking), whereas others are system-centric (e.g., unsafe parameter binding, privilege escalation through tool permissions, or replayable tool-trace abuse). This highlights that, as a result, evaluations can conflate "model safety" with "system safety" and obscure which layer actually needs redesign [@Zhang2025Security; @Gasmi2025Bridging].

Security benchmarks also emphasize scale and protocol realism. Suites that map large numbers of attack tasks to common weakness categories and implement tools in multiple formats make it harder to dismiss failures as contrived edge cases, and they stress the full pipeline rather than isolated model prompts [@Fu2025Eval; @Zhang2025Security]. Reported suites quantify this realism by enumerating large attack sets (e.g., thousands of attack tasks across dozens of test cases) and by measuring not only attack success, but also how attacks degrade task completion under the same nominal agent scaffold [@Fu2025Eval]. Complementary MCP-specific suites organize attacks at the interface layer (e.g., tool-description prompt injection, tool-transfer, and mixed strategies), which helps link failures back to concrete contract assumptions rather than to generic "jailbreak" narratives [@Zhang2025Security].

Monitoring and red-teaming workflows highlight a second constraint: defenses are themselves protocol-dependent. If the monitor lacks situational awareness, or if the adversary can exploit tool outputs and intermediate traces, then guardrails that look strong in controlled settings can fail in deployment-like loops [@Kale2025Reliable; @Fu2025Eval].

Architecture choices can measurably shift which vulnerabilities dominate. Comparative evaluations of function-calling style tool mediation versus protocol-based tool interfaces suggest that different tool mediation layers can change attack success rates and the balance between system-centric and model-centric exposures, especially under chained attacks [@Gasmi2025Bridging; @Zhang2025Security].

Behavioral safety issues also include seemingly "benign" failure modes such as quitting or refusal behaviors that can be triggered by adversarial prompts or tool failures. Systematic evaluation of such behaviors matters because they can interact with recovery policies and make long-horizon agents brittle even when single-step safety checks pass [@Bonagiri2025Check; @Engelberg2026Sola].

Governance and enforcement mechanisms often reduce to permissions and policy compliance at the interface layer: what the agent is allowed to call, what parameters are admissible, and how violations are detected and handled. These controls are most effective when they are enforced by the tool contract rather than by post-hoc prompt policies alone [@Kamath2025Enforcing; @Fang2025Should]. This makes it important to name the enforcement points and permission assumptions explicitly, since they determine what can be monitored and what can be blocked.

This means separating "monitoring" from "enforcement". Monitoring can detect anomalous tool traces and suspicious intermediate states, whereas enforcement constrains what actions are executable in the first place (least privilege, schema validation, parameter guards). Robust governance typically needs both: monitoring without enforcement is reactive, while enforcement without monitoring can be bypassed if the threat model includes tool-chain manipulation [@Kale2025Reliable; @Kamath2025Enforcing].

Adversarial evaluation further shows that "safe" behavior cannot be inferred from task success. Attacks can preserve primary task completion while leaking data or violating constraints, so evaluations must include explicit security metrics and threat-model reporting alongside utility metrics [@Wang2025Adversarial; @Chen2025Remsa; @Lichkovski2025Agent].

Security and governance are best treated as protocol design problems. The most reusable results specify the threat model, the tool interface assumptions, and the monitoring/enforcement points in the loop, so that defenses can be compared as systems rather than as isolated prompt patches [@Zhang2025Security; @Fu2025Eval; @Kale2025Reliable].
Put differently, security claims are interpretable only relative to an explicit threat model and a concrete tool contract.
