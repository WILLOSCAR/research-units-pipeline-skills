Recent surveys and benchmarks have begun to systematize agent behavior, but the field is still fragmented across interface assumptions and evaluation protocols. Broad overviews of agent evaluation emphasize that reported gains are often inseparable from benchmark design, tool access, and cost constraints, motivating protocol-aware comparisons rather than "leaderboard-first" summaries [@Mohammadi2025Evaluation; @Kim2026Beyond].

Our survey is aligned with this diagnosis, but differs in emphasis: we treat the agent boundary and tool interface as an explicit contract, then use evaluation protocols as the main lens for synthesis. This framing helps connect systems work on orchestration and replayability to evaluation work on comparability, and it makes cross-paper disagreements easier to attribute to concrete assumptions rather than to vague "implementation differences" [@Dong2025Etom; @Liu2025Mcpagentbench; @Khatchadourian2026Replayable].

On the interface side, tool use spans a spectrum from prompting patterns that interleave reasoning and actions to training and scaffolding methods that treat tool invocation as a structured prediction problem. Work on self-instructed tool use and action-trace prompting highlights how much behavior depends on the representation of actions and feedback [@Schick2023Toolformer; @Yao2022React], while orchestration-focused benchmarks and standards highlight the need for explicit tool schemas and reproducible tool-call traces [@Dong2025Etom; @Liu2025Mcpagentbench; @Khatchadourian2026Replayable]. Tool-selection methods and protocol critiques further show that small changes in evaluation setup can inflate pass rates, making "tool use" a moving target unless the contract is fixed [@Du2024Anytool; @Liu2025Toolscope].

Inside the agent loop, planning and search methods often resemble deliberative inference under budget constraints, where the main question is not whether the model can reason, but how reasoning is externalized, branched, and terminated. Deliberate search-style prompting and reflection-based adaptation illustrate this tension, but they also underscore that comparisons are brittle without shared environments and budgets [@Yao2023Tree; @Shinn2023Reflexion]. Memory and retrieval mechanisms add a complementary line of work that treats long-horizon performance as an information management problem, spanning compression, citation grounding, and retrieval-augmented execution [@Kang2025Distilling; @Tawosi2025Meta; @Abbineni2025Muallm]. Routing and adaptation methods then sit at the intersection, improving performance by changing which evidence and tools are used at each step, which again shifts the evaluation baseline [@Zhang2026Evoroute].

Multi-agent work introduces additional degrees of freedom: role specialization, communication protocols, and aggregation mechanisms. Frameworks for multi-turn training and evaluation, as well as tool-augmented multi-agent settings, highlight that coordination quality depends on protocol details (who speaks, when tools are called, and how disagreement is resolved), not just on the base model [@Cao2025Skyrl; @Shen2024Small; @Lumer2025Memtool].

Coordination also blurs into adaptation: multi-agent systems often change the effective policy by reallocating work across agents, changing the verification budget, or modifying tool access at run time. This makes cost normalization and protocol accounting central, since a coordination gain can reflect better division of labor, whereas it can also reflect a hidden increase in search or redundancy that would not be acceptable under a fixed deployment budget [@Zhang2026Evoroute; @Zhou2025Self; @Cao2025Skyrl].

Finally, security and risk-focused lines of work argue that agent evaluation must include threat models and end-to-end tool-chain behavior. Security benchmarks and red-teaming workflows show that agents can remain "competent" on primary tasks while being highly vulnerable to tool-layer attacks, and that adversarial effectiveness can depend strongly on chained or protocol-aware attacks [@Fu2025Eval; @Zhang2025Security; @Kale2025Reliable; @Zhou2026Beyond]. Our survey integrates these strands by using interface contracts and evaluation protocols as the organizing lens, so that differences in reported results can be traced to concrete assumptions rather than treated as unexplained variance.
