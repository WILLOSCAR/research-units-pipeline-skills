Evaluation is where the agent field either becomes cumulative or collapses into irreconcilable demos. Benchmarks and protocols determine what "success" means, which costs are counted, and which assumptions are comparable across papers; risk evaluations further determine whether an agent is merely capable or plausibly deployable under realistic threat models [@Mohammadi2025Evaluation; @Fu2025Eval].

We first map the benchmark landscape and the protocol details that most often drive variance (tool catalogs, budgets, environment stochasticity, replayability). We then survey safety, security, and governance work, focusing on tool-chain vulnerabilities and monitoring strategies that require end-to-end evaluation rather than isolated model checks [@Zhang2025Security; @Zhou2026Beyond; @Kale2025Reliable].
