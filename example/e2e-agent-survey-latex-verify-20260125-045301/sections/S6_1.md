Agent evaluation is hard for the same reason agent design is interesting: success depends on a coupled protocol of tools, environments, budgets, and stopping rules rather than on a single model forward pass. Benchmark scores can be high-signal only when the protocol details that govern comparability are made explicit and stable across studies [@Mohammadi2025Evaluation; @Ji2025Taxonomy].

One useful way to organize the benchmark landscape is by what is held fixed: some suites fix the environment and vary the agent scaffold, while others fix the scaffold and vary tool catalogs or task distributions. Multi-domain suites make protocol dependence visible by spanning very different action spaces, highlighting where an agent is robust versus where it is simply well-matched to one environment's affordances [@Shang2024Agentsquare; @Wang2025Flow].

Another axis is openness. Some benchmarks assume a closed world (fixed tool catalogs and constrained environments), whereas others move toward open-web or open-ended settings where tools, observations, and intermediate states are less controlled. This difference changes what is being measured: closed settings emphasize interface compliance and orchestration reliability, while open settings emphasize error recovery and robustness under distribution shift [@Liu2026Agents; @Chen2025Towards; @Wang2025Flow].
This is one reason cross-benchmark comparisons are often fragile even when the underlying model family is held constant.

Protocol details often dominate variance. Tool access (which APIs exist), budget models (tokens, latency, monetary cost), and retry policies can each change outcomes without any change in the underlying model. Benchmarks that model noisy tool outputs and dynamic environments therefore provide a stronger substrate for claims about deployable agents than idealized API settings [@Kim2026Beyond; @Shi2025Progent].
When protocols omit these details, comparisons implicitly reward systems that exploit the evaluation blind spot rather than systems that are genuinely more reliable.

Whereas idealized API settings can make success look like pure model capability, noisy and dynamic settings often reveal that "agent performance" is a joint property of tool mediation, error handling, and accounting. In contrast to single-number leaderboards, protocol-aware reporting (budgets, retries, tool formats, and stochasticity) makes it possible to explain why two systems disagree and what would need to be held fixed to reproduce a claim [@Wang2025Flow; @Kim2026Beyond].

Security benchmarks reinforce this point by extending the protocol to include threat models. End-to-end suites that stress the full tool-use pipeline (planning, invocation, response handling) show that agents can be simultaneously "capable" and highly vulnerable, and that attack success rates depend on protocol details such as tool formats and environment assumptions [@Fu2025Eval; @Shi2025Progent].
Security evaluations also surface failure modes (data exfiltration, unsafe tool invocation) that are orthogonal to task success, so they require explicit metrics rather than narrative claims.

Adversarial settings also reveal a second confound: agents may appear robust on primary tasks while leaking sensitive information or violating policies. Evaluations that explicitly measure privacy leakage or attack success under realistic tool-use scenarios therefore complement task-success metrics and help prevent "silent failures" from being treated as benign edge cases [@Mo2025Attractive; @Fu2025Eval].

Benchmarking effort increasingly includes meta-evaluation: measuring how evaluation itself behaves under changes in prompts, tool schemas, and scoring rules. This line of work is essential for survey synthesis because it distinguishes methodological variance from genuine capability differences, especially in fast-moving settings where models and scaffolds change quickly [@Mohammadi2025Evaluation; @Wang2025Flow; @Agrawal2025Language].

Agent benchmarks also vary in the degree to which they probe system-level behaviors (coordination, memory, recovery) rather than isolated skills. Suites that explicitly characterize agents as systems, rather than as single-turn predictors, make it easier to evaluate long-horizon behavior and to connect benchmark outcomes to design decisions in earlier chapters [@Liu2026Agents; @Chen2025Towards].

Limitations remain substantial. Many benchmarks under-specify costs and protocol details, making it hard to reproduce results or to interpret improvements as anything beyond benchmark fit. Even when evaluation is careful, reporting can omit the accounting needed for fair comparison (e.g., tool calls, retries, failure recovery), which complicates synthesis across papers [@Engelberg2026Sola; @Ji2025Taxonomy].

The most reusable contribution of an evaluation paper is often not a single leaderboard number, but a protocol: a clear description of tasks, metrics, constraints, and threat models that makes results comparable. A recurring implication across evaluation work is that protocol details should be treated as first-class evidence: score improvements are meaningful when they hold under comparable budgets, tool access, and threat models, whereas improvements that rely on hidden protocol changes are better interpreted as benchmark fit [@Mohammadi2025Evaluation; @Kim2026Beyond; @Fu2025Eval].
A practical review heuristic follows: if a result cannot be reproduced from the protocol description alone, it should be treated as an implementation-specific finding.
