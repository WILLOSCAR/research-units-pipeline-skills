Tool interfaces are not only a way to "call APIs"; they define the language in which an agent can express intent, specify constraints, and interpret feedback. As a result, orchestration design largely determines whether an agent's behavior is auditable and whether evaluation results transfer across environments that expose different tools and error semantics [@Dong2025Etom; @Khatchadourian2026Replayable].

A useful distinction is between ad-hoc interfaces (where tools are described in free-form text and invoked via loosely structured prompts) and schema-first interfaces (where tool calls are typed, validated, and replayable). Whereas ad-hoc interfaces rely on natural language negotiation to encode constraints and tool semantics, schema-first designs shift work from "prompt cleverness" to contract engineering, making failures more attributable (interface ambiguity vs model behavior) and traces more replayable for audits [@Khatchadourian2026Replayable; @Xuan2026Confidence].

Benchmark design increasingly reflects this shift. Hierarchical, multi-hop orchestration benchmarks emphasize end-to-end execution rather than isolated tool selection, forcing agents to manage intermediate states, tool dependencies, and failure recovery across multiple calls [@Dong2025Etom]. This matters because the "hard part" is often not picking the right tool once, but composing a tool chain under partial observability and brittle intermediate outputs.
Orchestration also makes error semantics visible: whether the agent can inspect intermediate tool failures and recover determines whether a multi-hop plan is robust or brittle.

At the same time, protocol details can inflate apparent success. Critiques of common evaluation setups show that pass rates can be artificially high under permissive matching or under-specified constraints, which makes "tool-use capability" hard to compare unless the protocol explicitly defines what counts as a correct tool trace and which errors are recoverable [@Du2024Anytool; @Li2026Toolprmbench]. A recurring theme is that evaluation must specify both surface-form correctness (the tool call) and semantic correctness (the resulting state), otherwise agents can optimize the scoring shortcut rather than the intended capability.
Replayability constraints also change what can be audited: if the same tool trace cannot be reproduced under the same schema and permissions, then "success" may not be a stable property of the agent at all [@Khatchadourian2026Replayable].

Self-training and adversarial self-challenging approaches report improvements on multi-turn tool-use benchmarks, but their claims are meaningful only relative to the fixed tool catalog, turn budget, and scoring rules of the benchmark. In practice, this means that improvements should be read as "under this tool interface contract" rather than as generic tool-use competence, because the training signal is shaped by the same interface constraints the system is later evaluated under [@Zhou2025Self].

Tool routing and tool selection remain a common failure point, especially when the tool set is large or when tools are semantically overlapping. Methods that explicitly target tool selection accuracy can therefore improve reliability without changing the underlying reasoning loop, but they again depend on how tools are represented and on what negative examples the protocol includes [@Liu2025Toolscope; @Hao2026From]. In contrast to orchestration-heavy settings where execution is the bottleneck, selection-heavy settings can fail simply because the interface makes multiple tools look equivalent to the model.

Cost and robustness constraints provide another lens that typical accuracy-only leaderboards miss. Attacks that expand tool-use tasks into extremely long trajectories illustrate how token budgets, cost models, and tool output noise can dominate end-to-end viability even when nominal task success remains high [@Zhou2026Beyond]. Reported attacks on tool-use benchmarks quantify this gap by showing orders-of-magnitude cost inflation under adversarial prompting, which is hard to detect if evaluation reports only binary success and ignores resource accounting [@Zhou2026Beyond].

Orchestration complexity also interacts with coordination strategies. Multi-LLM or multi-agent settings can reduce single-model brittleness through specialization or verification, but they can also amplify failure modes when tool schemas, communication formats, or aggregation rules are not fixed as part of the protocol [@Shen2024Small; @Lumer2025Memtool]. Here again the key contrast is contractual: coordination helps when the protocol defines how agents share state and validate tool traces, whereas it can devolve into redundant search when the contract is implicit and costs are not normalized.

Tool interfaces and orchestration are best evaluated as contracts that jointly constrain correctness, auditability, and cost. Comparisons are most informative when they report (i) the tool schema and permissions model, (ii) the budget and retry policy, and (iii) whether tool traces are replayable under the same scoring rules, since these protocol details often explain variance that otherwise appears as "model differences" [@Khatchadourian2026Replayable; @Dong2025Etom; @Zhou2025Self]. Confidence calibration and determinism signals can further help interpret tool traces, because an agent that knows when it is uncertain can route to verification rather than silently committing an invalid call [@Xuan2026Confidence].
This highlights a broader point: interface design and evaluation are coupled, so improving one without fixing the other often yields brittle, non-transferable gains.
