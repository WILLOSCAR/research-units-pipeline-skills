Memory is the subsystem that turns a single interaction into a persistent process: it determines what the agent can condition on, what it can retrieve, and how it maintains consistency across long horizons. The core tension is that richer memory can improve grounding and reduce hallucination, yet it can also introduce staleness, retrieval errors, and citation drift that are invisible if evaluation only measures end-task success [@Tawosi2025Meta; @Huang2025Retrieval].

Most agent memory mechanisms can be read as RAG variants embedded in a loop: an index over external information, a retrieval policy, and a write/update policy that decides what gets stored. This framing makes it clear why "more context" is not always better: the protocol must specify how evidence is selected and how it is used during action selection [@Zhang2024Large; @Yu2026Agentic].
Once memory is part of the loop, errors can compound: a single mistaken write can bias future retrieval and turn a local mistake into a persistent behavioral drift.

One pragmatic direction is to make memory cheaper. Distillation-style approaches suggest that smaller models can inherit retrieval- or memory-conditioned behavior from larger systems, but the resulting gains depend on the training distribution and on the evaluation tasks used to define "memory usefulness" [@Kang2025Distilling].

Whereas distillation amortizes memory behavior into parameters, another direction is to make memory more active: retrieval can be conditioned on the agent's current plan, and the agent can iteratively refine what it searches for. Evaluations that explicitly stress context-intensive problem instances help expose when performance is driven by retrieval quality versus by reasoning or tool execution, which is crucial for interpreting gains as "better memory" rather than as accidental benchmark fit [@Verma2026Active; @Li2025Graphcodeagent; @Kang2025Distilling].

Benchmarking memory requires metrics that do not collapse into generic "accuracy". Custom datasets that separate retrieval/citation behavior from reasoning behavior illustrate that agents can be strong at retrieval but weak at attributing evidence correctly, or vice versa, depending on how the protocol scores citations and intermediate steps [@Abbineni2025Muallm; @Hu2023Avis].

This separation also clarifies what should be measured during agent execution. A memory system can retrieve the right document yet still degrade decisions if the evidence is not integrated into action selection, and conversely a strong reasoner can still fail if retrieval returns stale or misattributed evidence. Protocols that score intermediate citations and retrieval traces therefore provide more diagnostic signal than end-task accuracy alone [@Abbineni2025Muallm; @Hu2023Avis].

Compression-based memories highlight a related trade-off: summaries can make long contexts tractable, but they also risk erasing details that later steps depend on. Systems that integrate summarization into the retrieval loop therefore need protocols that measure not just task success, but also whether compressed memories preserve the evidence needed for later decisions [@Tawosi2025Meta; @Pham2025Agent].
Whereas summary-based memories reduce context length and stabilize inputs, retrieval-heavy memories preserve source granularity but can fail through missed retrieval or spurious matches; hybrid designs effectively choose a point on this spectrum per step. Making that choice explicit (when to summarize, when to retrieve raw sources) is often more informative than simply reporting a single RAG score [@Tawosi2025Meta; @Verma2026Active].
Naming the retrieval trigger and the write policy (what gets stored, when it is updated) can make memory claims far easier to reproduce than reporting aggregate accuracy alone.
It also provides a clear hook for ablations: changing the write policy while holding retrieval fixed (or vice versa) isolates where the gain actually comes from.

Domain-specific memory settings make the same issues concrete. When the environment includes structured artifacts (codebases, documents, or technical diagrams), the memory interface must define what representations are stored and how retrieval results are grounded back to the environment, otherwise evaluation can conflate representation mismatch with model limitations [@Zhang2025Large; @Zhang2024Large].

Limitations often arise from evaluation shortcuts: retrieval systems can look strong when the benchmark implicitly rewards lexical overlap, and they can look weak when the benchmark penalizes any mismatch without accounting for protocol uncertainty. In contrast, task designs that vary evidence availability, scoring strictness, and citation requirements make it harder to overfit to surface cues and therefore provide a more reliable basis for claims about memory robustness [@Ye2025Taska; @Abbineni2025Muallm].

Memory and retrieval are most usefully treated as a contract with explicit protocol elements (index scope, retrieval policy, write policy, and citation grounding). When these elements are reported, it becomes easier to compare memory mechanisms across agents and to diagnose whether failures come from retrieval, reasoning, or execution rather than from an opaque mixture of all three [@Yao2022React; @Huang2025Retrieval; @Yu2026Agentic].
This implies that memory papers are most reusable when they report not only "what was retrieved", but also "how retrieval affected subsequent actions" under a fixed protocol.
