## Discussion

Protocol drift is the most common hidden confound in agent results. Across the literature, "tool use" can mean different action representations, tool catalogs, retry rules, and observability assumptions; "success" can mean different stopping conditions and cost models; and "robustness" can mean different threat models. Without a shared protocol surface, even careful ablations can fail to transfer, because the claim is anchored to an implicit contract rather than to a comparable evaluation setting [@Mohammadi2025Evaluation; @Liu2025Mcpagentbench; @Khatchadourian2026Replayable].

This also reframes how to read progress on planning, memory, and adaptation. Deliberative prompting and reflection can look like algorithmic improvements, but their benefit often depends on budget and termination policies; similarly, retrieval and memory can look like capability gains, but much of the variance comes from what evidence is retrievable and how it is cited or validated in the loop [@Yao2023Tree; @Shinn2023Reflexion; @Tawosi2025Meta]. A practical implication is that future benchmarks should report key protocol details (tool access, budget, environment stochasticity) as first-class metadata, so that comparisons can be interpreted as "under the same contract" rather than "under similar intent."

Security results reinforce the same point. Many attacks operate at the tool boundary: prompt injection in tool descriptions, unsafe parameter requests, or tool-transfer exploits. These failures can be invisible if evaluation only measures end-task completion, since a system can remain functional while leaking data or violating policies. End-to-end benchmarks that exercise the full tool-use pipeline therefore provide a stronger substrate for claims about deployable agents than isolated safety checks [@Fu2025Eval; @Zhang2025Security; @Kale2025Reliable].

Finally, tables and taxonomies are most valuable when they compress decisions, not when they enumerate papers. We therefore treat the Appendix as a protocol map: representative approaches are grouped by their loop and interface assumptions, and benchmarks are grouped by the task, metric, and protocol constraints that govern comparability. This design is intended to support reuse: practitioners can locate the relevant protocol assumptions first, then read the associated sections as decision-relevant contrasts rather than as a narrative catalog.
