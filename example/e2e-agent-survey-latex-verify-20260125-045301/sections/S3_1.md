For system builders, the first decision is what counts as an action: whether an agent "acts" by emitting text, calling an API, manipulating a UI, or controlling an embodied interface. That choice fixes the action space and therefore determines what kinds of failures can be detected, retried, or recovered within a closed-loop protocol [@Yao2022React]. It also determines what "success" means: if an action is a structured tool call, then executability and schema compliance become measurable; if an action is free-form text, then correctness is mediated by downstream interpreters and implicit conventions.

A second decision is the scope of the loop: what is treated as state, what feedback is observable, and what resets between episodes. Multi-domain evaluations make this explicit by spanning environments with very different affordances (web, embodied, tool-use, games), which helps distinguish general loop assumptions from benchmark-specific quirks [@Shang2024Agentsquare; @Li2025Agentswift]. In practice, two agents can share the same base model yet behave very differently if one has richer observability (tool traces, error codes, intermediate state) while the other operates with partial or noisy feedback.

Within this framing, "better agents" can come from very different sources: improved control policies, better representations of actions, or better search over agent designs. Automated agent search reports gains when evaluated across heterogeneous benchmarks, but those gains still hinge on how the search space encodes action primitives and observability [@Li2025Agentswift]. This differs from hand-engineered agent scaffolds, where the design space is narrower but the interface contract is often tighter and easier to audit across runs.

These two routes also lead to different evaluation artifacts. Search-centric approaches treat the environment as an objective and optimize agent designs against it, whereas trajectory-centric approaches treat the environment as both a benchmark and a data generator that shapes what the agent can learn from interaction [@Li2025Agentswift; @Xi2026Toolgym].

The evaluation substrate can also be used as a data engine. Work that collects interaction trajectories to finetune tool-using models illustrates that the loop is not just a test harness: it shapes the data distribution the agent will later rely on, and it can amplify or dampen the apparent benefit of downstream training strategies [@Xi2026Toolgym; @Feng2025Group]. For example, collecting tool-use trajectories for finetuning can improve downstream behavior, but the gain is conditional on what the environment exposes as actions and feedback (e.g., which errors are surfaced versus silently absorbed) [@Xi2026Toolgym].
That blurs the line between "evaluation" and "training": the loop design that defines what is measured also defines what data becomes available for future agents.

Many "agent" failures are better read as mismatches between planning and execution under constraints: a model may generate plausible plans but fail at constraint following or low-level tool execution, especially when tool outputs are noisy or when the environment is partially observed. Comparative evaluations that separate planning from execution reinforce that action-space design and constraint specification are as important as the base model choice [@Xi2026Toolgym; @Zhang2025Datascibench].
Papers that report only end-task success can hide whether failures arise from poor decision policies or from brittle actuation under an underspecified tool contract.

Interface standards make the contract more legible. Benchmarks grounded in real tool definitions (rather than idealized APIs) highlight how much practical performance depends on tool schemas, permissions, and error semantics that are often hidden in paper summaries [@Liu2025Mcpagentbench].
Standardization also makes negative results more actionable: when failures are tied to a named contract element (schema, permission, observability), downstream work can fix the right layer rather than iterating on prompts.

The same interface boundary also defines an attack surface. Comparative analyses of function-calling style architectures versus protocol-based tool interfaces suggest that differences in tool semantics and mediation can shift which vulnerabilities dominate and how easily attacks chain across the loop [@Gasmi2025Bridging]. In particular, reported comparisons quantify large shifts in attack success under different mediation layers (e.g., higher success under function calling than under MCP in one comparative setup), and show that chained attacks can amplify success rates relative to simple attacks, which makes "secure by default" assumptions fragile unless the loop is explicitly hardened [@Gasmi2025Bridging].

A recurring limitation is that loop design and evaluation cannot optimize all objectives at once: pushing for stronger task success, stronger safety guarantees, and stronger efficiency can create tensions that look like "model limits" but are often system-level trade-offs [@Zhang2026Evoroute]. In contrast to single-benchmark improvements, cross-domain transfer often fails precisely because the action space and feedback contract change. Results in specialized domains (e.g., geometry or domain-specific optimization agents) further emphasize that transferring an action policy across domains typically requires rethinking the action space and protocol, not just swapping models [@Zhao2025Achieving; @Ghose2025Orfs].

A practical reading rule emerges from this literature: when two papers report different "agent performance", first align their action spaces and observability assumptions, then compare policies under a shared protocol. Without that contract alignment, improvements can be real yet non-composable across settings, whereas apparent regressions can simply be artifacts of incomparable loops and evaluation assumptions [@Li2025Agentswift; @Xi2026Toolgym; @Liu2025Mcpagentbench; @Fumero2025Cybersleuth].
"Agent progress" is best tracked by tightening and standardizing loop contracts, not only by scaling models.
The most informative comparisons make the loop contract explicit before attributing outcomes to model choice.
