Large language models are increasingly embedded in agents that do more than respond: they decide, act, observe, and iterate under real constraints. A practical agent is therefore a closed-loop system whose behavior depends not only on the base model, but also on the action space it is allowed to execute, the feedback it can observe, and the protocol that couples these components over many steps [@Yao2022React; @Wang2023Voyager; @Shinn2023Reflexion].

A recurring lesson in recent work is that interfaces are not plumbing: they are contracts. Tool APIs, schemas, permissions, and observability determine what actions are even expressible and which failures are detectable. As the community moves from ad-hoc textual tool calls toward more structured orchestration and tool standards, claims that look similar on paper can become incomparable in practice unless the interface contract is made explicit [@Schick2023Toolformer; @Dong2025Etom; @Liu2025Mcpagentbench; @Khatchadourian2026Replayable].

Beyond interface design, the agent loop itself has diversified. Planning and search style loops trade off deliberation depth against latency and cost, while memory and retrieval mechanisms trade off longer context against staleness, grounding, and citation hygiene [@Yao2023Tree; @Kang2025Distilling; @Tawosi2025Meta]. Adaptation methods add another axis: self-improvement can raise performance, but it also changes what should be held fixed in evaluation (policy updates, tool access, or budget assumptions) [@Zhang2026Evoroute; @Du2024Anytool].

These dynamics make evaluation the central bottleneck. Many benchmarks measure end-task success, but the results can be dominated by protocol details: tool catalog design, budget and cost models, retry policies, environment stochasticity, and security assumptions. Security benchmarks further show that the same interface surface that enables tool use also enables prompt injection, tool abuse, and data exfiltration, which must be evaluated end-to-end rather than as isolated model vulnerabilities [@Mohammadi2025Evaluation; @Shi2025Progent; @Fu2025Eval; @Zhang2025Security].

This survey adopts a protocol-first organization. After defining the agent boundary and interface contract, we review the design space across (i) planning and reasoning loops, (ii) memory and retrieval, (iii) self-improvement and multi-agent coordination, and (iv) benchmarks and risk surfaces. Our goal is not to exhaustively summarize every paper, but to make comparisons auditable: each subsection is framed around concrete trade-offs, protocol anchors, and limitations, and the Appendix tables provide compact maps of representative approaches and benchmark settings.

We retrieved an arXiv candidate pool of 800 papers (809 before deduplication) using keyword-based queries focused on LLM agents and tool use, and curated a 220-paper core set for mapping and evidence extraction. Unless otherwise stated, our evidence is derived from metadata and abstracts rather than full-text annotation, so we report quantitative claims only when they are explicitly supported by the cited sources.
