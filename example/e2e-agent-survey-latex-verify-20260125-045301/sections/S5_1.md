Adaptation methods promise to turn agents from static prompt programs into systems that improve with experience. The central tension is that learning can raise task success, yet it can also change what is held fixed in evaluation (policy updates, tool access, budgets), making gains hard to interpret unless the protocol specifies the learning loop explicitly [@Mohammadi2025Evaluation; @Zhang2026Evoroute].
Adaptation claims are most comparable when papers state what was updated (policy, prompts, routing), when updates occur, and how costs and data are accounted.

One family of approaches treats adaptation as reflection and revision: the agent uses its own traces to critique failures, refine plans, or update internal heuristics. In practice, these loops blur the line between "reasoning" and "learning", since the improvement mechanism is itself mediated by the model and by the feedback channel it can access [@Yao2022React; @Van2025Survey]. Whereas reflection-based adaptation often changes the agent's behavior by changing prompts, rules, or memory, it typically does so without changing the tool contract or the benchmark itself.
Reflection loops can be brittle when the feedback channel is ambiguous (e.g., sparse reward vs rich traces), because the agent can learn to rationalize failures rather than to correct them.

Another family treats adaptation as data generation and self-training under fixed tools. Self-challenging methods report gains on multi-turn tool-use benchmarks using self-generated training data, but these gains remain conditional on the benchmark's tool catalog, scoring, and budget assumptions [@Zhou2025Self; @Du2024Anytool]. In contrast to reflection loops that reshape behavior at inference time, self-training reshapes the policy via data and optimization, making the "training protocol" part of what must be reported for results to be comparable.

A protocol-first comparison therefore asks: what is being updated, on what signal, and under what accounting? Two methods can both claim "self-improvement", yet one may be updating only a routing policy while another updates the underlying policy parameters; without this distinction, benchmark-to-benchmark transfer results are hard to interpret [@Mohammadi2025Evaluation; @Zhou2025Self].

Routing-based adaptation emphasizes a different lever: rather than changing the base model, the system changes which tools, prompts, or sub-policies are used in a given context. Results on agentic benchmarks suggest that such routing can improve performance when integrated into existing systems, but it also increases protocol complexity because the routing policy becomes part of the evaluation contract [@Zhang2026Evoroute; @Wu2025Evolver]. Whereas weight-updating methods pay their cost during training, routing methods often pay cost during inference (additional calls, verification, or search), so cost normalization becomes decisive.

Evolution-style adaptation pushes this further by explicitly searching over policies, prompts, or routes under an evaluation signal. These methods can be effective, but they also make overfitting risks more acute: when the search objective is tied to a benchmark, the system may learn to exploit the scoring procedure unless constraints and held-out protocols are part of the evaluation design [@Wu2025Evolver; @Nitin2025Faultline].
Evolution-style methods also make reporting burdens higher: without clear accounting of search budget and selection criteria, it is hard to separate algorithmic improvement from brute-force exploration.

When adaptation is viewed through an optimization lens, a persistent failure mode is reward hacking: the system can learn shortcuts that exploit benchmark scoring without improving the underlying capability. This risk is amplified when protocols under-specify constraints or when intermediate traces are not audited, since the learning loop can overfit to hidden degrees of freedom [@Nitin2025Faultline; @Mohammadi2025Evaluation].
Held-out protocols and explicit constraint reporting are often more important for adaptation papers than small average score gains.

Safety and reliability constraints further complicate adaptation. Methods that prioritize sandboxing or fault containment highlight that an "improved" policy can still be unacceptable if it increases the likelihood of unsafe tool invocations or makes behavior harder to monitor, which again requires protocol-level reporting rather than anecdotal success stories [@Xia2025Sand; @Nitin2025Faultline].

Adaptation is also domain-sensitive. Work that demonstrates autonomous performance on specialized scientific or engineering tasks illustrates that "autonomy" depends on domain interfaces and evaluation settings, and that transfer across domains often requires redesigning the feedback loop rather than simply scaling the same adaptation mechanism [@Li2026Autonomous; @Van2025Survey].

A practical limitation is that many benchmarks treat agents as stationary policies, while adaptation methods explicitly violate that assumption. This makes it easy to conflate learning progress with leakage, implicit curriculum effects, or unreported protocol changes unless the evaluation explicitly accounts for learning dynamics (when updates occur, what data is used, and under which budgets) [@Mohammadi2025Evaluation; @Zhou2025Self].

Self-improvement results are easiest to compare when papers make three elements explicit: the feedback channel (what signals are available), the update rule (what changes in the agent), and the accounting rule (how cost and data are counted). Making these elements explicit enables fair comparison across reflection-style, self-training, routing, and evolution-style methods, and it makes the remaining limitations (stability, reward hacking, safety) easier to diagnose and reproduce [@Zhang2026Evoroute; @Wu2025Evolver; @Du2024Anytool; @Nitin2025Faultline].
A simple rule of thumb follows: if an adaptation method cannot state its update and accounting rules precisely, its improvements should be treated as protocol-specific until independently reproduced.
