{"section_id": "3", "section_title": "Foundations & Interfaces", "subsections": [{"sub_id": "3.1", "title": "Agent loop and action spaces"}, {"sub_id": "3.2", "title": "Tool interfaces and orchestration"}], "synthesis_mode": "tradeoff_matrix", "synthesis_preview": ["Synthesize by comparing approaches along a small trade-off matrix (axes + evaluation protocol), not by listing papers.", "Keep contrasts decision-relevant (reliability/cost/safety) and reuse consistent evaluation anchors across H3s."], "throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "key_contrasts": ["evaluation", "tool interfaces"], "lead_paragraph_plan": ["Para 1: state the chapter’s role and the decision-relevant comparison axes (no new facts).", "Para 2: preview the H3 subsections and how they decompose the chapter question.", "Para 3 (optional): highlight evaluation anchors + recurrent limitations that will recur across H3s."], "bridge_terms": ["benchmarks/metrics", "compute", "function calling", "tool schema", "routing", "sandbox", "observability"], "generated_at": "2026-01-25T04:54:07"}
{"section_id": "4", "section_title": "Core Components (Planning + Memory)", "subsections": [{"sub_id": "4.1", "title": "Planning and reasoning loops"}, {"sub_id": "4.2", "title": "Memory and retrieval (RAG)"}], "synthesis_mode": "tradeoff_matrix", "synthesis_preview": ["Synthesize by comparing approaches along a small trade-off matrix (axes + evaluation protocol), not by listing papers.", "Keep contrasts decision-relevant (reliability/cost/safety) and reuse consistent evaluation anchors across H3s."], "throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "key_contrasts": ["planning/control loop", "memory/retrieval"], "lead_paragraph_plan": ["Para 1: state the chapter’s role and the decision-relevant comparison axes (no new facts).", "Para 2: preview the H3 subsections and how they decompose the chapter question.", "Para 3 (optional): highlight evaluation anchors + recurrent limitations that will recur across H3s."], "bridge_terms": ["planner/executor", "search", "deliberation", "action grounding", "benchmarks/metrics", "compute", "retrieval", "index", "write policy", "long-term memory"], "generated_at": "2026-01-25T04:54:07"}
{"section_id": "5", "section_title": "Learning, Adaptation & Coordination", "subsections": [{"sub_id": "5.1", "title": "Self-improvement and adaptation"}, {"sub_id": "5.2", "title": "Multi-agent coordination"}], "synthesis_mode": "tradeoff_matrix", "synthesis_preview": ["Synthesize by comparing approaches along a small trade-off matrix (axes + evaluation protocol), not by listing papers.", "Keep contrasts decision-relevant (reliability/cost/safety) and reuse consistent evaluation anchors across H3s."], "throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: communication protocol / roles.", "Compare approaches along: aggregation (vote / debate / referee).", "Compare approaches along: stability / robustness."], "key_contrasts": ["learning/feedback", "coordination"], "lead_paragraph_plan": ["Para 1: state the chapter’s role and the decision-relevant comparison axes (no new facts).", "Para 2: preview the H3 subsections and how they decompose the chapter question.", "Para 3 (optional): highlight evaluation anchors + recurrent limitations that will recur across H3s."], "bridge_terms": ["preference", "reward", "feedback", "self-improvement", "benchmarks/metrics", "compute", "roles", "communication", "debate", "aggregation", "stability"], "generated_at": "2026-01-25T04:54:07"}
{"section_id": "6", "section_title": "Evaluation & Risks", "subsections": [{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols"}, {"sub_id": "6.2", "title": "Safety, security, and governance"}], "synthesis_mode": "tradeoff_matrix", "synthesis_preview": ["Synthesize by comparing approaches along a small trade-off matrix (axes + evaluation protocol), not by listing papers.", "Keep contrasts decision-relevant (reliability/cost/safety) and reuse consistent evaluation anchors across H3s."], "throughline": ["Pin scope to goal: agent survey latex.", "Compare approaches along: evaluation protocol (datasets, metrics, human evaluation).", "Compare approaches along: compute and latency constraints.", "Compare approaches along: tool interface contract (schemas / protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability."], "key_contrasts": ["tool interfaces", "security"], "lead_paragraph_plan": ["Para 1: state the chapter’s role and the decision-relevant comparison axes (no new facts).", "Para 2: preview the H3 subsections and how they decompose the chapter question.", "Para 3 (optional): highlight evaluation anchors + recurrent limitations that will recur across H3s."], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks", "threat model", "prompt/tool injection", "monitoring", "guardrails", "benchmarks/metrics", "compute"], "generated_at": "2026-01-25T04:54:07"}
