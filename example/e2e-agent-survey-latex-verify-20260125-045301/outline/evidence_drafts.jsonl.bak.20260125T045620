{"sub_id": "3.1", "title": "Agent loop and action spaces", "evidence_ids": ["E-P0070-904ba35500", "E-P0099-312a342670", "E-P0099-f7a14123f9", "E-P0136-38a26e4777", "E-P0095-4b027dfb27", "E-P0181-895b04aa5c", "E-P0181-8c9597d805", "E-P0023-8e34a29629", "E-P0023-8a2c2e2291", "E-P0083-39281e5083", "E-P0057-60cc0d458f", "E-P0188-1063eee7ce", "E-P0081-c8c4670812", "E-P0099-3a4792de2b", "E-P0001-ca4a00b5cf", "E-P0105-1d5f67b08e", "E-P0105-ddd045953e", "E-P0070-9d669c9af7"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0070-904ba35500", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "paper_id": "P0070", "citations": ["Li2025Agentswift"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]"}}, {"evidence_id": "E-P0099-312a342670", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "paper_id": "P0099", "citations": ["Liu2025Mcpagentbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#method"}}, {"evidence_id": "E-P0099-f7a14123f9", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "paper_id": "P0099", "citations": ["Liu2025Mcpagentbench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#limitations[1]"}}, {"evidence_id": "E-P0136-38a26e4777", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "paper_id": "P0136", "citations": ["Shang2024Agentsquare"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]"}}, {"evidence_id": "E-P0095-4b027dfb27", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "paper_id": "P0095", "citations": ["Feng2025Group"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0095#key_results[0]"}}, {"evidence_id": "E-P0181-895b04aa5c", "text": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "paper_id": "P0181", "citations": ["Xi2026Toolgym"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}}, {"evidence_id": "E-P0181-8c9597d805", "text": "Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents.", "paper_id": "P0181", "citations": ["Xi2026Toolgym"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[1]"}}, {"evidence_id": "E-P0023-8e34a29629", "text": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "paper_id": "P0023", "citations": ["Gasmi2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}}, {"evidence_id": "E-P0023-8a2c2e2291", "text": "Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates.", "paper_id": "P0023", "citations": ["Gasmi2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[1]"}}, {"evidence_id": "E-P0083-39281e5083", "text": "Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.", "paper_id": "P0083", "citations": ["Zhang2025Datascibench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[1]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zhang2026Evoroute", "Song2026Envscaler", "Soliman2026Intagent"]}], "claim_candidates": [{"claim": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "evidence_field": "evidence_snippet", "citations": ["Li2025Agentswift"]}, {"claim": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "evidence_field": "evidence_snippet", "citations": ["Liu2025Mcpagentbench"]}, {"claim": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "evidence_field": "evidence_snippet", "citations": ["Liu2025Mcpagentbench"]}, {"claim": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "evidence_field": "evidence_snippet", "citations": ["Shang2024Agentsquare"]}, {"claim": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "evidence_field": "evidence_snippet", "citations": ["Feng2025Group"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0057`, `P0170`, `P0173`", "B_papers": "Planning / reasoning loops: `P0024`, `P0070`, `P0200`", "A_highlights": [{"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 3}, {"paper_id": "P0181", "evidence_id": "E-P0181-8c9597d805", "excerpt": "Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents.", "citations": ["Xi2026Toolgym"], "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[1]", "score": 3}], "B_highlights": [{"paper_id": "P0136", "evidence_id": "E-P0136-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]", "score": 3}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 3}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Li2025Agentswift", "Xi2026Toolgym", "Shang2024Agentsquare"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0057`, `P0170`, `P0173`", "B_papers": "Planning / reasoning loops: `P0024`, `P0070`, `P0200`", "A_highlights": [{"paper_id": "P0181", "evidence_id": "E-P0181-895b04aa5c", "excerpt": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["Xi2026Toolgym"], "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]", "score": 2}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0136", "evidence_id": "E-P0136-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]", "score": 1}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Xi2026Toolgym", "Li2025Agentswift", "Shang2024Agentsquare"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0057`, `P0170`, `P0173`", "B_papers": "Planning / reasoning loops: `P0024`, `P0070`, `P0200`", "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]", "score": 3}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0136", "evidence_id": "E-P0136-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]", "score": 2}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'tool interface contract (schemas / protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Gasmi2025Bridging", "Li2025Agentswift", "Shang2024Agentsquare"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0057`, `P0170`, `P0173`", "B_papers": "Planning / reasoning loops: `P0024`, `P0070`, `P0200`", "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]", "score": 3}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0136", "evidence_id": "E-P0136-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]", "score": 2}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Gasmi2025Bridging", "Li2025Agentswift", "Shang2024Agentsquare"]}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0057`, `P0170`, `P0173`", "B_papers": "Planning / reasoning loops: `P0024`, `P0070`, `P0200`", "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-8e34a29629", "excerpt": "Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]", "score": 2}, {"paper_id": "P0023", "evidence_id": "E-P0023-8a2c2e2291", "excerpt": "Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates.", "citations": ["Gasmi2025Bridging"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[1]", "score": 2}], "B_highlights": [{"paper_id": "P0136", "evidence_id": "E-P0136-38a26e4777", "excerpt": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]", "score": 1}, {"paper_id": "P0070", "evidence_id": "E-P0070-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "sandboxing / permissions / observability", "citations": ["Gasmi2025Bridging", "Shang2024Agentsquare", "Li2025Agentswift"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: LLMs; GAIA; EvoRoute; BrowseComp; LLM-simulated; SFT; RUC-NLPIR; EnvScaler; SkelBuilder; ScenGenerator.", "citations": ["Zhang2026Evoroute", "Song2026Envscaler", "Soliman2026Intagent", "Xi2026Toolgym"]}], "failures_limitations": [{"bullet": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"]}, {"bullet": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"]}, {"bullet": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"]}, {"bullet": "We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service).", "citations": ["Gasmi2025Bridging"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "evidence_ids": ["E-P0011-b6b7af5a81", "E-P0085-192e78b614", "E-P0046-4da9e4ae32", "E-P0036-2e6956a116", "E-P0128-468a77ff1d", "E-P0046-d5c234444e", "E-P0178-1f16a03ad8", "E-P0011-1564795a81", "E-P0011-eb0bf8bedb", "E-P0101-35271418ac", "E-P0036-17d0e7f9d9", "E-P0085-55ce44af76", "E-P0085-6ee6d5b951", "E-P0158-9640816b42", "E-P0060-3e2edc05cd", "E-P0178-70f5de1cf1", "E-P0171-ed4427964c", "E-P0180-c2fdc5ad72"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0011-b6b7af5a81", "text": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "paper_id": "P0011", "citations": ["Zhou2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]"}}, {"evidence_id": "E-P0085-192e78b614", "text": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "paper_id": "P0085", "citations": ["Dong2025Etom"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0085#method"}}, {"evidence_id": "E-P0046-4da9e4ae32", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "paper_id": "P0046", "citations": ["Du2024Anytool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0046#limitations[1]"}}, {"evidence_id": "E-P0036-2e6956a116", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "paper_id": "P0036", "citations": ["Zhou2025Self"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]"}}, {"evidence_id": "E-P0128-468a77ff1d", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "paper_id": "P0128", "citations": ["Liu2025Toolscope"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0128#key_results[0]"}}, {"evidence_id": "E-P0046-d5c234444e", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "paper_id": "P0046", "citations": ["Du2024Anytool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[0]"}}, {"evidence_id": "E-P0178-1f16a03ad8", "text": "In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "paper_id": "P0178", "citations": ["Khatchadourian2026Replayable"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[1]"}}, {"evidence_id": "E-P0011-1564795a81", "text": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task.", "paper_id": "P0011", "citations": ["Zhou2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#method"}}, {"evidence_id": "E-P0011-eb0bf8bedb", "text": "Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm.", "paper_id": "P0011", "citations": ["Zhou2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0011#summary_bullets[1]"}}, {"evidence_id": "E-P0101-35271418ac", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "paper_id": "P0101", "citations": ["Lumer2025Memtool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench", "Hao2026From"]}], "claim_candidates": [{"claim": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "evidence_field": "evidence_snippet", "citations": ["Zhou2026Beyond"]}, {"claim": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "evidence_field": "evidence_snippet", "citations": ["Dong2025Etom"]}, {"claim": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "evidence_field": "evidence_snippet", "citations": ["Du2024Anytool"]}, {"claim": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "evidence_field": "evidence_snippet", "citations": ["Zhou2025Self"]}, {"claim": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "evidence_field": "evidence_snippet", "citations": ["Liu2025Toolscope"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0011`, `P0060`, `P0171`", "B_papers": "Tool-use and function calling: `P0011`, `P0060`, `P0171`", "A_highlights": [{"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 3}, {"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0085", "evidence_id": "E-P0085-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0085#method", "score": 4}, {"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 3}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhou2025Self", "Zhou2026Beyond", "Dong2025Etom"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0011`, `P0060`, `P0171`", "B_papers": "Tool-use and function calling: `P0011`, `P0060`, `P0171`", "A_highlights": [{"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 3}, {"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}, {"paper_id": "P0178", "evidence_id": "E-P0178-1f16a03ad8", "excerpt": "In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "citations": ["Khatchadourian2026Replayable"], "pointer": "papers/paper_notes.jsonl:paper_id=P0178#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhou2025Self", "Zhou2026Beyond", "Khatchadourian2026Replayable"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0011`, `P0060`, `P0171`", "B_papers": "Tool-use and function calling: `P0011`, `P0060`, `P0171`", "A_highlights": [{"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 2}, {"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0085", "evidence_id": "E-P0085-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0085#method", "score": 3}, {"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'tool interface contract (schemas / protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Zhou2025Self", "Zhou2026Beyond", "Dong2025Etom"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0011`, `P0060`, `P0171`", "B_papers": "Tool-use and function calling: `P0011`, `P0060`, `P0171`", "A_highlights": [{"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 2}, {"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0085", "evidence_id": "E-P0085-192e78b614", "excerpt": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Etom"], "pointer": "papers/paper_notes.jsonl:paper_id=P0085#method", "score": 3}, {"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Zhou2025Self", "Zhou2026Beyond", "Dong2025Etom"]}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0011`, `P0060`, `P0171`", "B_papers": "Tool-use and function calling: `P0011`, `P0060`, `P0171`", "A_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}, {"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0011", "evidence_id": "E-P0011-b6b7af5a81", "excerpt": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#key_results[0]", "score": 2}, {"paper_id": "P0011", "evidence_id": "E-P0011-eb0bf8bedb", "excerpt": "Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm.", "citations": ["Zhou2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0011#summary_bullets[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "sandboxing / permissions / observability", "citations": ["Zhou2026Beyond", "Zhou2025Self"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: RAG; MCP; MCTS; LLMs; BFCL; GPU; ToolBench; PRMs; PRM; ToolPRMBench.", "citations": ["Zhou2026Beyond", "Li2026Toolprmbench", "Hao2026From", "Khatchadourian2026Replayable"]}], "failures_limitations": [{"bullet": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents.", "citations": ["Zhou2026Beyond"]}, {"bullet": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task.", "citations": ["Zhou2026Beyond"]}, {"bullet": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x.", "citations": ["Zhou2026Beyond"]}, {"bullet": "These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "citations": ["Zhou2026Beyond"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "evidence_ids": ["E-P0033-68db58914f", "E-P0024-9d9d60644a", "E-P0033-6829c8b583", "E-P0130-771620f84f", "E-P0001-ca4a00b5cf", "E-P0144-2a7ea60588", "E-P0159-c0a98eb625", "E-P0033-4792fb4704", "E-P0012-8517628bd0", "E-P0091-e4ac5005b3", "E-P0012-e38b4bdff3", "E-P0076-b35b53de13", "E-P0076-baa622fa7f", "E-P0113-4bcafdb221", "E-P0213-837ee80fad", "E-P0091-4334d22ac9", "E-P0024-04c60086db", "E-P0108-1ad09376fe"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0033-68db58914f", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "paper_id": "P0033", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]"}}, {"evidence_id": "E-P0024-9d9d60644a", "text": "We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM).", "paper_id": "P0024", "citations": ["Kim2025Bridging"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0024#method"}}, {"evidence_id": "E-P0033-6829c8b583", "text": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "paper_id": "P0033", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#limitations[1]"}}, {"evidence_id": "E-P0130-771620f84f", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "paper_id": "P0130", "citations": ["Hu2025Training"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0144-2a7ea60588", "text": "Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate.", "paper_id": "P0144", "citations": ["Shi2024Ehragent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0144#key_results[0]"}}, {"evidence_id": "E-P0159-c0a98eb625", "text": "We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4).", "paper_id": "P0159", "citations": ["Ji2024Testing"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0159#key_results[0]"}}, {"evidence_id": "E-P0033-4792fb4704", "text": "The core problem that enables attacks to succeed lies in over-privileged tool access.", "paper_id": "P0033", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[2]"}}, {"evidence_id": "E-P0012-8517628bd0", "text": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "paper_id": "P0012", "citations": ["Zhou2025Reasoning"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0012#summary_bullets[1]"}}, {"evidence_id": "E-P0091-e4ac5005b3", "text": "Our best performing strategy generates executable API calls 88% of the time.", "paper_id": "P0091", "citations": ["Mudur2025Feabench"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0091#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Planning and reasoning loops'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Zhou2025Reasoning", "Kim2025Bridging", "Shi2025Progent"]}], "claim_candidates": [{"claim": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "evidence_field": "evidence_snippet", "citations": ["Shi2025Progent"]}, {"claim": "We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM).", "evidence_field": "evidence_snippet", "citations": ["Kim2025Bridging"]}, {"claim": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "evidence_field": "evidence_snippet", "citations": ["Shi2025Progent"]}, {"claim": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "evidence_field": "evidence_snippet", "citations": ["Hu2025Training"]}, {"claim": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "evidence_field": "evidence_snippet", "citations": ["Yao2022React"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0024`, `P0033`", "B_papers": "Planning / reasoning loops: `P0012`, `P0024`, `P0034`", "A_highlights": [{"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 2}, {"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 2}, {"paper_id": "P0091", "evidence_id": "E-P0091-e4ac5005b3", "excerpt": "Our best performing strategy generates executable API calls 88% of the time.", "citations": ["Mudur2025Feabench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0091#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Hu2025Training", "Shi2025Progent", "Mudur2025Feabench"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0024`, `P0033`", "B_papers": "Planning / reasoning loops: `P0012`, `P0024`, `P0034`", "A_highlights": [{"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 2}, {"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 2}, {"paper_id": "P0091", "evidence_id": "E-P0091-e4ac5005b3", "excerpt": "Our best performing strategy generates executable API calls 88% of the time.", "citations": ["Mudur2025Feabench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0091#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Hu2025Training", "Shi2025Progent", "Mudur2025Feabench"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0024`, `P0033`", "B_papers": "Planning / reasoning loops: `P0012`, `P0024`, `P0034`", "A_highlights": [{"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 1}, {"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0091", "evidence_id": "E-P0091-e4ac5005b3", "excerpt": "Our best performing strategy generates executable API calls 88% of the time.", "citations": ["Mudur2025Feabench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0091#key_results[0]", "score": 2}, {"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'tool interface contract (schemas / protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Hu2025Training", "Shi2025Progent", "Mudur2025Feabench"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0024`, `P0033`", "B_papers": "Planning / reasoning loops: `P0012`, `P0024`, `P0034`", "A_highlights": [{"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 1}, {"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0091", "evidence_id": "E-P0091-e4ac5005b3", "excerpt": "Our best performing strategy generates executable API calls 88% of the time.", "citations": ["Mudur2025Feabench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0091#key_results[0]", "score": 2}, {"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Hu2025Training", "Shi2025Progent", "Mudur2025Feabench"]}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "A_papers": "Agent frameworks / architectures: `P0012`, `P0024`, `P0033`", "B_papers": "Planning / reasoning loops: `P0012`, `P0024`, `P0034`", "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}, {"paper_id": "P0012", "evidence_id": "E-P0012-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#summary_bullets[1]", "score": 2}], "B_highlights": [{"paper_id": "P0012", "evidence_id": "E-P0012-8517628bd0", "excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": "papers/paper_notes.jsonl:paper_id=P0012#summary_bullets[1]", "score": 2}, {"paper_id": "P0130", "evidence_id": "E-P0130-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0130#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "sandboxing / permissions / observability", "citations": ["Shi2025Progent", "Zhou2025Reasoning", "Hu2025Training"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: RSP; GSI; RSV; FEVER; RSP-M; HotpotQA; ReAct; SCL; CCAM; GPT-4o-powered.", "citations": ["Zhou2025Reasoning", "Kim2025Bridging", "Shi2025Progent", "Hatalis2025Review"]}], "failures_limitations": [{"bullet": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"]}, {"bullet": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "citations": ["Shi2025Progent"]}, {"bullet": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments.", "citations": ["Shi2025Progent"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "evidence_ids": ["E-P0030-897bcc2f50", "E-P0084-17fb12f5b7", "E-P0131-c9781caf3b", "E-P0104-e294aeefb5", "E-P0032-f36b515991", "E-P0131-4af0cf3c02", "E-P0001-ca4a00b5cf", "E-P0166-9abcf1bf8a", "E-P0084-dc7fe955d8", "E-P0191-3e2348e239", "E-P0050-52fea1d199", "E-P0030-79064ef6b3", "E-P0167-f0f0faaada", "E-P0191-f55e33ae8d", "E-P0093-d568c53e1d", "E-P0127-53536132a8", "E-P0164-1c25e10ffc", "E-P0032-e3ebb83eb2"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0030-897bcc2f50", "text": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs.", "paper_id": "P0030", "citations": ["Zhang2025Large"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]"}}, {"evidence_id": "E-P0084-17fb12f5b7", "text": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "paper_id": "P0084", "citations": ["Kang2025Distilling"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0084#method"}}, {"evidence_id": "E-P0131-c9781caf3b", "text": "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability.", "paper_id": "P0131", "citations": ["Huang2025Retrieval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0131#limitations[1]"}}, {"evidence_id": "E-P0104-e294aeefb5", "text": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "paper_id": "P0104", "citations": ["Abbineni2025Muallm"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0104#key_results[1]"}}, {"evidence_id": "E-P0032-f36b515991", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "paper_id": "P0032", "citations": ["Tawosi2025Meta"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[1]"}}, {"evidence_id": "E-P0131-4af0cf3c02", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "paper_id": "P0131", "citations": ["Huang2025Retrieval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0131#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0166-9abcf1bf8a", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "paper_id": "P0166", "citations": ["Verma2026Active"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0166#key_results[1]"}}, {"evidence_id": "E-P0084-dc7fe955d8", "text": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "paper_id": "P0084", "citations": ["Kang2025Distilling"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]"}}, {"evidence_id": "E-P0191-3e2348e239", "text": "Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B).", "paper_id": "P0191", "citations": ["Pham2025Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0191#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Memory and retrieval (RAG)'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox"]}], "claim_candidates": [{"claim": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Large"]}, {"claim": "In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.", "evidence_field": "evidence_snippet", "citations": ["Kang2025Distilling"]}, {"claim": "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability.", "evidence_field": "evidence_snippet", "citations": ["Huang2025Retrieval"]}, {"claim": "To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design.", "evidence_field": "evidence_snippet", "citations": ["Abbineni2025Muallm"]}, {"claim": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "evidence_field": "evidence_snippet", "citations": ["Tawosi2025Meta"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0166`, `P0167`, `P0177`", "B_papers": "Memory / retrieval augmentation: `P0166`, `P0167`, `P0177`", "A_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 2}, {"paper_id": "P0030", "evidence_id": "E-P0030-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 2}, {"paper_id": "P0030", "evidence_id": "E-P0030-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Kang2025Distilling", "Zhang2025Large"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0166`, `P0167`, `P0177`", "B_papers": "Memory / retrieval augmentation: `P0166`, `P0167`, `P0177`", "A_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 1}, {"paper_id": "P0166", "evidence_id": "E-P0166-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0166#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 1}, {"paper_id": "P0032", "evidence_id": "E-P0032-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Kang2025Distilling", "Verma2026Active", "Tawosi2025Meta"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0166`, `P0167`, `P0177`", "B_papers": "Memory / retrieval augmentation: `P0166`, `P0167`, `P0177`", "A_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 2}, {"paper_id": "P0030", "evidence_id": "E-P0030-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 2}, {"paper_id": "P0030", "evidence_id": "E-P0030-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'tool interface contract (schemas / protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Kang2025Distilling", "Zhang2025Large"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0166`, `P0167`, `P0177`", "B_papers": "Memory / retrieval augmentation: `P0166`, `P0167`, `P0177`", "A_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 2}, {"paper_id": "P0030", "evidence_id": "E-P0030-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 2}, {"paper_id": "P0030", "evidence_id": "E-P0030-897bcc2f50", "excerpt": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc.", "citations": ["Zhang2025Large"], "pointer": "papers/paper_notes.jsonl:paper_id=P0030#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Kang2025Distilling", "Zhang2025Large"]}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "A_papers": "Agent frameworks / architectures: `P0166`, `P0167`, `P0177`", "B_papers": "Memory / retrieval augmentation: `P0166`, `P0167`, `P0177`", "A_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 1}, {"paper_id": "P0166", "evidence_id": "E-P0166-9abcf1bf8a", "excerpt": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "pointer": "papers/paper_notes.jsonl:paper_id=P0166#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0084", "evidence_id": "E-P0084-dc7fe955d8", "excerpt": "Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents.", "citations": ["Kang2025Distilling"], "pointer": "papers/paper_notes.jsonl:paper_id=P0084#key_results[1]", "score": 1}, {"paper_id": "P0032", "evidence_id": "E-P0032-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "sandboxing / permissions / observability", "citations": ["Kang2025Distilling", "Verma2026Active", "Tawosi2025Meta"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: SWE-bench; LTM; STM; GRPO; AgeMem; MEM; LoCoMo; AI-based; RAG; AutoCAD.", "citations": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox", "Zhang2025Large"]}], "failures_limitations": [{"bullet": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "citations": ["Yu2026Agentic"]}, {"bullet": "This results in the failure to retrieve the relevant code of these fine-grained subtasks.", "citations": ["Li2025Graphcodeagent"]}, {"bullet": "To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations.", "citations": ["Li2025Graphcodeagent"]}, {"bullet": "MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection.", "citations": ["Wu2025Meta"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "evidence_ids": ["E-P0089-37f9ea924c", "E-P0055-67ea29ce26", "E-P0046-4da9e4ae32", "E-P0036-2e6956a116", "E-P0057-60cc0d458f", "E-P0055-9980bf7642", "E-P0001-ca4a00b5cf", "E-P0046-d5c234444e", "E-P0035-6273763a98", "E-P0055-499402e2fb", "E-P0092-74188ef933", "E-P0092-91a368737e", "E-P0036-17d0e7f9d9", "E-P0035-c32e04ef1b", "E-P0062-a68f39bc04", "E-P0062-e012f792c6", "E-P0089-e973488a75", "E-P0090-1a05555e85"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0089-37f9ea924c", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling.", "paper_id": "P0089", "citations": ["Mohammadi2025Evaluation"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}}, {"evidence_id": "E-P0055-67ea29ce26", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "paper_id": "P0055", "citations": ["Li2026Autonomous"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#method"}}, {"evidence_id": "E-P0046-4da9e4ae32", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "paper_id": "P0046", "citations": ["Du2024Anytool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0046#limitations[1]"}}, {"evidence_id": "E-P0036-2e6956a116", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "paper_id": "P0036", "citations": ["Zhou2025Self"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]"}}, {"evidence_id": "E-P0057-60cc0d458f", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "paper_id": "P0057", "citations": ["Zhang2026Evoroute"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[0]"}}, {"evidence_id": "E-P0055-9980bf7642", "text": "Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential.", "paper_id": "P0055", "citations": ["Li2026Autonomous"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[1]"}}, {"evidence_id": "E-P0001-ca4a00b5cf", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "paper_id": "P0001", "citations": ["Yao2022React"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}}, {"evidence_id": "E-P0046-d5c234444e", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "paper_id": "P0046", "citations": ["Du2024Anytool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[0]"}}, {"evidence_id": "E-P0035-6273763a98", "text": "Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "paper_id": "P0035", "citations": ["Xia2025Sand"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0035#key_results[0]"}}, {"evidence_id": "E-P0055-499402e2fb", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "paper_id": "P0055", "citations": ["Li2026Autonomous"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0055#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Self-improvement and adaptation'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; communication protocol / roles; aggregation (vote / debate / referee); stability / robustness.", "citations": ["Li2026Autonomous", "Zhang2026Evoroute", "Xia2025Sand"]}], "claim_candidates": [{"claim": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation", "evidence_field": "evidence_snippet", "citations": ["Mohammadi2025Evaluation"]}, {"claim": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "evidence_field": "evidence_snippet", "citations": ["Li2026Autonomous"]}, {"claim": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "evidence_field": "evidence_snippet", "citations": ["Du2024Anytool"]}, {"claim": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "evidence_field": "evidence_snippet", "citations": ["Zhou2025Self"]}, {"claim": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "evidence_field": "evidence_snippet", "citations": ["Zhang2026Evoroute"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0055`, `P0057`, `P0035`", "B_papers": "Tool-use and function calling: `P0037`, `P0062`, `P0046`", "A_highlights": [{"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 3}, {"paper_id": "P0057", "evidence_id": "E-P0057-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0046", "evidence_id": "E-P0046-d5c234444e", "excerpt": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[0]", "score": 3}, {"paper_id": "P0046", "evidence_id": "E-P0046-4da9e4ae32", "excerpt": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#limitations[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Du2024Anytool"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0055`, `P0057`, `P0035`", "B_papers": "Tool-use and function calling: `P0037`, `P0062`, `P0046`", "A_highlights": [{"paper_id": "P0057", "evidence_id": "E-P0057-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[0]", "score": 3}, {"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0046", "evidence_id": "E-P0046-d5c234444e", "excerpt": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[0]", "score": 1}, {"paper_id": "P0046", "evidence_id": "E-P0046-4da9e4ae32", "excerpt": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#limitations[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2026Evoroute", "Zhou2025Self", "Du2024Anytool"]}, {"axis": "communication protocol / roles", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0055`, `P0057`, `P0035`", "B_papers": "Tool-use and function calling: `P0037`, `P0062`, `P0046`", "A_highlights": [{"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 2}, {"paper_id": "P0057", "evidence_id": "E-P0057-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0046", "evidence_id": "E-P0046-d5c234444e", "excerpt": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[0]", "score": 2}, {"paper_id": "P0046", "evidence_id": "E-P0046-4da9e4ae32", "excerpt": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#limitations[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'communication protocol / roles'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "communication protocol / roles", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Du2024Anytool"]}, {"axis": "aggregation (vote / debate / referee)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0055`, `P0057`, `P0035`", "B_papers": "Tool-use and function calling: `P0037`, `P0062`, `P0046`", "A_highlights": [{"paper_id": "P0057", "evidence_id": "E-P0057-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[0]", "score": 1}, {"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0046", "evidence_id": "E-P0046-d5c234444e", "excerpt": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[0]", "score": 1}, {"paper_id": "P0046", "evidence_id": "E-P0046-4da9e4ae32", "excerpt": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#limitations[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'aggregation (vote / debate / referee)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "aggregation (vote / debate / referee)", "citations": ["Zhang2026Evoroute", "Zhou2025Self", "Du2024Anytool"]}, {"axis": "stability / robustness", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "A_papers": "Agent frameworks / architectures: `P0055`, `P0057`, `P0035`", "B_papers": "Tool-use and function calling: `P0037`, `P0062`, `P0046`", "A_highlights": [{"paper_id": "P0057", "evidence_id": "E-P0057-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0057#key_results[0]", "score": 1}, {"paper_id": "P0036", "evidence_id": "E-P0036-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0036#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0046", "evidence_id": "E-P0046-d5c234444e", "excerpt": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#key_results[0]", "score": 1}, {"paper_id": "P0046", "evidence_id": "E-P0046-4da9e4ae32", "excerpt": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0046#limitations[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'stability / robustness'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "stability / robustness", "citations": ["Zhang2026Evoroute", "Zhou2025Self", "Du2024Anytool"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: DeepSeek-V3; LLMs; GAIA; EvoRoute; BrowseComp; SAND; ReAct-style; TauBench; MCP; LLM-based.", "citations": ["Li2026Autonomous", "Zhang2026Evoroute", "Xia2025Sand", "Zhou2025Self"]}], "failures_limitations": [{"bullet": "Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "citations": ["Li2026Autonomous"]}, {"bullet": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"]}, {"bullet": "The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks.", "citations": ["Zhou2025Self"]}, {"bullet": "The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "citations": ["Sarkar2025Survey"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "evidence_ids": ["E-P0101-35271418ac", "E-P0122-32b2c8cf19", "E-P0158-c92ed293ba", "E-P0158-9640816b42", "E-P0188-1063eee7ce", "E-P0041-9af6a59afb", "E-P0105-1d5f67b08e", "E-P0122-5ed988eb67", "E-P0076-baa622fa7f", "E-P0105-ddd045953e", "E-P0129-15e523063d", "E-P0205-e31a1bbba7", "E-P0101-38dc800de9", "E-P0076-b35b53de13", "E-P0129-171b93237a", "E-P0158-8158d9909c", "E-P0164-1c25e10ffc", "E-P0213-837ee80fad"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0101-35271418ac", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "paper_id": "P0101", "citations": ["Lumer2025Memtool"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[0]"}}, {"evidence_id": "E-P0122-32b2c8cf19", "text": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation.", "paper_id": "P0122", "citations": ["Cao2025Skyrl"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method"}}, {"evidence_id": "E-P0158-c92ed293ba", "text": "While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models.", "paper_id": "P0158", "citations": ["Shen2024Small"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#limitations[1]"}}, {"evidence_id": "E-P0158-9640816b42", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "paper_id": "P0158", "citations": ["Shen2024Small"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[1]"}}, {"evidence_id": "E-P0188-1063eee7ce", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "paper_id": "P0188", "citations": ["Zhao2025Achieving"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0188#key_results[0]"}}, {"evidence_id": "E-P0041-9af6a59afb", "text": "Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "paper_id": "P0041", "citations": ["Ji2025Tree"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0041#key_results[0]"}}, {"evidence_id": "E-P0105-1d5f67b08e", "text": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "paper_id": "P0105", "citations": ["Ghose2025Orfs"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0105#key_results[0]"}}, {"evidence_id": "E-P0122-5ed988eb67", "text": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency.", "paper_id": "P0122", "citations": ["Cao2025Skyrl"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[1]"}}, {"evidence_id": "E-P0076-baa622fa7f", "text": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "paper_id": "P0076", "citations": ["Silva2025Agents"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]"}}, {"evidence_id": "E-P0105-ddd045953e", "text": "ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics.", "paper_id": "P0105", "citations": ["Ghose2025Orfs"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0105#key_results[1]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Ji2025Tree", "Silva2025Agents", "Lumer2025Memtool"]}], "claim_candidates": [{"claim": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "evidence_field": "evidence_snippet", "citations": ["Lumer2025Memtool"]}, {"claim": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation.", "evidence_field": "evidence_snippet", "citations": ["Cao2025Skyrl"]}, {"claim": "While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models.", "evidence_field": "evidence_snippet", "citations": ["Shen2024Small"]}, {"claim": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "evidence_field": "evidence_snippet", "citations": ["Shen2024Small"]}, {"claim": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "evidence_field": "evidence_snippet", "citations": ["Zhao2025Achieving"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0041`, `P0076`, `P0101`", "B_papers": "Multi-agent coordination: `P0076`, `P0187`, `P0205`", "A_highlights": [{"paper_id": "P0101", "evidence_id": "E-P0101-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[0]", "score": 4}, {"paper_id": "P0122", "evidence_id": "E-P0122-5ed988eb67", "excerpt": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[1]", "score": 2}], "B_highlights": [{"paper_id": "P0076", "evidence_id": "E-P0076-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Lumer2025Memtool", "Cao2025Skyrl", "Silva2025Agents"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0041`, `P0076`, `P0101`", "B_papers": "Multi-agent coordination: `P0076`, `P0187`, `P0205`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-5ed988eb67", "excerpt": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[1]", "score": 4}, {"paper_id": "P0122", "evidence_id": "E-P0122-32b2c8cf19", "excerpt": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation.", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#method", "score": 4}], "B_highlights": [{"paper_id": "P0076", "evidence_id": "E-P0076-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Cao2025Skyrl", "Silva2025Agents"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0041`, `P0076`, `P0101`", "B_papers": "Multi-agent coordination: `P0076`, `P0187`, `P0205`", "A_highlights": [{"paper_id": "P0101", "evidence_id": "E-P0101-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[0]", "score": 3}, {"paper_id": "P0122", "evidence_id": "E-P0122-5ed988eb67", "excerpt": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[1]", "score": 2}], "B_highlights": [{"paper_id": "P0076", "evidence_id": "E-P0076-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'tool interface contract (schemas / protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Lumer2025Memtool", "Cao2025Skyrl", "Silva2025Agents"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0041`, `P0076`, `P0101`", "B_papers": "Multi-agent coordination: `P0076`, `P0187`, `P0205`", "A_highlights": [{"paper_id": "P0101", "evidence_id": "E-P0101-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0101#key_results[0]", "score": 3}, {"paper_id": "P0122", "evidence_id": "E-P0122-5ed988eb67", "excerpt": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[1]", "score": 2}], "B_highlights": [{"paper_id": "P0076", "evidence_id": "E-P0076-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Lumer2025Memtool", "Cao2025Skyrl", "Silva2025Agents"]}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "A_papers": "Agent frameworks / architectures: `P0041`, `P0076`, `P0101`", "B_papers": "Multi-agent coordination: `P0076`, `P0187`, `P0205`", "A_highlights": [{"paper_id": "P0122", "evidence_id": "E-P0122-5ed988eb67", "excerpt": "We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and", "citations": ["Cao2025Skyrl"], "pointer": "papers/paper_notes.jsonl:paper_id=P0122#key_results[1]", "score": 1}, {"paper_id": "P0105", "evidence_id": "E-P0105-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0105#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0076", "evidence_id": "E-P0076-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[1]", "score": 0}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "sandboxing / permissions / observability", "citations": ["Cao2025Skyrl", "Ghose2025Orfs", "Silva2025Agents"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: LLMs; GRPO; MCP; MemTool; ScaleMCP; ORFS-agent; LLM-based; RFS-agent; SA-SWE-32B; AST-based.", "citations": ["Ji2025Tree", "Silva2025Agents", "Lumer2025Memtool", "Ghose2025Orfs"]}], "failures_limitations": [{"bullet": "To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step.", "citations": ["Ji2025Tree"]}, {"bullet": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"]}, {"bullet": "Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.", "citations": ["Papadakis2025Atlas"]}, {"bullet": "While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics.", "citations": ["Chuang2025Debate"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "evidence_ids": ["E-P0111-753416ce70", "E-P0111-8b3195ff10", "E-P0098-c1e63c5b8f", "E-P0033-68db58914f", "E-P0089-37f9ea924c", "E-P0136-38a26e4777", "E-P0111-2895472ae1", "E-P0072-a0b404d928", "E-P0039-5607dc887c", "E-P0014-79f88927fa", "E-P0040-4fc221fdea", "E-P0174-8b56718f74", "E-P0179-772eb45970", "E-P0014-593af2dd94", "E-P0039-e2d4798c18", "E-P0043-5603d51445", "E-P0043-3c01157d9b", "E-P0089-e973488a75"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0111-753416ce70", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "paper_id": "P0111", "citations": ["Fu2025Eval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]"}}, {"evidence_id": "E-P0111-8b3195ff10", "text": "To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution.", "paper_id": "P0111", "citations": ["Fu2025Eval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0111#method"}}, {"evidence_id": "E-P0098-c1e63c5b8f", "text": "To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training.", "paper_id": "P0098", "citations": ["Wang2025Flow"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}}, {"evidence_id": "E-P0033-68db58914f", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "paper_id": "P0033", "citations": ["Shi2025Progent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]"}}, {"evidence_id": "E-P0089-37f9ea924c", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling.", "paper_id": "P0089", "citations": ["Mohammadi2025Evaluation"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]"}}, {"evidence_id": "E-P0136-38a26e4777", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "paper_id": "P0136", "citations": ["Shang2024Agentsquare"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]"}}, {"evidence_id": "E-P0111-2895472ae1", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "paper_id": "P0111", "citations": ["Fu2025Eval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[0]"}}, {"evidence_id": "E-P0072-a0b404d928", "text": "Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution.", "paper_id": "P0072", "citations": ["Mo2025Attractive"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0072#key_results[0]"}}, {"evidence_id": "E-P0039-5607dc887c", "text": "Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses.", "paper_id": "P0039", "citations": ["Ji2025Taxonomy"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0039#key_results[1]"}}, {"evidence_id": "E-P0014-79f88927fa", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "paper_id": "P0014", "citations": ["Kim2026Beyond"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Kim2026Beyond", "Liu2026Agents", "Engelberg2026Sola"]}], "claim_candidates": [{"claim": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "evidence_field": "evidence_snippet", "citations": ["Fu2025Eval"]}, {"claim": "To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution.", "evidence_field": "evidence_snippet", "citations": ["Fu2025Eval"]}, {"claim": "To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training.", "evidence_field": "evidence_snippet", "citations": ["Wang2025Flow"]}, {"claim": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "evidence_field": "evidence_snippet", "citations": ["Shi2025Progent"]}, {"claim": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation", "evidence_field": "evidence_snippet", "citations": ["Mohammadi2025Evaluation"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0174`, `P0179`", "B_papers": "Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`", "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}, {"paper_id": "P0014", "evidence_id": "E-P0014-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0089", "evidence_id": "E-P0089-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 5}, {"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Shi2025Progent", "Kim2026Beyond", "Mohammadi2025Evaluation", "Fu2025Eval"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0174`, `P0179`", "B_papers": "Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`", "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}, {"paper_id": "P0014", "evidence_id": "E-P0014-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 2}], "B_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 2}, {"paper_id": "P0089", "evidence_id": "E-P0089-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Shi2025Progent", "Kim2026Beyond", "Mohammadi2025Evaluation"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0174`, `P0179`", "B_papers": "Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`", "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 2}, {"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}, {"paper_id": "P0089", "evidence_id": "E-P0089-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'tool interface contract (schemas / protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Kim2026Beyond", "Shi2025Progent", "Fu2025Eval", "Mohammadi2025Evaluation"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0174`, `P0179`", "B_papers": "Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`", "A_highlights": [{"paper_id": "P0014", "evidence_id": "E-P0014-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0014#key_results[0]", "score": 2}, {"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}, {"paper_id": "P0089", "evidence_id": "E-P0089-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Kim2026Beyond", "Shi2025Progent", "Fu2025Eval", "Mohammadi2025Evaluation"]}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "A_papers": "Agent frameworks / architectures: `P0014`, `P0174`, `P0179`", "B_papers": "Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`", "A_highlights": [{"paper_id": "P0033", "evidence_id": "E-P0033-68db58914f", "excerpt": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "pointer": "papers/paper_notes.jsonl:paper_id=P0033#key_results[0]", "score": 2}, {"paper_id": "P0039", "evidence_id": "E-P0039-5607dc887c", "excerpt": "Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses.", "citations": ["Ji2025Taxonomy"], "pointer": "papers/paper_notes.jsonl:paper_id=P0039#key_results[1]", "score": 1}], "B_highlights": [{"paper_id": "P0089", "evidence_id": "E-P0089-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0089#key_results[0]", "score": 2}, {"paper_id": "P0111", "evidence_id": "E-P0111-2895472ae1", "excerpt": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[0]", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "sandboxing / permissions / observability", "citations": ["Shi2025Progent", "Ji2025Taxonomy", "Mohammadi2025Evaluation", "Fu2025Eval"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: API; LLMs; WildAGTEval; ISPM; AWS; ASB; AgentDojo; AgentPoison; IPI; IPI-centric.", "citations": ["Kim2026Beyond", "Liu2026Agents", "Engelberg2026Sola", "Shi2025Progent"]}], "failures_limitations": [{"bullet": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "citations": ["Liu2026Agents"]}, {"bullet": "Identity Security Posture Management (ISPM) is a core challenge for modern enterprises operating across cloud and SaaS environments.", "citations": ["Engelberg2026Sola"]}, {"bullet": "Overall, this work provides a practical and reproducible benchmark for evaluating agentic AI systems in identity security and establishes a foundation for future ISPM benchmarks covering more advanced identity analysis and governance tasks.", "citations": ["Engelberg2026Sola"]}, {"bullet": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "citations": ["Shi2025Progent"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "evidence_ids": ["E-P0097-d6095e10e9", "E-P0097-8bcb673a7d", "E-P0219-a256400826", "E-P0111-753416ce70", "E-P0114-e0345118bc", "E-P0097-7a6ec4daed", "E-P0111-2895472ae1", "E-P0189-8512d7eecf", "E-P0189-ee6f97d5e5", "E-P0079-7edb91824f", "E-P0023-8e34a29629", "E-P0112-c664716bd8", "E-P0135-e26328e18c", "E-P0135-51b52c66a1", "E-P0179-772eb45970", "E-P0132-3cd6217549", "E-P0220-2718dbc45e", "E-P0023-8a2c2e2291"], "evidence_level_summary": {"fulltext": 0, "abstract": 18, "title": 0}, "evidence_snippets": [{"evidence_id": "E-P0097-d6095e10e9", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP).", "paper_id": "P0097", "citations": ["Zhang2025Security"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]"}}, {"evidence_id": "E-P0097-8bcb673a7d", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "paper_id": "P0097", "citations": ["Zhang2025Security"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0097#method"}}, {"evidence_id": "E-P0219-a256400826", "text": "We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions.", "paper_id": "P0219", "citations": ["Lichkovski2025Agent"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0219#limitations[1]"}}, {"evidence_id": "E-P0111-753416ce70", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "paper_id": "P0111", "citations": ["Fu2025Eval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]"}}, {"evidence_id": "E-P0114-e0345118bc", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents.", "paper_id": "P0114", "citations": ["Kale2025Reliable"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}}, {"evidence_id": "E-P0097-7a6ec4daed", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "paper_id": "P0097", "citations": ["Zhang2025Security"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[1]"}}, {"evidence_id": "E-P0111-2895472ae1", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "paper_id": "P0111", "citations": ["Fu2025Eval"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[0]"}}, {"evidence_id": "E-P0189-8512d7eecf", "text": "Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks.", "paper_id": "P0189", "citations": ["Wang2025Adversarial"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[0]"}}, {"evidence_id": "E-P0189-ee6f97d5e5", "text": "However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections.", "paper_id": "P0189", "citations": ["Wang2025Adversarial"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0189#key_results[1]"}}, {"evidence_id": "E-P0079-7edb91824f", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "paper_id": "P0079", "citations": ["Bonagiri2025Check"], "provenance": {"evidence_level": "abstract", "source": "paper_notes", "pointer": "papers/paper_notes.jsonl:paper_id=P0079#key_results[0]"}}], "definitions_setup": [{"bullet": "Setup: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Safety, security, and governance'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability.", "citations": ["Engelberg2026Sola", "Gasmi2025Bridging", "Hadeliya2025When"]}], "claim_candidates": [{"claim": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Security"]}, {"claim": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "evidence_field": "evidence_snippet", "citations": ["Zhang2025Security"]}, {"claim": "We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions.", "evidence_field": "evidence_snippet", "citations": ["Lichkovski2025Agent"]}, {"claim": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "evidence_field": "evidence_snippet", "citations": ["Fu2025Eval"]}, {"claim": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents.", "evidence_field": "evidence_snippet", "citations": ["Kale2025Reliable"]}], "concrete_comparisons": [{"axis": "evaluation protocol (datasets, metrics, human evaluation)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0179`, `P0023`, `P0042`", "B_papers": "Safety / security / guardrails: `P0179`, `P0023`, `P0042`", "A_highlights": [{"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 4}, {"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}], "B_highlights": [{"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 4}, {"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'evaluation protocol (datasets, metrics, human evaluation)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "evaluation protocol (datasets, metrics, human evaluation)", "citations": ["Zhang2025Security", "Fu2025Eval"]}, {"axis": "compute and latency constraints", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0179`, `P0023`, `P0042`", "B_papers": "Safety / security / guardrails: `P0179`, `P0023`, `P0042`", "A_highlights": [{"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 2}, {"paper_id": "P0111", "evidence_id": "E-P0111-2895472ae1", "excerpt": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[0]", "score": 1}], "B_highlights": [{"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 2}, {"paper_id": "P0111", "evidence_id": "E-P0111-2895472ae1", "excerpt": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[0]", "score": 1}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'compute and latency constraints'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "compute and latency constraints", "citations": ["Zhang2025Security", "Fu2025Eval"]}, {"axis": "tool interface contract (schemas / protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0179`, `P0023`, `P0042`", "B_papers": "Safety / security / guardrails: `P0179`, `P0023`, `P0042`", "A_highlights": [{"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}, {"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}, {"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 3}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'tool interface contract (schemas / protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool interface contract (schemas / protocols)", "citations": ["Fu2025Eval", "Zhang2025Security"]}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0179`, `P0023`, `P0042`", "B_papers": "Safety / security / guardrails: `P0179`, `P0023`, `P0042`", "A_highlights": [{"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}, {"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 3}], "B_highlights": [{"paper_id": "P0111", "evidence_id": "E-P0111-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0111#key_results[1]", "score": 4}, {"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 3}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "tool selection / routing policy", "citations": ["Fu2025Eval", "Zhang2025Security"]}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "A_papers": "Agent frameworks / architectures: `P0179`, `P0023`, `P0042`", "B_papers": "Safety / security / guardrails: `P0179`, `P0023`, `P0042`", "A_highlights": [{"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 4}, {"paper_id": "P0097", "evidence_id": "E-P0097-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#method", "score": 2}], "B_highlights": [{"paper_id": "P0097", "evidence_id": "E-P0097-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#key_results[0]", "score": 4}, {"paper_id": "P0097", "evidence_id": "E-P0097-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0097#method", "score": 2}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence.", "evidence_field": "sandboxing / permissions / observability", "citations": ["Zhang2025Security"]}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: ISPM; AWS; AI-specific; MCP; JSON; LLM-centric; LLMs; GPT-4; CIA; AGrail.", "citations": ["Engelberg2026Sola", "Gasmi2025Bridging", "Hadeliya2025When", "Luo2025Agrail"]}], "failures_limitations": [{"bullet": "Identity Security Posture Management (ISPM) is a core challenge for modern enterprises operating across cloud and SaaS environments.", "citations": ["Engelberg2026Sola"]}, {"bullet": "Overall, this work provides a practical and reproducible benchmark for evaluating agentic AI systems in identity security and establishes a foundation for future ISPM benchmarks covering more advanced identity analysis and governance tasks.", "citations": ["Engelberg2026Sola"]}, {"bullet": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"]}, {"bullet": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"]}], "blocking_missing": [], "verify_fields": ["named benchmarks/datasets used", "metrics/human-eval protocol", "compute/training/inference cost", "training data and supervision signal", "baseline choices and ablation evidence"], "generated_at": "2026-01-25T04:54:07"}
