**Index Table 1. Subsection map (axes + representative works).**

| Subsection | Axes | Representative works |
|---|---|---|
| 3.1 Agent loop and action spaces | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Li2025Agentswift; @Liu2025Mcpagentbench; @Shang2024Agentsquare; @Feng2025Group; @Xi2026Toolgym] |
| 3.2 Tool interfaces and orchestration | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Zhou2026Beyond; @Dong2025Etom; @Du2024Anytool; @Zhou2025Self; @Liu2025Toolscope] |
| 4.1 Planning and reasoning loops | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Shi2025Progent; @Kim2025Bridging; @Hu2025Training; @Yao2022React; @Shi2024Ehragent] |
| 4.2 Memory and retrieval (RAG) | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Zhang2025Large; @Kang2025Distilling; @Huang2025Retrieval; @Abbineni2025Muallm; @Tawosi2025Meta] |
| 5.1 Self-improvement and adaptation | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>communication protocol / roles<br>aggregation (vote / debate / referee)<br>stability / robustness | [@Mohammadi2025Evaluation; @Li2026Autonomous; @Du2024Anytool; @Zhou2025Self; @Zhang2026Evoroute] |
| 5.2 Multi-agent coordination | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Lumer2025Memtool; @Cao2025Skyrl; @Shen2024Small; @Zhao2025Achieving; @Ji2025Tree] |
| 6.1 Benchmarks and evaluation protocols | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Fu2025Eval; @Wang2025Flow; @Shi2025Progent; @Mohammadi2025Evaluation; @Shang2024Agentsquare] |
| 6.2 Safety, security, and governance | evaluation protocol (datasets, metrics, human evaluation)<br>compute and latency constraints<br>tool interface contract (schemas / protocols)<br>tool selection / routing policy<br>sandboxing / permissions / observability | [@Zhang2025Security; @Lichkovski2025Agent; @Fu2025Eval; @Kale2025Reliable; @Wang2025Adversarial] |

**Index Table 2. Concrete anchors (benchmarks / numbers / caveats).**

| Subsection | Anchor facts | Representative works |
|---|---|---|
| 3.1 Agent loop and action spaces | Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discover<br>Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 1<br>Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show | [@Li2025Agentswift; @Xi2026Toolgym; @Shang2024Agentsquare] |
| 3.2 Tool interfaces and orchestration | Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework ach<br>Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates<br>In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels | [@Zhou2025Self; @Zhou2026Beyond; @Khatchadourian2026Replayable] |
| 4.1 Planning and reasoning loops | Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn<br>Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that<br>Our best performing strategy generates executable API calls 88% of the time | [@Hu2025Training; @Shi2025Progent; @Mudur2025Feabench] |
| 4.2 Memory and retrieval (RAG) | Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3<br>Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on<br>Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense code | [@Kang2025Distilling; @Verma2026Active; @Tawosi2025Meta] |
| 5.1 Self-improvement and adaptation | Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework ach<br>Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the<br>Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 v | [@Zhou2025Self; @Zhang2026Evoroute; @Du2024Anytool] |
| 5.2 Multi-agent coordination | Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interac<br>We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchrono<br>Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve | [@Lumer2025Memtool; @Cao2025Skyrl; @Ghose2025Orfs] |
| 6.1 Benchmarks and evaluation protocols | Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that<br>Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval ac<br>This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy th | [@Shi2025Progent; @Kim2026Beyond; @Mohammadi2025Evaluation] |
| 6.2 Safety, security, and governance | MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in too<br>RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools impl<br>We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task co | [@Zhang2025Security; @Fu2025Eval] |
