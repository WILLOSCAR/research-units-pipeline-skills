# Evidence draft: 3.1 Agent loop and action spaces

## Evidence snippets (with provenance)
- (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
- (E-P0099-312a342670) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0099#method)
- (E-P0099-f7a14123f9) To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0099#limitations[1])
- (E-P0136-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
- (E-P0095-4b027dfb27) We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0095#key_results[0])
- (E-P0181-895b04aa5c) Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Xi2026Toolgym (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
- (E-P0181-8c9597d805) Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Xi2026Toolgym (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0181#key_results[1])
- (E-P0023-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
- (E-P0023-8a2c2e2291) Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Gasmi2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0023#key_results[1])
- (E-P0083-39281e5083) Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. Zhang2025Datascibench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#key_results[1])

## Definitions / setup

- Setup: Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Agent loop and action spaces'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Zhang2026Evoroute Song2026Envscaler Soliman2026Intagent

## Claim candidates

- Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift
- To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench
- To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. Liu2025Mcpagentbench
- Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare
- We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Feng2025Group

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0057`, `P0170`, `P0173`; B: Planning / reasoning loops: `P0024`, `P0070`, `P0200`. Li2025Agentswift Xi2026Toolgym Shang2024Agentsquare
  - A highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
  - A highlight: (E-P0181-8c9597d805) Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Xi2026Toolgym (pointer: papers/paper_notes.jsonl:paper_id=P0181#key_results[1])
  - B highlight: (E-P0136-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
  - B highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0057`, `P0170`, `P0173`; B: Planning / reasoning loops: `P0024`, `P0070`, `P0200`. Xi2026Toolgym Li2025Agentswift Shang2024Agentsquare
  - A highlight: (E-P0181-895b04aa5c) Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Xi2026Toolgym (pointer: papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
  - A highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
  - B highlight: (E-P0136-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
  - B highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0057`, `P0170`, `P0173`; B: Planning / reasoning loops: `P0024`, `P0070`, `P0200`. Gasmi2025Bridging Li2025Agentswift Shang2024Agentsquare
  - A highlight: (E-P0023-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - A highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
  - B highlight: (E-P0136-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
  - B highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0057`, `P0170`, `P0173`; B: Planning / reasoning loops: `P0024`, `P0070`, `P0200`. Gasmi2025Bridging Li2025Agentswift Shang2024Agentsquare
  - A highlight: (E-P0023-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - A highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
  - B highlight: (E-P0136-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
  - B highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0057`, `P0170`, `P0173`; B: Planning / reasoning loops: `P0024`, `P0070`, `P0200`. Gasmi2025Bridging Shang2024Agentsquare Li2025Agentswift
  - A highlight: (E-P0023-8e34a29629) Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - A highlight: (E-P0023-8a2c2e2291) Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Gasmi2025Bridging (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[1])
  - B highlight: (E-P0136-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (pointer: papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
  - B highlight: (E-P0070-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0070#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; GAIA; EvoRoute; BrowseComp; LLM-simulated; SFT; RUC-NLPIR; EnvScaler; SkelBuilder; ScenGenerator. Zhang2026Evoroute Song2026Envscaler Soliman2026Intagent Xi2026Toolgym

## Failures / limitations

- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. Gasmi2025Bridging
- This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. Gasmi2025Bridging
- We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Gasmi2025Bridging

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
