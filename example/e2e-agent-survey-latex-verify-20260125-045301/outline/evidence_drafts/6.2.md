# Evidence draft: 6.2 Safety, security, and governance

## Evidence snippets (with provenance)
- (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
- (E-P0097-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0097#method)
- (E-P0219-a256400826) We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. Lichkovski2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0219#limitations[1])
- (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
- (E-P0114-e0345118bc) To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. Kale2025Reliable (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0114#key_results[0])
- (E-P0097-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0097#key_results[1])
- (E-P0111-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0111#key_results[0])
- (E-P0189-8512d7eecf) Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. Wang2025Adversarial (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0189#key_results[0])
- (E-P0189-ee6f97d5e5) However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. Wang2025Adversarial (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0189#key_results[1])
- (E-P0079-7edb91824f) Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Bonagiri2025Check (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0079#key_results[0])

## Definitions / setup

- Setup: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Safety, security, and governance'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Engelberg2026Sola Gasmi2025Bridging Hadeliya2025When

## Claim candidates

- MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP Zhang2025Security
- We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security
- We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. Lichkovski2025Agent
- RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval
- To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. Kale2025Reliable

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0179`, `P0023`, `P0042`; B: Safety / security / guardrails: `P0179`, `P0023`, `P0042`. Zhang2025Security Fu2025Eval
  - A highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - A highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
  - B highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - B highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0179`, `P0023`, `P0042`; B: Safety / security / guardrails: `P0179`, `P0023`, `P0042`. Zhang2025Security Fu2025Eval
  - A highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - A highlight: (E-P0111-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[0])
  - B highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - B highlight: (E-P0111-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0179`, `P0023`, `P0042`; B: Safety / security / guardrails: `P0179`, `P0023`, `P0042`. Fu2025Eval Zhang2025Security
  - A highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
  - A highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - B highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
  - B highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0179`, `P0023`, `P0042`; B: Safety / security / guardrails: `P0179`, `P0023`, `P0042`. Fu2025Eval Zhang2025Security
  - A highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
  - A highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - B highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
  - B highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0179`, `P0023`, `P0042`; B: Safety / security / guardrails: `P0179`, `P0023`, `P0042`. Zhang2025Security
  - A highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - A highlight: (E-P0097-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#method)
  - B highlight: (E-P0097-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#key_results[0])
  - B highlight: (E-P0097-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0097#method)

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: ISPM; AWS; AI-specific; MCP; JSON; LLM-centric; LLMs; GPT-4; CIA; AGrail. Engelberg2026Sola Gasmi2025Bridging Hadeliya2025When Luo2025Agrail

## Failures / limitations

- Identity Security Posture Management (ISPM) is a core challenge for modern enterprises operating across cloud and SaaS environments. Engelberg2026Sola
- Overall, this work provides a practical and reproducible benchmark for evaluating agentic AI systems in identity security and establishes a foundation for future ISPM benchmarks covering more advanced identity analysis and governance tasks. Engelberg2026Sola
- Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. Gasmi2025Bridging
- This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. Gasmi2025Bridging

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
