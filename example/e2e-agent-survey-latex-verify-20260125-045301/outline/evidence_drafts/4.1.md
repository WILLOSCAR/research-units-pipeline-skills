# Evidence draft: 4.1 Planning and reasoning loops

## Evidence snippets (with provenance)
- (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
- (E-P0024-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0024#method)
- (E-P0033-6829c8b583) Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0033#limitations[1])
- (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0144-2a7ea60588) Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. Shi2024Ehragent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0144#key_results[0])
- (E-P0159-c0a98eb625) We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4). Ji2024Testing (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0159#key_results[0])
- (E-P0033-4792fb4704) The core problem that enables attacks to succeed lies in over-privileged tool access. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0033#summary_bullets[2])
- (E-P0012-8517628bd0) While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0012#summary_bullets[1])
- (E-P0091-e4ac5005b3) Our best performing strategy generates executable API calls 88% of the time. Mudur2025Feabench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0091#key_results[0])

## Definitions / setup

- Setup: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Planning and reasoning loops'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Zhou2025Reasoning Kim2025Bridging Shi2025Progent

## Claim candidates

- Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent
- We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Shi2025Progent
- Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training
- On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0012`, `P0024`, `P0033`; B: Planning / reasoning loops: `P0012`, `P0024`, `P0034`. Hu2025Training Shi2025Progent Mudur2025Feabench
  - A highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - B highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
  - B highlight: (E-P0091-e4ac5005b3) Our best performing strategy generates executable API calls 88% of the time. Mudur2025Feabench (pointer: papers/paper_notes.jsonl:paper_id=P0091#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0012`, `P0024`, `P0033`; B: Planning / reasoning loops: `P0012`, `P0024`, `P0034`. Hu2025Training Shi2025Progent Mudur2025Feabench
  - A highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - B highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
  - B highlight: (E-P0091-e4ac5005b3) Our best performing strategy generates executable API calls 88% of the time. Mudur2025Feabench (pointer: papers/paper_notes.jsonl:paper_id=P0091#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0012`, `P0024`, `P0033`; B: Planning / reasoning loops: `P0012`, `P0024`, `P0034`. Hu2025Training Shi2025Progent Mudur2025Feabench
  - A highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - B highlight: (E-P0091-e4ac5005b3) Our best performing strategy generates executable API calls 88% of the time. Mudur2025Feabench (pointer: papers/paper_notes.jsonl:paper_id=P0091#key_results[0])
  - B highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0012`, `P0024`, `P0033`; B: Planning / reasoning loops: `P0012`, `P0024`, `P0034`. Hu2025Training Shi2025Progent Mudur2025Feabench
  - A highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - B highlight: (E-P0091-e4ac5005b3) Our best performing strategy generates executable API calls 88% of the time. Mudur2025Feabench (pointer: papers/paper_notes.jsonl:paper_id=P0091#key_results[0])
  - B highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0012`, `P0024`, `P0033`; B: Planning / reasoning loops: `P0012`, `P0024`, `P0034`. Shi2025Progent Zhou2025Reasoning Hu2025Training
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - A highlight: (E-P0012-8517628bd0) While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning (pointer: papers/paper_notes.jsonl:paper_id=P0012#summary_bullets[1])
  - B highlight: (E-P0012-8517628bd0) While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning (pointer: papers/paper_notes.jsonl:paper_id=P0012#summary_bullets[1])
  - B highlight: (E-P0130-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0130#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: RSP; GSI; RSV; FEVER; RSP-M; HotpotQA; ReAct; SCL; CCAM; GPT-4o-powered. Zhou2025Reasoning Kim2025Bridging Shi2025Progent Hatalis2025Review

## Failures / limitations

- While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning
- We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. Zhou2025Reasoning
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Shi2025Progent
- LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Shi2025Progent

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
