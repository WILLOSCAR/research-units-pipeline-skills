# Evidence draft: 5.1 Self-improvement and adaptation

## Evidence snippets (with provenance)
- (E-P0089-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- (E-P0055-67ea29ce26) We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Li2026Autonomous (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0055#method)
- (E-P0046-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0046#limitations[1])
- (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
- (E-P0057-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0057#key_results[0])
- (E-P0055-9980bf7642) Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Li2026Autonomous (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0055#key_results[1])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0046-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0046#key_results[0])
- (E-P0035-6273763a98) Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches. Xia2025Sand (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0035#key_results[0])
- (E-P0055-499402e2fb) We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Li2026Autonomous (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0055#key_results[0])

## Definitions / setup

- Setup: Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Self-improvement and adaptation'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; communication protocol / roles; aggregation (vote / debate / referee); stability / robustness. Li2026Autonomous Zhang2026Evoroute Xia2025Sand

## Claim candidates

- This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation Mohammadi2025Evaluation
- We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Li2026Autonomous
- We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool
- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self
- Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0055`, `P0057`, `P0035`; B: Tool-use and function calling: `P0037`, `P0062`, `P0046`. Zhou2025Self Zhang2026Evoroute Du2024Anytool
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - A highlight: (E-P0057-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0057#key_results[0])
  - B highlight: (E-P0046-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#key_results[0])
  - B highlight: (E-P0046-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#limitations[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0055`, `P0057`, `P0035`; B: Tool-use and function calling: `P0037`, `P0062`, `P0046`. Zhang2026Evoroute Zhou2025Self Du2024Anytool
  - A highlight: (E-P0057-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0057#key_results[0])
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - B highlight: (E-P0046-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#key_results[0])
  - B highlight: (E-P0046-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#limitations[1])
- Axis: communication protocol / roles; A: Agent frameworks / architectures: `P0055`, `P0057`, `P0035`; B: Tool-use and function calling: `P0037`, `P0062`, `P0046`. Zhou2025Self Zhang2026Evoroute Du2024Anytool
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - A highlight: (E-P0057-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0057#key_results[0])
  - B highlight: (E-P0046-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#key_results[0])
  - B highlight: (E-P0046-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#limitations[1])
- Axis: aggregation (vote / debate / referee); A: Agent frameworks / architectures: `P0055`, `P0057`, `P0035`; B: Tool-use and function calling: `P0037`, `P0062`, `P0046`. Zhang2026Evoroute Zhou2025Self Du2024Anytool
  - A highlight: (E-P0057-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0057#key_results[0])
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - B highlight: (E-P0046-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#key_results[0])
  - B highlight: (E-P0046-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#limitations[1])
- Axis: stability / robustness; A: Agent frameworks / architectures: `P0055`, `P0057`, `P0035`; B: Tool-use and function calling: `P0037`, `P0062`, `P0046`. Zhang2026Evoroute Zhou2025Self Du2024Anytool
  - A highlight: (E-P0057-60cc0d458f) Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$. Zhang2026Evoroute (pointer: papers/paper_notes.jsonl:paper_id=P0057#key_results[0])
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - B highlight: (E-P0046-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#key_results[0])
  - B highlight: (E-P0046-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (pointer: papers/paper_notes.jsonl:paper_id=P0046#limitations[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: DeepSeek-V3; LLMs; GAIA; EvoRoute; BrowseComp; SAND; ReAct-style; TauBench; MCP; LLM-based. Li2026Autonomous Zhang2026Evoroute Xia2025Sand Zhou2025Self

## Failures / limitations

- Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures. Li2026Autonomous
- We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. Zhang2026Evoroute
- The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. Zhou2025Self
- The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems. Sarkar2025Survey

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
