# Evidence draft: 6.1 Benchmarks and evaluation protocols

## Evidence snippets (with provenance)
- (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
- (E-P0111-8b3195ff10) To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0111#method)
- (E-P0098-c1e63c5b8f) To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. Wang2025Flow (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
- (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
- (E-P0089-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- (E-P0136-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
- (E-P0111-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0111#key_results[0])
- (E-P0072-a0b404d928) Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\%-95\%) and significant privacy leakage, with negligible impact on primary task execution. Mo2025Attractive (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0072#key_results[0])
- (E-P0039-5607dc887c) Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Ji2025Taxonomy (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0039#key_results[1])
- (E-P0014-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0014#key_results[0])

## Definitions / setup

- Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Kim2026Beyond Liu2026Agents Engelberg2026Sola

## Claim candidates

- RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval
- To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. Fu2025Eval
- To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. Wang2025Flow
- Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent
- This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation Mohammadi2025Evaluation

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0014`, `P0174`, `P0179`; B: Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`. Shi2025Progent Kim2026Beyond Mohammadi2025Evaluation Fu2025Eval
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - A highlight: (E-P0014-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0089-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0014`, `P0174`, `P0179`; B: Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`. Shi2025Progent Kim2026Beyond Mohammadi2025Evaluation
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - A highlight: (E-P0014-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0014-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - B highlight: (E-P0089-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0014`, `P0174`, `P0179`; B: Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`. Kim2026Beyond Shi2025Progent Fu2025Eval Mohammadi2025Evaluation
  - A highlight: (E-P0014-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - B highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
  - B highlight: (E-P0089-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0014`, `P0174`, `P0179`; B: Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`. Kim2026Beyond Shi2025Progent Fu2025Eval Mohammadi2025Evaluation
  - A highlight: (E-P0014-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0014#key_results[0])
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - B highlight: (E-P0111-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[1])
  - B highlight: (E-P0089-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0014`, `P0174`, `P0179`; B: Evaluation / benchmark-focused works: `P0014`, `P0179`, `P0039`. Shi2025Progent Ji2025Taxonomy Mohammadi2025Evaluation Fu2025Eval
  - A highlight: (E-P0033-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (pointer: papers/paper_notes.jsonl:paper_id=P0033#key_results[0])
  - A highlight: (E-P0039-5607dc887c) Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Ji2025Taxonomy (pointer: papers/paper_notes.jsonl:paper_id=P0039#key_results[1])
  - B highlight: (E-P0089-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0089#key_results[0])
  - B highlight: (E-P0111-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0111#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: API; LLMs; WildAGTEval; ISPM; AWS; ASB; AgentDojo; AgentPoison; IPI; IPI-centric. Kim2026Beyond Liu2026Agents Engelberg2026Sola Shi2025Progent

## Failures / limitations

- Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Liu2026Agents
- Identity Security Posture Management (ISPM) is a core challenge for modern enterprises operating across cloud and SaaS environments. Engelberg2026Sola
- Overall, this work provides a practical and reproducible benchmark for evaluating agentic AI systems in identity security and establishes a foundation for future ISPM benchmarks covering more advanced identity analysis and governance tasks. Engelberg2026Sola
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Shi2025Progent

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
