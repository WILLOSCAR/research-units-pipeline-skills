# Evidence draft: 3.2 Tool interfaces and orchestration

## Evidence snippets (with provenance)
- (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
- (E-P0085-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0085#method)
- (E-P0046-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0046#limitations[1])
- (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
- (E-P0128-468a77ff1d) Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0128#key_results[0])
- (E-P0046-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0046#key_results[0])
- (E-P0178-1f16a03ad8) In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements. Khatchadourian2026Replayable (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0178#key_results[1])
- (E-P0011-1564795a81) We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Zhou2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0011#method)
- (E-P0011-eb0bf8bedb) Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. Zhou2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0011#summary_bullets[1])
- (E-P0101-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0101#key_results[0])

## Definitions / setup

- Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Zhou2026Beyond Li2026Toolprmbench Hao2026From

## Claim candidates

- Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond
- We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom
- We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool
- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self
- Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0011`, `P0060`, `P0171`; B: Tool-use and function calling: `P0011`, `P0060`, `P0171`. Zhou2025Self Zhou2026Beyond Dong2025Etom
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - A highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0085-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0085#method)
  - B highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0011`, `P0060`, `P0171`; B: Tool-use and function calling: `P0011`, `P0060`, `P0171`. Zhou2025Self Zhou2026Beyond Khatchadourian2026Replayable
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - A highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0178-1f16a03ad8) In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements. Khatchadourian2026Replayable (pointer: papers/paper_notes.jsonl:paper_id=P0178#key_results[1])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0011`, `P0060`, `P0171`; B: Tool-use and function calling: `P0011`, `P0060`, `P0171`. Zhou2025Self Zhou2026Beyond Dong2025Etom
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - A highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0085-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0085#method)
  - B highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0011`, `P0060`, `P0171`; B: Tool-use and function calling: `P0011`, `P0060`, `P0171`. Zhou2025Self Zhou2026Beyond Dong2025Etom
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - A highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0085-192e78b614) We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Etom (pointer: papers/paper_notes.jsonl:paper_id=P0085#method)
  - B highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0011`, `P0060`, `P0171`; B: Tool-use and function calling: `P0011`, `P0060`, `P0171`. Zhou2026Beyond Zhou2025Self
  - A highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - A highlight: (E-P0036-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0036#key_results[0])
  - B highlight: (E-P0011-b6b7af5a81) Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#key_results[0])
  - B highlight: (E-P0011-eb0bf8bedb) Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. Zhou2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0011#summary_bullets[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: RAG; MCP; MCTS; LLMs; BFCL; GPU; ToolBench; PRMs; PRM; ToolPRMBench. Zhou2026Beyond Li2026Toolprmbench Hao2026From Khatchadourian2026Replayable

## Failures / limitations

- The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Zhou2026Beyond
- We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Zhou2026Beyond
- Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. Zhou2026Beyond
- These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process. Zhou2026Beyond

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
