# Evidence draft: 5.2 Multi-agent coordination

## Evidence snippets (with provenance)
- (E-P0101-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0101#key_results[0])
- (E-P0122-32b2c8cf19) We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. Cao2025Skyrl (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0122#method)
- (E-P0158-c92ed293ba) While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0158#limitations[1])
- (E-P0158-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0158#key_results[1])
- (E-P0188-1063eee7ce) However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. Zhao2025Achieving (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0188#key_results[0])
- (E-P0041-9af6a59afb) Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method. Ji2025Tree (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0041#key_results[0])
- (E-P0105-1d5f67b08e) Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Ghose2025Orfs (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0105#key_results[0])
- (E-P0122-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Cao2025Skyrl (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0122#key_results[1])
- (E-P0076-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0076#key_results[1])
- (E-P0105-ddd045953e) ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics. Ghose2025Orfs (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0105#key_results[1])

## Definitions / setup

- Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: evaluation protocol (datasets, metrics, human evaluation); compute and latency constraints; tool interface contract (schemas / protocols); tool selection / routing policy; sandboxing / permissions / observability. Ji2025Tree Silva2025Agents Lumer2025Memtool

## Claim candidates

- Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool
- We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. Cao2025Skyrl
- While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Shen2024Small
- Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small
- However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. Zhao2025Achieving

## Concrete comparisons

- Axis: evaluation protocol (datasets, metrics, human evaluation); A: Agent frameworks / architectures: `P0041`, `P0076`, `P0101`; B: Multi-agent coordination: `P0076`, `P0187`, `P0205`. Lumer2025Memtool Cao2025Skyrl Silva2025Agents
  - A highlight: (E-P0101-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0101#key_results[0])
  - A highlight: (E-P0122-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0122#key_results[1])
  - B highlight: (E-P0076-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[1])
- Axis: compute and latency constraints; A: Agent frameworks / architectures: `P0041`, `P0076`, `P0101`; B: Multi-agent coordination: `P0076`, `P0187`, `P0205`. Cao2025Skyrl Silva2025Agents
  - A highlight: (E-P0122-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0122#key_results[1])
  - A highlight: (E-P0122-32b2c8cf19) We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0122#method)
  - B highlight: (E-P0076-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[1])
- Axis: tool interface contract (schemas / protocols); A: Agent frameworks / architectures: `P0041`, `P0076`, `P0101`; B: Multi-agent coordination: `P0076`, `P0187`, `P0205`. Lumer2025Memtool Cao2025Skyrl Silva2025Agents
  - A highlight: (E-P0101-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0101#key_results[0])
  - A highlight: (E-P0122-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0122#key_results[1])
  - B highlight: (E-P0076-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[1])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0041`, `P0076`, `P0101`; B: Multi-agent coordination: `P0076`, `P0187`, `P0205`. Lumer2025Memtool Cao2025Skyrl Silva2025Agents
  - A highlight: (E-P0101-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0101#key_results[0])
  - A highlight: (E-P0122-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0122#key_results[1])
  - B highlight: (E-P0076-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[1])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0041`, `P0076`, `P0101`; B: Multi-agent coordination: `P0076`, `P0187`, `P0205`. Cao2025Skyrl Ghose2025Orfs Silva2025Agents
  - A highlight: (E-P0122-5ed988eb67) We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and Cao2025Skyrl (pointer: papers/paper_notes.jsonl:paper_id=P0122#key_results[1])
  - A highlight: (E-P0105-1d5f67b08e) Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Ghose2025Orfs (pointer: papers/paper_notes.jsonl:paper_id=P0105#key_results[0])
  - B highlight: (E-P0076-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: LLMs; GRPO; MCP; MemTool; ScaleMCP; ORFS-agent; LLM-based; RFS-agent; SA-SWE-32B; AST-based. Ji2025Tree Silva2025Agents Lumer2025Memtool Ghose2025Orfs

## Failures / limitations

- To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. Ji2025Tree
- This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents
- Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains. Papadakis2025Atlas
- While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Chuang2025Debate

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- training data and supervision signal
- baseline choices and ablation evidence
