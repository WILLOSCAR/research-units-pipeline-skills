{"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "http://arxiv.org/abs/2210.03629v3", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "source": "arxiv", "arxiv_id": "2210.03629v3", "pdf_url": "https://arxiv.org/pdf/2210.03629v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2022-10-06T01:00:32Z", "updated": "2023-03-10T01:00:17Z", "provenance": [{"route": "pinned_arxiv_id:2210.03629v3", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "authors": ["Timo Schick", "Jane Dwivedi-Yu", "Roberto DessÃ¬", "Roberta Raileanu", "Maria Lomeli", "Luke Zettlemoyer", "Nicola Cancedda", "Thomas Scialom"], "year": 2023, "url": "http://arxiv.org/abs/2302.04761v1", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.", "source": "arxiv", "arxiv_id": "2302.04761v1", "pdf_url": "https://arxiv.org/pdf/2302.04761v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-02-09T16:49:57Z", "updated": "2023-02-09T16:49:57Z", "provenance": [{"route": "pinned_arxiv_id:2302.04761v1", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "authors": ["Noah Shinn", "Federico Cassano", "Edward Berman", "Ashwin Gopinath", "Karthik Narasimhan", "Shunyu Yao"], "year": 2023, "url": "http://arxiv.org/abs/2303.11366v4", "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.", "source": "arxiv", "arxiv_id": "2303.11366v4", "pdf_url": "https://arxiv.org/pdf/2303.11366v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-03-20T18:08:50Z", "updated": "2023-10-10T05:21:45Z", "provenance": [{"route": "pinned_arxiv_id:2303.11366v4", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "Thomas L. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "year": 2023, "url": "http://arxiv.org/abs/2305.10601v2", "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "source": "arxiv", "arxiv_id": "2305.10601v2", "pdf_url": "https://arxiv.org/pdf/2305.10601v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-17T23:16:17Z", "updated": "2023-12-03T22:50:35Z", "provenance": [{"route": "pinned_arxiv_id:2305.10601v2", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Voyager: An Open-Ended Embodied Agent with Large Language Models", "authors": ["Guanzhi Wang", "Yuqi Xie", "Yunfan Jiang", "Ajay Mandlekar", "Chaowei Xiao", "Yuke Zhu", "Linxi Fan", "Anima Anandkumar"], "year": 2023, "url": "http://arxiv.org/abs/2305.16291v2", "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.", "source": "arxiv", "arxiv_id": "2305.16291v2", "pdf_url": "https://arxiv.org/pdf/2305.16291v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-05-25T17:46:38Z", "updated": "2023-10-19T16:27:03Z", "provenance": [{"route": "pinned_arxiv_id:2305.16291v2", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Survey on Large Language Model based Autonomous Agents", "authors": ["Lei Wang", "Chen Ma", "Xueyang Feng", "Zeyu Zhang", "Hao Yang", "Jingsen Zhang", "Zhiyuan Chen", "Jiakai Tang", "Xu Chen", "Yankai Lin", "Wayne Xin Zhao", "Zhewei Wei", "Ji-Rong Wen"], "year": 2023, "url": "http://arxiv.org/abs/2308.11432v7", "abstract": "Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.", "source": "arxiv", "arxiv_id": "2308.11432v7", "pdf_url": "https://arxiv.org/pdf/2308.11432v7", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "10.1007/s11704-024-40231-1", "venue": "", "published": "2023-08-22T13:30:37Z", "updated": "2025-03-02T04:04:03Z", "provenance": [{"route": "pinned_arxiv_id:2308.11432v7", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges", "authors": ["Junyu Luo", "Weizhi Zhang", "Ye Yuan", "Yusheng Zhao", "Junwei Yang", "Yiyang Gu", "Bohan Wu", "Binqi Chen", "Ziyue Qiao", "Qingqing Long", "Rongcheng Tu", "Xiao Luo", "Wei Ju", "Zhiping Xiao", "Yifan Wang", "Meng Xiao", "Chenwu Liu", "Jingyang Yuan", "Shichang Zhang", "Yiqiao Jin", "Fan Zhang", "Xian Wu", "Hanqing Zhao", "Dacheng Tao", "Philip S. Yu", "Ming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.21460v1", "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.", "source": "arxiv", "arxiv_id": "2503.21460v1", "pdf_url": "https://arxiv.org/pdf/2503.21460v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T12:50:17Z", "updated": "2025-03-27T12:50:17Z", "provenance": [{"route": "pinned_arxiv_id:2503.21460v1", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}, {"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agentic Large Language Models, a survey", "authors": ["Aske Plaat", "Max van Duijn", "Niki van Stein", "Mike Preuss", "Peter van der Putten", "Kees Joost Batenburg"], "year": 2025, "url": "http://arxiv.org/abs/2503.23037v3", "abstract": "Background: There is great interest in agentic LLMs, large language models that act as agents.\n  Objectives: We review the growing body of work in this area and provide a research agenda.\n  Methods: Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories.\n  Results: The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories.\n  Conclusions: We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "source": "arxiv", "arxiv_id": "2503.23037v3", "pdf_url": "https://arxiv.org/pdf/2503.23037v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "10.1613/jair.1.18675", "venue": "JAIR volume 84, article 29, December 2025", "published": "2025-03-29T11:02:20Z", "updated": "2025-11-22T08:55:19Z", "provenance": [{"route": "pinned_arxiv_id:2503.23037v3", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model-Brained GUI Agents: A Survey", "authors": ["Chaoyun Zhang", "Shilin He", "Jiaxu Qian", "Bowen Li", "Liqun Li", "Si Qin", "Yu Kang", "Minghua Ma", "Guyue Liu", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.18279v12", "abstract": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.", "source": "arxiv", "arxiv_id": "2411.18279v12", "pdf_url": "https://arxiv.org/pdf/2411.18279v12", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-27T12:13:39Z", "updated": "2025-05-06T15:08:00Z", "provenance": [{"route": "pinned_arxiv_id:2411.18279v12", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query?id_list=...", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering", "authors": ["Bo Yu", "Lei Zhao"], "year": 2026, "url": "http://arxiv.org/abs/2601.04556v1", "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.", "source": "arxiv", "arxiv_id": "2601.04556v1", "pdf_url": "https://arxiv.org/pdf/2601.04556v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-08T03:36:06Z", "updated": "2026-01-08T03:36:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging", "authors": ["Zhuoka Feng", "Kang Chen", "Sihan Zhao", "Kai Xiong", "Yaoning Wang", "Minshen Yu", "Junjie Nian", "Changyi Xiao", "Yixin Cao", "Yugang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.07309v1", "abstract": "Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.", "source": "arxiv", "arxiv_id": "2601.07309v1", "pdf_url": "https://arxiv.org/pdf/2601.07309v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T08:31:53Z", "updated": "2026-01-12T08:31:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Active Context Compression: Autonomous Memory Management in LLM Agents", "authors": ["Nikhil Verma"], "year": 2026, "url": "http://arxiv.org/abs/2601.07190v1", "abstract": "Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to \"Context Bloat.\" As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent \"Knowledge\" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.", "source": "arxiv", "arxiv_id": "2601.07190v1", "pdf_url": "https://arxiv.org/pdf/2601.07190v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T04:31:00Z", "updated": "2026-01-12T04:31:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering", "authors": ["Di Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.04620v1", "abstract": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.", "source": "arxiv", "arxiv_id": "2601.04620v1", "pdf_url": "https://arxiv.org/pdf/2601.04620v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T05:49:01Z", "updated": "2026-01-08T05:49:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "year": 2026, "url": "http://arxiv.org/abs/2601.01885v1", "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "source": "arxiv", "arxiv_id": "2601.01885v1", "pdf_url": "https://arxiv.org/pdf/2601.01885v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-05T08:24:16Z", "updated": "2026-01-05T08:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation", "authors": ["Nicolas Bougie", "Gian Maria Marconi", "Tony Yip", "Narimasa Watanabe"], "year": 2026, "url": "http://arxiv.org/abs/2601.00930v1", "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.", "source": "arxiv", "arxiv_id": "2601.00930v1", "pdf_url": "https://arxiv.org/pdf/2601.00930v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2026-01-02T03:01:33Z", "updated": "2026-01-02T03:01:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Autonomous Quantum Simulation through Large Language Model Agents", "authors": ["Weitang Li", "Jiajun Ren", "Lixue Cheng", "Cunxi Gong"], "year": 2026, "url": "http://arxiv.org/abs/2601.10194v1", "abstract": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Tensor network methods are powerful tools for quantum simulation, but their effective use requires expertise typically acquired through years of graduate training. By combining in-context learning with curated documentation and multi-agent decomposition, we create autonomous AI agents that can be trained in specialized computational domains within minutes. We benchmark three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "source": "arxiv", "arxiv_id": "2601.10194v1", "pdf_url": "https://arxiv.org/pdf/2601.10194v1", "categories": ["quant-ph", "physics.chem-ph"], "primary_category": "quant-ph", "doi": "", "venue": "", "published": "2026-01-15T08:50:57Z", "updated": "2026-01-15T08:50:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents", "authors": ["Miao Su", "Yucan Guo", "Zhongni Hou", "Long Bai", "Zixuan Li", "Yufei Zhang", "Guojun Yin", "Wei Lin", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "year": 2026, "url": "http://arxiv.org/abs/2601.07468v1", "abstract": "Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.", "source": "arxiv", "arxiv_id": "2601.07468v1", "pdf_url": "https://arxiv.org/pdf/2601.07468v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-12T12:24:44Z", "updated": "2026-01-12T12:24:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence", "authors": ["Sumanth Balaji", "Piyush Mishra", "Aashraya Sachdeva", "Suraj Agrawal"], "year": 2026, "url": "http://arxiv.org/abs/2601.00596v1", "abstract": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.", "source": "arxiv", "arxiv_id": "2601.00596v1", "pdf_url": "https://arxiv.org/pdf/2601.00596v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-02T07:21:23Z", "updated": "2026-01-02T07:21:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents", "authors": ["Kaiyu Zhou", "Yongsen Zheng", "Yicheng He", "Meng Xue", "Xueluan Gong", "Yuji Wang", "Kwok-Yan Lam"], "year": 2026, "url": "http://arxiv.org/abs/2601.10955v1", "abstract": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "source": "arxiv", "arxiv_id": "2601.10955v1", "pdf_url": "https://arxiv.org/pdf/2601.10955v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-16T02:47:45Z", "updated": "2026-01-16T02:47:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity", "authors": ["Doyoung Kim", "Zhiwei Ren", "Jie Hao", "Zhongkai Sun", "Lichao Wang", "Xiyao Ma", "Zack Ye", "Xu Han", "Jun Yin", "Heng Ji", "Wei Shen", "Xing Fan", "Benjamin Yao", "Chenlei Guo"], "year": 2026, "url": "http://arxiv.org/abs/2601.00268v1", "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.", "source": "arxiv", "arxiv_id": "2601.00268v1", "pdf_url": "https://arxiv.org/pdf/2601.00268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-01T09:19:20Z", "updated": "2026-01-01T09:19:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond Static Summarization: Proactive Memory Extraction for LLM Agents", "authors": ["Chengyuan Yang", "Zequn Sun", "Wei Wei", "Wei Hu"], "year": 2026, "url": "http://arxiv.org/abs/2601.04463v1", "abstract": "Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is \"ahead-of-time\", acting as a blind \"feed-forward\" process that misses important details because it doesn't know future tasks. Second, extraction is usually \"one-off\", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.", "source": "arxiv", "arxiv_id": "2601.04463v1", "pdf_url": "https://arxiv.org/pdf/2601.04463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-08T00:37:29Z", "updated": "2026-01-08T00:37:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents", "authors": ["Xiucheng Xu", "Bingbing Xu", "Xueyun Tian", "Zihe Huang", "Rongxin Chen", "Yunfan Li", "Huawei Shen"], "year": 2026, "url": "http://arxiv.org/abs/2601.14287v1", "abstract": "External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.", "source": "arxiv", "arxiv_id": "2601.14287v1", "pdf_url": "https://arxiv.org/pdf/2601.14287v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-14T04:42:15Z", "updated": "2026-01-14T04:42:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Continuum Memory Architectures for Long-Horizon LLM Agents", "authors": ["Joe Logan"], "year": 2026, "url": "http://arxiv.org/abs/2601.09913v1", "abstract": "Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.", "source": "arxiv", "arxiv_id": "2601.09913v1", "pdf_url": "https://arxiv.org/pdf/2601.09913v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T22:40:35Z", "updated": "2026-01-14T22:40:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics", "authors": ["Sukesh Subaharan"], "year": 2026, "url": "http://arxiv.org/abs/2601.16087v1", "abstract": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.", "source": "arxiv", "arxiv_id": "2601.16087v1", "pdf_url": "https://arxiv.org/pdf/2601.16087v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-22T16:34:05Z", "updated": "2026-01-22T16:34:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants", "authors": ["Ziyi Shi", "Xusen Guo", "Hongliang Lu", "Mingxing Peng", "Haotian Wang", "Zheng Zhu", "Zhenning Li", "Yuxuan Liang", "Xinhu Zheng", "Hai Yang"], "year": 2026, "url": "http://arxiv.org/abs/2601.09264v1", "abstract": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...", "source": "arxiv", "arxiv_id": "2601.09264v1", "pdf_url": "https://arxiv.org/pdf/2601.09264v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T07:59:44Z", "updated": "2026-01-14T07:59:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Effects of personality steering on cooperative behavior in Large Language Model agents", "authors": ["Mizuki Sakai", "Mizuki Yokoyama", "Wakaba Tateishi", "Genki Ichinose"], "year": 2026, "url": "http://arxiv.org/abs/2601.05302v2", "abstract": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.", "source": "arxiv", "arxiv_id": "2601.05302v2", "pdf_url": "https://arxiv.org/pdf/2601.05302v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-08T14:23:45Z", "updated": "2026-01-14T12:54:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories", "authors": ["Qian Xiong", "Yuekai Huang", "Bo Yang", "Yujia Zheng", "Tianhao Li", "Ziyou Jiang", "Zhiyuan Chang", "Zhaoyang Li", "Huanxiang Feng", "Mingyang Li"], "year": 2026, "url": "http://arxiv.org/abs/2601.15120v2", "abstract": "LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.", "source": "arxiv", "arxiv_id": "2601.15120v2", "pdf_url": "https://arxiv.org/pdf/2601.15120v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-21T15:58:54Z", "updated": "2026-01-22T12:08:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents", "authors": ["Hyunjun Kim"], "year": 2026, "url": "http://arxiv.org/abs/2601.11585v1", "abstract": "Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.", "source": "arxiv", "arxiv_id": "2601.11585v1", "pdf_url": "https://arxiv.org/pdf/2601.11585v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-01T19:43:05Z", "updated": "2026-01-01T19:43:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "authors": ["Xiaoshuai Song", "Haofei Chang", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou", "Ji-Rong Wen"], "year": 2026, "url": "http://arxiv.org/abs/2601.05808v1", "abstract": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "source": "arxiv", "arxiv_id": "2601.05808v1", "pdf_url": "https://arxiv.org/pdf/2601.05808v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-09T14:32:06Z", "updated": "2026-01-09T14:32:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems", "authors": ["Guibin Zhang", "Haiyang Yu", "Kaiming Yang", "Bingli Wu", "Fei Huang", "Yongbin Li", "Shuicheng Yan"], "year": 2026, "url": "http://arxiv.org/abs/2601.02695v1", "abstract": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "source": "arxiv", "arxiv_id": "2601.02695v1", "pdf_url": "https://arxiv.org/pdf/2601.02695v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-06T04:06:46Z", "updated": "2026-01-06T04:06:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FROAV: A Framework for RAG Observation and Agent Verification -- Lowering the Barrier to LLM Agent Research", "authors": ["Tzu-Hsuan Lin", "Chih-Hsuan Kao"], "year": 2026, "url": "http://arxiv.org/abs/2601.07504v1", "abstract": "The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous \"LLM-as-a-Judge\" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.", "source": "arxiv", "arxiv_id": "2601.07504v1", "pdf_url": "https://arxiv.org/pdf/2601.07504v1", "categories": ["cs.LG", "cs.SE"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2026-01-12T13:02:32Z", "updated": "2026-01-12T13:02:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents", "authors": ["Bingguang Hao", "Zengzhuang Xu", "Yuntao Wen", "Xinyi Xu", "Yang Liu", "Tong Zhao", "Maolin Wang", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Xiangyu Zhao", "Chenyi Zhuang", "Ji Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.01498v1", "abstract": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.", "source": "arxiv", "arxiv_id": "2601.01498v1", "pdf_url": "https://arxiv.org/pdf/2601.01498v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-04T11:56:33Z", "updated": "2026-01-04T11:56:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Holmes: An Evidence-Grounded LLM Agent for Auditable DDoS Investigation in Cloud Networks", "authors": ["Haodong Chen", "Ziheng Zhang", "Jinghui Jiang", "Qiang Su", "Qiao Xiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.14601v1", "abstract": "Cloud environments face frequent DDoS threats due to centralized resources and broad attack surfaces. Modern cloud-native DDoS attacks further evolve rapidly and often blend multi-vector strategies, creating an operational dilemma: defenders need wire-speed monitoring while also requiring explainable, auditable attribution for response. Existing rule-based and supervised-learning approaches typically output black-box scores or labels, provide limited evidence chains, and generalize poorly to unseen attack variants; meanwhile, high-quality labeled data is often difficult to obtain in cloud settings.\n  We present Holmes (DDoS Detective), an LLM-based DDoS detection agent that reframes the model as a virtual SRE investigator rather than an end-to-end classifier. Holmes couples a funnel-like hierarchical workflow (counters/sFlow for continuous sensing and triage; PCAP evidence collection triggered only on anomaly windows) with an Evidence Pack abstraction that converts binary packets into compact, reproducible, high-signal structured evidence. On top of this evidence interface, Holmes enforces a structure-first investigation protocol and strict JSON/quotation constraints to produce machine-consumable reports with auditable evidence anchors.\n  We evaluate Holmes on CICDDoS2019 reflection/amplification attacks and script-triggered flooding scenarios. Results show that Holmes produces attribution decisions grounded in salient evidence anchors across diverse attack families, and when errors occur, its audit logs make the failure source easy to localize, demonstrating the practicality of an LLM agent for cost-controlled and traceable DDoS investigation in cloud operations.", "source": "arxiv", "arxiv_id": "2601.14601v1", "pdf_url": "https://arxiv.org/pdf/2601.14601v1", "categories": ["cs.CR", "cs.NI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-21T02:39:46Z", "updated": "2026-01-21T02:39:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents", "authors": ["Xin Quan", "Jiafeng Xiong", "Marco Valentino", "AndrÃ© Freitas"], "year": 2026, "url": "http://arxiv.org/abs/2601.08742v1", "abstract": "Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.", "source": "arxiv", "arxiv_id": "2601.08742v1", "pdf_url": "https://arxiv.org/pdf/2601.08742v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T17:18:38Z", "updated": "2026-01-13T17:18:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation Networks", "authors": ["Abdelrahman Soliman", "Ahmed Refaey", "Aiman Erbad", "Amr Mohamed"], "year": 2026, "url": "http://arxiv.org/abs/2601.13114v1", "abstract": "Intent-based networks (IBNs) are gaining prominence as an innovative technology that automates network operations through high-level request statements, defining what the network should achieve. In this work, we introduce IntAgent, an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. Unlike previous approaches, we develop an intent tools engine directly within the NWDAF analytics engine, allowing our agent to utilize live network analytics to inform its reasoning and tool selection. We offer an enriched, 3GPP-compliant data source that enhances the dynamic, context-aware fulfillment of network operator goals, along with an MCP tools server for scheduling, monitoring, and analytics tools. We demonstrate the efficacy of our framework through two practical use cases: ML-based traffic prediction and scheduled policy enforcement, which validate IntAgent's ability to autonomously fulfill complex network intents.", "source": "arxiv", "arxiv_id": "2601.13114v1", "pdf_url": "https://arxiv.org/pdf/2601.13114v1", "categories": ["cs.NI", "cs.AI", "eess.SY"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2026-01-19T14:55:48Z", "updated": "2026-01-19T14:55:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "KryptoPilot: An Open-World Knowledge-Augmented LLM Agent for Automated Cryptographic Exploitation", "authors": ["Xiaonan Liu", "Zhihao Li", "Xiao Lan", "Hao Ren", "Haizhou Wang", "Xingshu Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.09129v1", "abstract": "Capture-the-Flag (CTF) competitions play a central role in modern cybersecurity as a platform for training practitioners and evaluating offensive and defensive techniques derived from real-world vulnerabilities. Despite recent advances in large language models (LLMs), existing LLM-based agents remain ineffective on high-difficulty cryptographic CTF challenges, which require precise cryptanalytic knowledge, stable long-horizon reasoning, and disciplined interaction with specialized toolchains. Through a systematic exploratory study, we show that insufficient knowledge granularity, rather than model reasoning capacity, is a primary factor limiting successful cryptographic exploitation: coarse or abstracted external knowledge often fails to support correct attack modeling and implementation. Motivated by this observation, we propose KryptoPilot, an open-world knowledge-augmented LLM agent for automated cryptographic exploitation. KryptoPilot integrates dynamic open-world knowledge acquisition via a Deep Research pipeline, a persistent workspace for structured knowledge reuse, and a governance subsystem that stabilizes reasoning through behavioral constraints and cost-aware model routing. This design enables precise knowledge alignment while maintaining efficient reasoning across heterogeneous subtasks. We evaluate KryptoPilot on two established CTF benchmarks and in six real-world CTF competitions. KryptoPilot achieves a complete solve rate on InterCode-CTF, solves between 56 and 60 percent of cryptographic challenges on the NYU-CTF benchmark, and successfully solves 26 out of 33 cryptographic challenges in live competitions, including multiple earliest-solved and uniquely-solved instances. These results demonstrate the necessity of open-world, fine-grained knowledge augmentation and governed reasoning for scaling LLM-based agents to real-world cryptographic exploitation.", "source": "arxiv", "arxiv_id": "2601.09129v1", "pdf_url": "https://arxiv.org/pdf/2601.09129v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-14T04:02:40Z", "updated": "2026-01-14T04:02:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "authors": ["Zixuan Xiao", "Jun Ma"], "year": 2026, "url": "http://arxiv.org/abs/2601.02757v1", "abstract": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.", "source": "arxiv", "arxiv_id": "2601.02757v1", "pdf_url": "https://arxiv.org/pdf/2601.02757v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1016/j.autcon.2025.106341", "venue": "Automation in Construction 177 (2025) 106341", "published": "2026-01-06T06:49:51Z", "updated": "2026-01-06T06:49:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents in Law: Taxonomy, Applications, and Challenges", "authors": ["Shuang Liu", "Ruijia Zhang", "Ruoyun Ma", "Yujia Deng", "Lanyi Zhu", "Jiayu Li", "Zelong Li", "Zhibin Shen", "Mengnan Du"], "year": 2026, "url": "http://arxiv.org/abs/2601.06216v1", "abstract": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Recently, LLM agents have attracted significant attention as a solution to these challenges, utilizing advanced capabilities such as planning, memory, and tool usage to meet the rigorous standards of legal practice. In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs. Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.", "source": "arxiv", "arxiv_id": "2601.06216v1", "pdf_url": "https://arxiv.org/pdf/2601.06216v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2026-01-08T21:04:35Z", "updated": "2026-01-08T21:04:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach", "authors": ["Kuo Liang", "Yuhang Lu", "Jianming Mao", "Shuyi Sun", "Chunwei Yang", "Congcong Zeng", "Xiao Jin", "Hanzhang Qin", "Ruihao Zhu", "Chung-Piaw Teo"], "year": 2026, "url": "http://arxiv.org/abs/2601.09635v1", "abstract": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.", "source": "arxiv", "arxiv_id": "2601.09635v1", "pdf_url": "https://arxiv.org/pdf/2601.09635v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T17:09:57Z", "updated": "2026-01-14T17:09:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent for User-friendly Chemical Process Simulations", "authors": ["Jingkang Liang", "Niklas Groll", "GÃ¼rkan Sin"], "year": 2026, "url": "http://arxiv.org/abs/2601.11650v1", "abstract": "Modern process simulators enable detailed process design, simulation, and optimization; however, constructing and interpreting simulations is time-consuming and requires expert knowledge. This limits early exploration by inexperienced users. To address this, a large language model (LLM) agent is integrated with AVEVA Process Simulation (APS) via Model Context Protocol (MCP), allowing natural language interaction with rigorous process simulations. An MCP server toolset enables the LLM to communicate programmatically with APS using Python, allowing it to execute complex simulation tasks from plain-language instructions. Two water-methanol separation case studies assess the framework across different task complexities and interaction modes. The first shows the agent autonomously analyzing flowsheets, finding improvement opportunities, and iteratively optimizing, extracting data, and presenting results clearly. The framework benefits both educational purposes, by translating technical concepts and demonstrating workflows, and experienced practitioners by automating data extraction, speeding routine tasks, and supporting brainstorming. The second case study assesses autonomous flowsheet synthesis through both a step-by-step dialogue and a single prompt, demonstrating its potential for novices and experts alike. The step-by-step mode gives reliable, guided construction suitable for educational contexts; the single-prompt mode constructs fast baseline flowsheets for later refinement. While current limitations such as oversimplification, calculation errors, and technical hiccups mean expert oversight is still needed, the framework's capabilities in analysis, optimization, and guided construction suggest LLM-based agents can become valuable collaborators.", "source": "arxiv", "arxiv_id": "2601.11650v1", "pdf_url": "https://arxiv.org/pdf/2601.11650v1", "categories": ["physics.chem-ph", "cs.AI"], "primary_category": "physics.chem-ph", "doi": "", "venue": "", "published": "2026-01-15T12:18:45Z", "updated": "2026-01-15T12:18:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis", "authors": ["Yiyang Li", "Zheyuan Zhang", "Tianyi Ma", "Zehong Wang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "year": 2026, "url": "http://arxiv.org/abs/2601.02598v2", "abstract": "We introduce LongDA, a data analysis benchmark for evaluating LLM-based agents under documentation-intensive analytical workflows. In contrast to existing benchmarks that assume well-specified schemas and inputs, LongDA targets real-world settings in which navigating long documentation and complex data is the primary bottleneck. To this end, we manually curate raw data files, long and heterogeneous documentation, and expert-written publications from 17 publicly available U.S. national surveys, from which we extract 505 analytical queries grounded in real analytical practice. Solving these queries requires agents to first retrieve and integrate key information from multiple unstructured documents, before performing multi-step computations and writing executable code, which remains challenging for existing data analysis agents. To support the systematic evaluation under this setting, we develop LongTA, a tool-augmented agent framework that enables document access, retrieval, and code execution, and evaluate a range of proprietary and open-source models. Our experiments reveal substantial performance gaps even among state-of-the-art models, highlighting the challenges researchers should consider before applying LLM agents for decision support in real-world, high-stakes analytical settings.", "source": "arxiv", "arxiv_id": "2601.02598v2", "pdf_url": "https://arxiv.org/pdf/2601.02598v2", "categories": ["cs.DL", "cs.AI"], "primary_category": "cs.DL", "doi": "", "venue": "", "published": "2026-01-05T23:23:16Z", "updated": "2026-01-11T22:21:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games", "authors": ["Sixiong Xie", "Zhuofan Shi", "Haiyang Shen", "Gang Huang", "Yun Ma", "Xiang Jing"], "year": 2026, "url": "http://arxiv.org/abs/2601.08462v1", "abstract": "As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.", "source": "arxiv", "arxiv_id": "2601.08462v1", "pdf_url": "https://arxiv.org/pdf/2601.08462v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-13T11:38:51Z", "updated": "2026-01-13T11:38:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "authors": ["Jian Zhang", "Zhiyuan Wang", "Zhangqi Wang", "Yu He", "Haoran Luo", "li yuan", "Lingling Zhang", "Rui Mao", "Qika Lin", "Jun Liu"], "year": 2026, "url": "http://arxiv.org/abs/2601.09259v1", "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "source": "arxiv", "arxiv_id": "2601.09259v1", "pdf_url": "https://arxiv.org/pdf/2601.09259v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T07:48:00Z", "updated": "2026-01-14T07:48:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis", "authors": ["Zixuan Xiao", "Jun Ma", "Siwei Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.05483v1", "abstract": "Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.", "source": "arxiv", "arxiv_id": "2601.05483v1", "pdf_url": "https://arxiv.org/pdf/2601.05483v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.1016/j.asoc.2026.114576", "venue": "Applied Soft Computing 190 (2026) 114576", "published": "2026-01-09T02:34:35Z", "updated": "2026-01-09T02:34:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents", "authors": ["Shouju Wang", "Haopeng Zhang"], "year": 2026, "url": "http://arxiv.org/abs/2601.08235v2", "abstract": "As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.", "source": "arxiv", "arxiv_id": "2601.08235v2", "pdf_url": "https://arxiv.org/pdf/2601.08235v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-13T05:39:43Z", "updated": "2026-01-14T05:26:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents", "authors": ["Dehao Tao", "Guoliang Ma", "Yongfeng Huang", "Minghu Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.03785v2", "abstract": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.", "source": "arxiv", "arxiv_id": "2601.03785v2", "pdf_url": "https://arxiv.org/pdf/2601.03785v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-07T10:36:29Z", "updated": "2026-01-20T07:09:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Memory Poisoning Attack and Defense on Memory Based LLM-Agents", "authors": ["Balachandra Devarangadi Sunil", "Isheeta Sinha", "Piyush Maheshwari", "Shantanu Todmal", "Shreyan Mallik", "Shuchi Mishra"], "year": 2026, "url": "http://arxiv.org/abs/2601.05504v2", "abstract": "Large language model agents equipped with persistent memory are vulnerable to memory poisoning attacks, where adversaries inject malicious instructions through query only interactions that corrupt the agents long term memory and influence future responses. Recent work demonstrated that the MINJA (Memory Injection Attack) achieves over 95 % injection success rate and 70 % attack success rate under idealized conditions. However, the robustness of these attacks in realistic deployments and effective defensive mechanisms remain understudied. This work addresses these gaps through systematic empirical evaluation of memory poisoning attacks and defenses in Electronic Health Record (EHR) agents. We investigate attack robustness by varying three critical dimensions: initial memory state, number of indication prompts, and retrieval parameters. Our experiments on GPT-4o-mini, Gemini-2.0-Flash and Llama-3.1-8B-Instruct models using MIMIC-III clinical data reveal that realistic conditions with pre-existing legitimate memories dramatically reduce attack effectiveness. We then propose and evaluate two novel defense mechanisms: (1) Input/Output Moderation using composite trust scoring across multiple orthogonal signals, and (2) Memory Sanitization with trust-aware retrieval employing temporal decay and pattern-based filtering. Our defense evaluation reveals that effective memory sanitization requires careful trust threshold calibration to prevent both overly conservative rejection (blocking all entries) and insufficient filtering (missing subtle attacks), establishing important baselines for future adaptive defense mechanisms. These findings provide crucial insights for securing memory-augmented LLM agents in production environments.", "source": "arxiv", "arxiv_id": "2601.05504v2", "pdf_url": "https://arxiv.org/pdf/2601.05504v2", "categories": ["cs.CR", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-09T03:26:10Z", "updated": "2026-01-12T03:35:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System", "authors": ["Hsiang-Wei Huang", "Junbin Lu", "Kuang-Ming Chen", "Jenq-Neng Hwang"], "year": 2026, "url": "http://arxiv.org/abs/2601.08829v1", "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.", "source": "arxiv", "arxiv_id": "2601.08829v1", "pdf_url": "https://arxiv.org/pdf/2601.08829v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-13T18:59:17Z", "updated": "2026-01-13T18:59:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "authors": ["Sourena Khanzadeh"], "year": 2026, "url": "http://arxiv.org/abs/2601.02314v1", "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($Ï$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($Ï$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "source": "arxiv", "arxiv_id": "2601.02314v1", "pdf_url": "https://arxiv.org/pdf/2601.02314v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T18:05:29Z", "updated": "2026-01-05T18:05:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions", "authors": ["Aayush Gupta"], "year": 2026, "url": "http://arxiv.org/abs/2601.06112v1", "abstract": "Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \\textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $Îµ$, and (iii) fault tolerance under controlled tool/API failures at intensity $Î»$. ReliabilityBench contributes a unified reliability surface $R(k,Îµ,Î»)$, \\textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $Îµ=0$ to 88.1% at $Îµ=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.", "source": "arxiv", "arxiv_id": "2601.06112v1", "pdf_url": "https://arxiv.org/pdf/2601.06112v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-03T13:41:33Z", "updated": "2026-01-03T13:41:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents", "authors": ["Raffi Khatchadourian"], "year": 2026, "url": "http://arxiv.org/abs/2601.15322v1", "abstract": "LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.\n  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.\n  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "source": "arxiv", "arxiv_id": "2601.15322v1", "pdf_url": "https://arxiv.org/pdf/2601.15322v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-17T19:47:55Z", "updated": "2026-01-17T19:47:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "authors": ["Jiaqi Liu", "Yaofeng Su", "Peng Xia", "Siwei Han", "Zeyu Zheng", "Cihang Xie", "Mingyu Ding", "Huaxiu Yao"], "year": 2026, "url": "http://arxiv.org/abs/2601.02553v1", "abstract": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.", "source": "arxiv", "arxiv_id": "2601.02553v1", "pdf_url": "https://arxiv.org/pdf/2601.02553v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-05T21:02:49Z", "updated": "2026-01-05T21:02:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Sola-Visibility-ISPM: Benchmarking Agentic AI for Identity Security Posture Management Visibility", "authors": ["Gal Engelberg", "Konstantin Koutsyi", "Leon Goldberg", "Reuven Elezra", "Idan Pinto", "Tal Moalem", "Shmuel Cohen", "Yoni Weintrob"], "year": 2026, "url": "http://arxiv.org/abs/2601.07880v1", "abstract": "Identity Security Posture Management (ISPM) is a core challenge for modern enterprises operating across cloud and SaaS environments. Answering basic ISPM visibility questions, such as understanding identity inventory and configuration hygiene, requires interpreting complex identity data, motivating growing interest in agentic AI systems. Despite this interest, there is currently no standardized way to evaluate how well such systems perform ISPM visibility tasks on real enterprise data. We introduce the Sola Visibility ISPM Benchmark, the first benchmark designed to evaluate agentic AI systems on foundational ISPM visibility tasks using a live, production-grade identity environment spanning AWS, Okta, and Google Workspace. The benchmark focuses on identity inventory and hygiene questions and is accompanied by the Sola AI Agent, a tool-using agent that translates natural-language queries into executable data exploration steps and produces verifiable, evidence-backed answers. Across 77 benchmark questions, the agent achieves strong overall performance, with an expert accuracy of 0.84 and a strict success rate of 0.77. Performance is highest on AWS hygiene tasks, where expert accuracy reaches 0.94, while results on Google Workspace and Okta hygiene tasks are more moderate, yet competitive. Overall, this work provides a practical and reproducible benchmark for evaluating agentic AI systems in identity security and establishes a foundation for future ISPM benchmarks covering more advanced identity analysis and governance tasks.", "source": "arxiv", "arxiv_id": "2601.07880v1", "pdf_url": "https://arxiv.org/pdf/2601.07880v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-11T18:36:33Z", "updated": "2026-01-11T18:36:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering", "authors": ["Vedant Nipane", "Pulkit Agrawal", "Amit Singh"], "year": 2026, "url": "http://arxiv.org/abs/2601.11688v1", "abstract": "Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.", "source": "arxiv", "arxiv_id": "2601.11688v1", "pdf_url": "https://arxiv.org/pdf/2601.11688v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-16T11:50:18Z", "updated": "2026-01-16T11:50:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Yunze Xiao", "Junjue Wang", "Naoto Yokoya"], "year": 2026, "url": "http://arxiv.org/abs/2601.07264v1", "abstract": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", "source": "arxiv", "arxiv_id": "2601.07264v1", "pdf_url": "https://arxiv.org/pdf/2601.07264v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2026-01-12T07:10:35Z", "updated": "2026-01-12T07:10:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation", "authors": ["Ziqiao Xi", "Shuang Liang", "Qi Liu", "Jiaqing Zhang", "Letian Peng", "Fang Nan", "Meshal Nayim", "Tianhui Zhang", "Rishika Mundada", "Lianhui Qin", "Biwei Huang", "Kun Zhou"], "year": 2026, "url": "http://arxiv.org/abs/2601.06328v1", "abstract": "Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.", "source": "arxiv", "arxiv_id": "2601.06328v1", "pdf_url": "https://arxiv.org/pdf/2601.06328v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-09T21:59:31Z", "updated": "2026-01-09T21:59:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "authors": ["Dawei Li", "Yuguang Yao", "Zhen Tan", "Huan Liu", "Ruocheng Guo"], "year": 2026, "url": "http://arxiv.org/abs/2601.12294v1", "abstract": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "source": "arxiv", "arxiv_id": "2601.12294v1", "pdf_url": "https://arxiv.org/pdf/2601.12294v1", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-18T07:48:36Z", "updated": "2026-01-18T07:48:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit", "authors": ["Junda Lin", "Zhaomeng Zhou", "Zhi Zheng", "Shuochen Liu", "Tong Xu", "Yong Chen", "Enhong Chen"], "year": 2026, "url": "http://arxiv.org/abs/2601.05755v2", "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility.", "source": "arxiv", "arxiv_id": "2601.05755v2", "pdf_url": "https://arxiv.org/pdf/2601.05755v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2026-01-09T12:19:49Z", "updated": "2026-01-14T18:19:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding", "authors": ["Siyuan Liu", "Hongbang Yuan", "Xinze Li", "Ziyue Zhu", "Yixin Cao", "Yu-Gang Jiang"], "year": 2026, "url": "http://arxiv.org/abs/2601.09503v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.", "source": "arxiv", "arxiv_id": "2601.09503v1", "pdf_url": "https://arxiv.org/pdf/2601.09503v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2026-01-14T14:09:11Z", "updated": "2026-01-14T14:09:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling", "authors": ["Niful Islam", "Ragib Shahriar Ayon", "Deepak George Thomas", "Shibbir Ahmed", "Mohammad Wardat"], "year": 2026, "url": "http://arxiv.org/abs/2601.15232v1", "abstract": "Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.", "source": "arxiv", "arxiv_id": "2601.15232v1", "pdf_url": "https://arxiv.org/pdf/2601.15232v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2026-01-21T18:13:10Z", "updated": "2026-01-21T18:13:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images", "authors": ["Yuxuan Chen", "Ruotong Yang", "Zhengyang Zhang", "Mehreen Ahmed", "Yanming Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.11260v1", "abstract": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.", "source": "arxiv", "arxiv_id": "2510.11260v1", "pdf_url": "https://arxiv.org/pdf/2510.11260v1", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI", "physics.data-an"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-13T10:50:54Z", "updated": "2025-10-13T10:50:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "year": 2025, "url": "http://arxiv.org/abs/2512.11819v1", "abstract": "Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics.", "source": "arxiv", "arxiv_id": "2512.11819v1", "pdf_url": "https://arxiv.org/pdf/2512.11819v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-11-28T22:24:40Z", "updated": "2025-11-28T22:24:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis", "authors": ["Yuzhi Hao", "Danyang Xie"], "year": 2025, "url": "http://arxiv.org/abs/2502.16879v1", "abstract": "This paper pioneers a novel approach to economic and public policy analysis by leveraging multiple Large Language Models (LLMs) as heterogeneous artificial economic agents. We first evaluate five LLMs' economic decision-making capabilities in solving two-period consumption allocation problems under two distinct scenarios: with explicit utility functions and based on intuitive reasoning. While previous research has often simulated heterogeneity by solely varying prompts, our approach harnesses the inherent variations in analytical capabilities across different LLMs to model agents with diverse cognitive traits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB) framework by mapping these LLMs to specific educational groups and corresponding income brackets. Using interest-income taxation as a case study, we demonstrate how the MLAB framework can simulate policy impacts across heterogeneous agents, offering a promising new direction for economic and public policy analysis by leveraging LLMs' human-like reasoning capabilities and computational power.", "source": "arxiv", "arxiv_id": "2502.16879v1", "pdf_url": "https://arxiv.org/pdf/2502.16879v1", "categories": ["cs.AI", "econ.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-24T06:27:07Z", "updated": "2025-02-24T06:27:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents", "authors": ["Andrew Kiruluta"], "year": 2025, "url": "http://arxiv.org/abs/2508.05311v1", "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework. Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system. Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and mediates communication across agents and external tools, enabling reasoning over both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On \\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through logic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in multistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it boosts abstraction accuracy by +6.0\\% through integration of symbolic oracles. Applications in clinical decision support and scientific discovery show how the system encodes domain rules symbolically while leveraging LLMs for contextual inference and hypothesis generation. This architecture offers a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning.", "source": "arxiv", "arxiv_id": "2508.05311v1", "pdf_url": "https://arxiv.org/pdf/2508.05311v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-07T12:11:53Z", "updated": "2025-08-07T12:11:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools", "authors": ["Minh-Hao Van", "Prateek Verma", "Chen Zhao", "Xintao Wu"], "year": 2025, "url": "http://arxiv.org/abs/2506.20743v1", "abstract": "Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.", "source": "arxiv", "arxiv_id": "2506.20743v1", "pdf_url": "https://arxiv.org/pdf/2506.20743v1", "categories": ["cs.LG", "cs.CE"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-25T18:10:30Z", "updated": "2025-06-25T18:10:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Survey of Large Language Model Agents for Question Answering", "authors": ["Murong Yue"], "year": 2025, "url": "http://arxiv.org/abs/2503.19213v1", "abstract": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.", "source": "arxiv", "arxiv_id": "2503.19213v1", "pdf_url": "https://arxiv.org/pdf/2503.19213v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-24T23:39:44Z", "updated": "2025-03-24T23:39:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval", "authors": ["Yu Zhang", "Shutong Qiao", "Jiaqi Zhang", "Tzu-Heng Lin", "Chen Gao", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.05659v2", "abstract": "Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.", "source": "arxiv", "arxiv_id": "2503.05659v2", "pdf_url": "https://arxiv.org/pdf/2503.05659v2", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-03-07T18:20:30Z", "updated": "2025-04-11T16:51:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A sketch of an AI control safety case", "authors": ["Tomek Korbak", "Joshua Clymer", "Benjamin Hilton", "Buck Shlegeris", "Geoffrey Irving"], "year": 2025, "url": "http://arxiv.org/abs/2501.17315v1", "abstract": "As LLM agents gain a greater capacity to cause harm, AI developers might increasingly rely on control measures such as monitoring to justify that they are safe. We sketch how developers could construct a \"control safety case\", which is a structured argument that models are incapable of subverting control measures in order to cause unacceptable outcomes. As a case study, we sketch an argument that a hypothetical LLM agent deployed internally at an AI company won't exfiltrate sensitive information. The sketch relies on evidence from a \"control evaluation,\"' where a red team deliberately designs models to exfiltrate data in a proxy for the deployment environment. The safety case then hinges on several claims: (1) the red team adequately elicits model capabilities to exfiltrate data, (2) control measures remain at least as effective in deployment, and (3) developers conservatively extrapolate model performance to predict the probability of data exfiltration in deployment. This safety case sketch is a step toward more concrete arguments that can be used to show that a dangerously capable LLM agent is safe to deploy.", "source": "arxiv", "arxiv_id": "2501.17315v1", "pdf_url": "https://arxiv.org/pdf/2501.17315v1", "categories": ["cs.AI", "cs.CR", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-28T21:52:15Z", "updated": "2025-01-28T21:52:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A-IDE : Agent-Integrated Denoising Experts", "authors": ["Uihyun Cho", "Namhun Kim"], "year": 2025, "url": "http://arxiv.org/abs/2503.16780v1", "abstract": "Recent advances in deep-learning based denoising methods have improved Low-Dose CT image quality. However, due to distinct HU distributions and diverse anatomical characteristics, a single model often struggles to generalize across multiple anatomies. To address this limitation, we introduce \\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates three anatomical region-specialized RED-CNN models under the management of decision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to dynamically route incoming LDCT scans to the most appropriate expert model. We highlight three major advantages of our approach. A-IDE excels in heterogeneous, data-scarce environments. The framework automatically prevents overfitting by distributing tasks among multiple experts. Finally, our LLM-driven agentic pipeline eliminates the need for manual interventions. Experimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves superior performance in RMSE, PSNR, and SSIM compared to a single unified denoiser.", "source": "arxiv", "arxiv_id": "2503.16780v1", "pdf_url": "https://arxiv.org/pdf/2503.16780v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-21T01:26:54Z", "updated": "2025-03-21T01:26:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A-MEM: Agentic Memory for LLM Agents", "authors": ["Wujiang Xu", "Zujie Liang", "Kai Mei", "Hang Gao", "Juntao Tan", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12110v11", "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/A-mem, while the source code of the agentic memory system is available at https://github.com/WujiangXu/A-mem-sys.", "source": "arxiv", "arxiv_id": "2502.12110v11", "pdf_url": "https://arxiv.org/pdf/2502.12110v11", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T18:36:14Z", "updated": "2025-10-08T01:46:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language", "authors": ["Aly Lidayan", "Jakob Bjorner", "Satvik Golechha", "Kartik Goyal", "Alane Suhr"], "year": 2025, "url": "http://arxiv.org/abs/2512.20111v1", "abstract": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.", "source": "arxiv", "arxiv_id": "2512.20111v1", "pdf_url": "https://arxiv.org/pdf/2512.20111v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-23T07:11:26Z", "updated": "2025-12-23T07:11:26Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ADAgent: LLM Agent for Alzheimer's Disease Analysis with Collaborative Coordinator", "authors": ["Wenlong Hou", "Guangqian Yang", "Ye Du", "Yeung Lau", "Lihao Liu", "Junjun He", "Ling Long", "Shujun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11150v3", "abstract": "Alzheimer's disease (AD) is a progressive and irreversible neurodegenerative disease. Early and precise diagnosis of AD is crucial for timely intervention and treatment planning to alleviate the progressive neurodegeneration. However, most existing methods rely on single-modality data, which contrasts with the multifaceted approach used by medical experts. While some deep learning approaches process multi-modal data, they are limited to specific tasks with a small set of input modalities and cannot handle arbitrary combinations. This highlights the need for a system that can address diverse AD-related tasks, process multi-modal or missing input, and integrate multiple advanced methods for improved performance. In this paper, we propose ADAgent, the first specialized AI agent for AD analysis, built on a large language model (LLM) to address user queries and support decision-making. ADAgent integrates a reasoning engine, specialized medical tools, and a collaborative outcome coordinator to facilitate multi-modal diagnosis and prognosis tasks in AD. Extensive experiments demonstrate that ADAgent outperforms SOTA methods, achieving significant improvements in accuracy, including a 2.7% increase in multi-modal diagnosis, a 0.7% improvement in multi-modal prognosis, and enhancements in MRI and PET diagnosis tasks.", "source": "arxiv", "arxiv_id": "2506.11150v3", "pdf_url": "https://arxiv.org/pdf/2506.11150v3", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV", "doi": "", "venue": "", "published": "2025-06-11T10:22:19Z", "updated": "2025-07-27T14:17:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection", "authors": ["Weidi Luo", "Shenghong Dai", "Xiaogeng Liu", "Suman Banerjee", "Huan Sun", "Muhao Chen", "Chaowei Xiao"], "year": 2025, "url": "http://arxiv.org/abs/2502.11448v2", "abstract": "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks.", "source": "arxiv", "arxiv_id": "2502.11448v2", "pdf_url": "https://arxiv.org/pdf/2502.11448v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T05:12:33Z", "updated": "2025-02-18T05:37:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AI Kill Switch for malicious web-based LLM agent", "authors": ["Sechan Lee", "Sangdon Park"], "year": 2025, "url": "http://arxiv.org/abs/2511.13725v2", "abstract": "Recently, web-based Large Language Model (LLM) agents autonomously perform increasingly complex tasks, thereby bringing significant convenience. However, they also amplify the risks of malicious misuse cases such as unauthorized collection of personally identifiable information (PII), generation of socially divisive content, and even automated web hacking. To address these threats, we propose an AI Kill Switch technique that can immediately halt the operation of malicious web-based LLM agents. To achieve this, we introduce AutoGuard - the key idea is generating defensive prompts that trigger the safety mechanisms of malicious LLM agents. In particular, generated defense prompts are transparently embedded into the website's DOM so that they remain invisible to human users but can be detected by the crawling process of malicious agents, triggering its internal safety mechanisms to abort malicious actions once read. To evaluate our approach, we constructed a dedicated benchmark consisting of three representative malicious scenarios. Experimental results show that AutoGuard achieves over 80% Defense Success Rate (DSR) across diverse malicious agents, including GPT-4o, Claude-4.5-Sonnet and generalizes well to advanced models like GPT-5.1, Gemini-2.5-flash, and Gemini-3-pro. Also, our approach demonstrates robust defense performance in real-world website environments without significant performance degradation for benign agents. Through this research, we demonstrate the controllability of web-based LLM agents, thereby contributing to the broader effort of AI control and safety.", "source": "arxiv", "arxiv_id": "2511.13725v2", "pdf_url": "https://arxiv.org/pdf/2511.13725v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-26T02:20:46Z", "updated": "2025-12-04T04:58:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "year": 2025, "url": "http://arxiv.org/abs/2508.11416v1", "abstract": "Recent advances in mathematical reasoning and the long-term planning capabilities of large language models (LLMs) have precipitated the development of agents, which are being increasingly leveraged in business operations processes. Decision models to optimize inventory levels are one of the core elements of operations management. However, the capabilities of the LLM agent in making inventory decisions in uncertain contexts, as well as the decision-making biases (e.g. framing effect, etc.) of the agent, remain largely unexplored. This prompts concerns regarding the capacity of LLM agents to effectively address real-world problems, as well as the potential implications of biases that may be present. To address this gap, we introduce AIM-Bench, a novel benchmark designed to assess the decision-making behaviour of LLM agents in uncertain supply chain management scenarios through a diverse series of inventory replenishment experiments. Our results reveal that different LLMs typically exhibit varying degrees of decision bias that are similar to those observed in human beings. In addition, we explored strategies to mitigate the pull-to-centre effect and the bullwhip effect, namely cognitive reflection and implementation of information sharing. These findings underscore the need for careful consideration of the potential biases in deploying LLMs in Inventory decision-making scenarios. We hope that these insights will pave the way for mitigating human decision bias and developing human-centred decision support systems for supply chains.", "source": "arxiv", "arxiv_id": "2508.11416v1", "pdf_url": "https://arxiv.org/pdf/2508.11416v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-15T11:38:19Z", "updated": "2025-08-15T11:38:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery", "authors": ["Yuqi Yin", "Yibo Fu", "Siyuan Wang", "Peng Sun", "Hongyu Wang", "Xiaohui Wang", "Lei Zheng", "Zhiyong Li", "Zhirong Liu", "Jianji Wang", "Zhaoxi Sun"], "year": 2025, "url": "http://arxiv.org/abs/2511.11257v1", "abstract": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.", "source": "arxiv", "arxiv_id": "2511.11257v1", "pdf_url": "https://arxiv.org/pdf/2511.11257v1", "categories": ["cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-14T12:53:57Z", "updated": "2025-11-14T12:53:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning", "authors": ["Edward Y. Chang", "Longling Geng"], "year": 2025, "url": "http://arxiv.org/abs/2505.12501v1", "abstract": "Large language models (LLMs) excel at rapid generation of text and multimodal content, yet they falter on transaction-style planning that demands ACID-like guarantees and real-time disruption recovery. We present Adaptive LLM Agent System (ALAS), a framework that tackles four fundamental LLM deficits: (i) absence of self-verification, (ii) context erosion, (iii) next-token myopia, and (iv) lack of persistent state. ALAS decomposes each plan into role-specialized agents, equips them with automatic state tracking, and coordinates them through a lightweight protocol. When disruptions arise, agents apply history-aware local compensation, avoiding costly global replanning and containing cascade effects. On real-world, large-scale job-shop scheduling benchmarks, ALAS sets new best results for static sequential planning and excels in dynamic reactive scenarios with unexpected disruptions. These gains show that principled modularization plus targeted compensation can unlock scalable and resilient planning with LLMs.", "source": "arxiv", "arxiv_id": "2505.12501v1", "pdf_url": "https://arxiv.org/pdf/2505.12501v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-18T17:27:08Z", "updated": "2025-05-18T17:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments", "authors": ["Pedro Gimenes", "Zeyu Cao", "Jeffrey Wong", "Yiren Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.21208v1", "abstract": "Recent research has shown that LLM performance on reasoning tasks can be enhanced by scaling test-time compute. One promising approach, particularly with decomposable problems, involves arranging intermediate solutions as a graph on which transformations are performed to explore the solution space. However, prior works rely on pre-determined, task-specific transformation schedules which are subject to a set of searched hyperparameters. In this work, we view thought graph transformations as actions in a Markov decision process, and implement policy agents to drive effective action policies for the underlying reasoning LLM agent. In particular, we investigate the ability for another LLM to act as a policy agent on thought graph environments and introduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES, reasoning LLM agents solve decomposed subproblems, while policy LLM agents maintain visibility of the thought graph states, and dynamically adapt the problem-solving strategy. Through extensive experiments, we observe that using off-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can yield up to $29\\%$ higher accuracy on HumanEval relative to static transformation schedules, as well as reducing inference costs by $35\\%$ and avoid any search requirements. We also conduct a thorough analysis of observed failure modes, highlighting that limitations on LLM sizes and the depth of problem decomposition can be seen as challenges to scaling LLM-guided reasoning.", "source": "arxiv", "arxiv_id": "2502.21208v1", "pdf_url": "https://arxiv.org/pdf/2502.21208v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-28T16:28:13Z", "updated": "2025-02-28T16:28:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination", "authors": ["Charidimos Papadakis", "Angeliki Dimitriou", "Giorgos Filandrianos", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "year": 2025, "url": "http://arxiv.org/abs/2510.15949v2", "abstract": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.", "source": "arxiv", "arxiv_id": "2510.15949v2", "pdf_url": "https://arxiv.org/pdf/2510.15949v2", "categories": ["q-fin.TR", "cs.AI"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2025-10-10T13:01:51Z", "updated": "2026-01-08T13:08:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Abstract Counterfactuals for Language Model Agents", "authors": ["Edoardo Pona", "Milad Kazemi", "Yali Du", "David Watson", "Nicola Paoletti"], "year": 2025, "url": "http://arxiv.org/abs/2506.02946v1", "abstract": "Counterfactual inference is a powerful tool for analysing and evaluating autonomous agents, but its application to language model (LM) agents remains challenging. Existing work on counterfactuals in LMs has primarily focused on token-level counterfactuals, which are often inadequate for LM agents due to their open-ended action spaces. Unlike traditional agents with fixed, clearly defined action spaces, the actions of LM agents are often implicit in the strings they output, making their action spaces difficult to define and interpret. Furthermore, the meanings of individual tokens can shift depending on the context, adding complexity to token-level reasoning and sometimes leading to biased or meaningless counterfactuals. We introduce \\emph{Abstract Counterfactuals}, a framework that emphasises high-level characteristics of actions and interactions within an environment, enabling counterfactual reasoning tailored to user-relevant features. Our experiments demonstrate that the approach produces consistent and meaningful counterfactuals while minimising the undesired side effects of token-level methods. We conduct experiments on text-based games and counterfactual text generation, while considering both token-level and latent-space interventions.", "source": "arxiv", "arxiv_id": "2506.02946v1", "pdf_url": "https://arxiv.org/pdf/2506.02946v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-03T14:44:26Z", "updated": "2025-06-03T14:44:26Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization", "authors": ["Genghan Zhang", "Shaowei Zhu", "Anjiang Wei", "Zhenyu Song", "Allen Nie", "Zhen Jia", "Nandita Vijaykumar", "Yida Wang", "Kunle Olukotun"], "year": 2025, "url": "http://arxiv.org/abs/2511.15915v1", "abstract": "We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\\%$ to $61\\%$ on Trainium 1 and from $45\\%$ to $59\\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\\times$ cheaper.", "source": "arxiv", "arxiv_id": "2511.15915v1", "pdf_url": "https://arxiv.org/pdf/2511.15915v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-19T22:49:37Z", "updated": "2025-11-19T22:49:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Accelerating Two-Dimensional Materials Research via a Universal Interatomic Potential and Large Language Model Agent", "authors": ["Haidi Wang", "Yufan Yao", "Haonan Song", "Xiaofeng Liu", "Zhao Chen", "Weiwei Chen", "Weiduo Zhu", "Zhongjun Li", "Jinlong Yang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07043v2", "abstract": "Accurate interatomic potentials (IAPs) are essential for modeling the potential energy surfaces (PES) that govern atomic interactions in materials. However, most existing IAPs are developed for bulk materials and struggle to accurately and efficiently capture the diverse chemical environment of two-dimensional (2D) materials. This limitation poses a significant barrier to the large-scale design and simulation of emerging 2D systems. To address this challenge, we present a universal interatomic potential tailored for 2D materials. Our model is trained on a dataset comprising 327,062 structure-energy-force-stress mappings derived from 20,114 2D materials, spanning 89 chemical elements. The results show high predictive accuracy, with mean absolute errors of 6 meV/atom for energies, 80 meV/Ãfor atomic forces, and 0.067 GPa for stress tensors. It demonstrates broad applicability across a range of atomistic tasks, including structural relaxation, lattice dynamics, molecular dynamics, material discovery, and so on. To further enhance usability and accessibility, we introduce an intelligent agent powered by a large language model (LLM), enabling natural language interaction for 2D materials property simulations. Our work provides not only a precise and universal IAP for 2D systems, but also an intelligent, user-friendly platform that enables high-throughput screening, property prediction, and theoretical exploration, thereby accelerating advances in 2D materials research.", "source": "arxiv", "arxiv_id": "2506.07043v2", "pdf_url": "https://arxiv.org/pdf/2506.07043v2", "categories": ["cond-mat.mtrl-sci"], "primary_category": "cond-mat.mtrl-sci", "doi": "", "venue": "", "published": "2025-06-08T08:41:47Z", "updated": "2025-10-16T01:45:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning", "authors": ["Haiteng Zhao", "Junhao Shen", "Yiming Zhang", "Songyang Gao", "Kuikun Liu", "Tianyou Ma", "Fan Zheng", "Dahua Lin", "Wenwei Zhang", "Kai Chen"], "year": 2025, "url": "http://arxiv.org/abs/2512.10534v2", "abstract": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.", "source": "arxiv", "arxiv_id": "2512.10534v2", "pdf_url": "https://arxiv.org/pdf/2512.10534v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-11T11:05:04Z", "updated": "2025-12-12T13:43:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents", "authors": ["Qiusi Zhan", "Richard Fang", "Henil Shalin Panchal", "Daniel Kang"], "year": 2025, "url": "http://arxiv.org/abs/2503.00061v2", "abstract": "Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks. In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%. This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability. The code is available at https://github.com/uiuc-kang-lab/AdaptiveAttackAgent.", "source": "arxiv", "arxiv_id": "2503.00061v2", "pdf_url": "https://arxiv.org/pdf/2503.00061v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-27T04:04:50Z", "updated": "2025-03-04T03:32:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Addressing the sustainable AI trilemma: a case study on LLM agents and RAG", "authors": ["Hui Wu", "Xiaoyang Wang", "Zhong Fan"], "year": 2025, "url": "http://arxiv.org/abs/2501.08262v1", "abstract": "Large language models (LLMs) have demonstrated significant capabilities, but their widespread deployment and more advanced applications raise critical sustainability challenges, particularly in inference energy consumption. We propose the concept of the Sustainable AI Trilemma, highlighting the tensions between AI capability, digital equity, and environmental sustainability. Through a systematic case study of LLM agents and retrieval-augmented generation (RAG), we analyze the energy costs embedded in memory module designs and introduce novel metrics to quantify the trade-offs between energy consumption and system performance. Our experimental results reveal significant energy inefficiencies in current memory-augmented frameworks and demonstrate that resource-constrained environments face disproportionate efficiency penalties. Our findings challenge the prevailing LLM-centric paradigm in agent design and provide practical insights for developing more sustainable AI systems.", "source": "arxiv", "arxiv_id": "2501.08262v1", "pdf_url": "https://arxiv.org/pdf/2501.08262v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-01-14T17:21:16Z", "updated": "2025-01-14T17:21:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety", "authors": ["Zizhao Wang", "Dingcheng Li", "Vaishakh Keshava", "Phillip Wallis", "Ananth Balashankar", "Peter Stone", "Lukas Rutishauser"], "year": 2025, "url": "http://arxiv.org/abs/2510.05442v1", "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.", "source": "arxiv", "arxiv_id": "2510.05442v1", "pdf_url": "https://arxiv.org/pdf/2510.05442v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-06T23:09:18Z", "updated": "2025-10-06T23:09:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents", "authors": ["Kevin Song", "Anand Jayarajan", "Yaoyao Ding", "Qidong Su", "Zhanda Zhu", "Sihang Liu", "Gennady Pekhimenko"], "year": 2025, "url": "http://arxiv.org/abs/2508.19504v1", "abstract": "Large Language Models (LLMs) agents augmented with domain tools promise to autonomously execute complex tasks requiring human-level intelligence, such as customer service and digital assistance. However, their practical deployment is often limited by their low success rates under complex real-world environments. To tackle this, prior research has primarily focused on improving the agents themselves, such as developing strong agentic LLMs, while overlooking the role of the system environment in which the agent operates.\n  In this paper, we study a complementary direction: improving agent success rates by optimizing the system environment in which the agent operates. We collect 142 agent traces (3,656 turns of agent-environment interactions) across 5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we propose a taxonomy for agent-environment interaction failures that includes 6 failure modes. Guided by these findings, we design Aegis, a set of targeted environment optimizations: 1) environment observability enhancement, 2) common computation offloading, and 3) speculative agentic actions. These techniques improve agent success rates on average by 6.7-12.5%, without any modifications to the agent and underlying LLM.", "source": "arxiv", "arxiv_id": "2508.19504v1", "pdf_url": "https://arxiv.org/pdf/2508.19504v1", "categories": ["cs.MA", "cs.DC"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-08-27T01:29:46Z", "updated": "2025-08-27T01:29:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?", "authors": ["Guibin Zhang", "Junhao Wang", "Junjie Chen", "Wangchunshu Zhou", "Kun Wang", "Shuicheng Yan"], "year": 2025, "url": "http://arxiv.org/abs/2509.03312v2", "abstract": "Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI.", "source": "arxiv", "arxiv_id": "2509.03312v2", "pdf_url": "https://arxiv.org/pdf/2509.03312v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-03T13:42:14Z", "updated": "2025-09-04T17:49:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent Safety Alignment via Reinforcement Learning", "authors": ["Zeyang Sha", "Hanling Tian", "Zhuoer Xu", "Shiwen Cui", "Changhua Meng", "Weiqiang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.08270v1", "abstract": "The emergence of autonomous Large Language Model (LLM) agents capable of tool usage has introduced new safety risks that go beyond traditional conversational misuse. These agents, empowered to execute external functions, are vulnerable to both user-initiated threats (e.g., adversarial prompts) and tool-initiated threats (e.g., malicious outputs from compromised tools). In this paper, we propose the first unified safety-alignment framework for tool-using agents, enabling models to handle both channels of threat via structured reasoning and sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including benign, malicious, and sensitive for both user prompts and tool responses, and define a policy-driven decision model. Our framework employs a custom-designed sandbox environment that simulates real-world tool execution and allows fine-grained reward shaping. Through extensive evaluations on public and self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we demonstrate that our safety-aligned agents significantly improve resistance to security threats while preserving strong utility on benign tasks. Our results show that safety and effectiveness can be jointly optimized, laying the groundwork for trustworthy deployment of autonomous LLM agents.", "source": "arxiv", "arxiv_id": "2507.08270v1", "pdf_url": "https://arxiv.org/pdf/2507.08270v1", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-11T02:34:16Z", "updated": "2025-07-11T02:34:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent-Enhanced Large Language Models for Researching Political Institutions", "authors": ["Joseph R. Loffredo", "Suyeol Yun"], "year": 2025, "url": "http://arxiv.org/abs/2503.13524v1", "abstract": "The applications of Large Language Models (LLMs) in political science are rapidly expanding. This paper demonstrates how LLMs, when augmented with predefined functions and specialized tools, can serve as dynamic agents capable of streamlining tasks such as data collection, preprocessing, and analysis. Central to this approach is agentic retrieval-augmented generation (Agentic RAG), which equips LLMs with action-calling capabilities for interaction with external knowledge bases. Beyond information retrieval, LLM agents may incorporate modular tools for tasks like document summarization, transcript coding, qualitative variable classification, and statistical modeling. To demonstrate the potential of this approach, we introduce CongressRA, an LLM agent designed to support scholars studying the U.S. Congress. Through this example, we highlight how LLM agents can reduce the costs of replicating, testing, and extending empirical research using the domain-specific data that drives the study of political institutions.", "source": "arxiv", "arxiv_id": "2503.13524v1", "pdf_url": "https://arxiv.org/pdf/2503.13524v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "10.1561/113.00000125", "venue": "", "published": "2025-03-14T22:04:40Z", "updated": "2025-03-14T22:04:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "authors": ["Mingyue Cheng", "Jie Ouyang", "Shuo Yu", "Ruiran Yan", "Yucong Luo", "Zirui Liu", "Daoyu Wang", "Qi Liu", "Enhong Chen"], "year": 2025, "url": "http://arxiv.org/abs/2511.14460v1", "abstract": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.", "source": "arxiv", "arxiv_id": "2511.14460v1", "pdf_url": "https://arxiv.org/pdf/2511.14460v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-18T13:03:15Z", "updated": "2025-11-18T13:03:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training", "authors": ["Siyu Yuan", "Zehui Chen", "Zhiheng Xi", "Junjie Ye", "Zhengyin Du", "Jiecao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2501.11425v3", "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%).", "source": "arxiv", "arxiv_id": "2501.11425v3", "pdf_url": "https://arxiv.org/pdf/2501.11425v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-20T11:46:04Z", "updated": "2025-03-24T10:18:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "authors": ["Hoang Pham", "Thuy-Duong Nguyen", "Khac-Hoai Nam Bui"], "year": 2025, "url": "http://arxiv.org/abs/2505.22571v3", "abstract": "This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.", "source": "arxiv", "arxiv_id": "2505.22571v3", "pdf_url": "https://arxiv.org/pdf/2505.22571v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T16:46:31Z", "updated": "2025-05-30T02:44:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text for Enhanced Recommendations", "authors": ["BlaÅ¾ Å krlj", "BenoÃ®t Guilleminot", "AndraÅ¾ Tori"], "year": 2025, "url": "http://arxiv.org/abs/2507.18993v1", "abstract": "Large language models (LLMs) and their associated agent-based frameworks have significantly advanced automated information extraction, a critical component of modern recommender systems. While these multitask frameworks are widely used in code generation, their application in data-centric research is still largely untapped. This paper presents Agent0, an LLM-driven, agent-based system designed to automate information extraction and feature construction from raw, unstructured text. Categorical features are crucial for large-scale recommender systems but are often expensive to acquire. Agent0 coordinates a group of interacting LLM agents to automatically identify the most valuable text aspects for subsequent tasks (such as models or AutoML pipelines). Beyond its feature engineering capabilities, Agent0 also offers an automated prompt-engineering tuning method that utilizes dynamic feedback loops from an oracle. Our findings demonstrate that this closed-loop methodology is both practical and effective for automated feature discovery, which is recognized as one of the most challenging phases in current recommender system development.", "source": "arxiv", "arxiv_id": "2507.18993v1", "pdf_url": "https://arxiv.org/pdf/2507.18993v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-07-25T06:45:10Z", "updated": "2025-07-25T06:45:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Hansu Gu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09723v3", "abstract": "A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.", "source": "arxiv", "arxiv_id": "2504.09723v3", "pdf_url": "https://arxiv.org/pdf/2504.09723v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-04-13T21:10:56Z", "updated": "2025-09-19T17:56:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentDNS: A Root Domain Naming System for LLM Agents", "authors": ["Enfang Cui", "Yujun Cheng", "Rui She", "Dan Liu", "Zhiyuan Liang", "Minxin Guo", "Tianzheng Li", "Qian Wei", "Wenjuan Xing", "Zhijie Zhong"], "year": 2025, "url": "http://arxiv.org/abs/2505.22368v1", "abstract": "The rapid evolution of Large Language Model (LLM) agents has highlighted critical challenges in cross-vendor service discovery, interoperability, and communication. Existing protocols like model context protocol and agent-to-agent protocol have made significant strides in standardizing interoperability between agents and tools, as well as communication among multi-agents. However, there remains a lack of standardized protocols and solutions for service discovery across different agent and tool vendors. In this paper, we propose AgentDNS, a root domain naming and service discovery system designed to enable LLM agents to autonomously discover, resolve, and securely invoke third-party agent and tool services across organizational and technological boundaries. Inspired by the principles of the traditional DNS, AgentDNS introduces a structured mechanism for service registration, semantic service discovery, secure invocation, and unified billing. We detail the architecture, core functionalities, and use cases of AgentDNS, demonstrating its potential to streamline multi-agent collaboration in real-world scenarios. The source code will be published on https://github.com/agentdns.", "source": "arxiv", "arxiv_id": "2505.22368v1", "pdf_url": "https://arxiv.org/pdf/2505.22368v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-28T13:56:22Z", "updated": "2025-05-28T13:56:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent", "authors": ["Yu Li", "Lehui Li", "Qingmin Liao", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.04921v1", "abstract": "Large language model agents are becoming increasingly capable at web-centric tasks such as information retrieval, complex reasoning. These emerging capabilities have given rise to surge research interests in developing LLM agent for facilitating scientific quest. One key application in AI research is to automate experiment design through agentic dataset and baseline retrieval. However, prior efforts suffer from limited data coverage, as recommendation datasets primarily harvest candidates from public portals and omit many datasets actually used in published papers, and from an overreliance on content similarity that biases model toward superficial similarity and overlooks experimental suitability. Harnessing collective perception embedded in the baseline and dataset citation network, we present a comprehensive framework for baseline and dataset recommendation. First, we design an automated data-collection pipeline that links roughly one hundred thousand accepted papers to the baselines and datasets they actually used. Second, we propose a collective perception enhanced retriever. To represent the position of each dataset or baseline within the scholarly network, it concatenates self-descriptions with aggregated citation contexts. To achieve efficient candidate recall, we finetune an embedding model on these representations. Finally, we develop a reasoning-augmented reranker that exact interaction chains to construct explicit reasoning chains and finetunes a large language model to produce interpretable justifications and refined rankings. The dataset we curated covers 85\\% of the datasets and baselines used at top AI conferences over the past five years. On our dataset, the proposed method outperforms the strongest prior baseline with average gains of +5.85\\% in Recall@20, +8.30\\% in HitRate@5. Taken together, our results advance reliable, interpretable automation of experimental design.", "source": "arxiv", "arxiv_id": "2511.04921v1", "pdf_url": "https://arxiv.org/pdf/2511.04921v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-07T01:51:56Z", "updated": "2025-11-07T01:51:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2510.24695v1", "abstract": "Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.", "source": "arxiv", "arxiv_id": "2510.24695v1", "pdf_url": "https://arxiv.org/pdf/2510.24695v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T17:50:47Z", "updated": "2025-10-28T17:50:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration", "authors": ["Jizhou Chen", "Samuel Lee Cong"], "year": 2025, "url": "http://arxiv.org/abs/2502.09809v1", "abstract": "The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AgentGuard leverages the LLM orchestrator's innate capabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges - to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated constraints, enables multiple security applications. We empirically demonstrate AgentGuard's feasibility with experiments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening procedures for LLM agents to enhance their trustworthiness in real-world applications.", "source": "arxiv", "arxiv_id": "2502.09809v1", "pdf_url": "https://arxiv.org/pdf/2502.09809v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-13T23:00:33Z", "updated": "2025-02-13T23:00:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning", "authors": ["Zhiheng Xi", "Jixuan Huang", "Chenyang Liao", "Baodai Huang", "Honglin Guo", "Jiaqi Liu", "Rui Zheng", "Junjie Ye", "Jiazheng Zhang", "Wenxiang Chen", "Wei He", "Yiwen Ding", "Guanyu Li", "Zehui Chen", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Tao Gui", "Zuxuan Wu", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2509.08755v1", "abstract": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.", "source": "arxiv", "arxiv_id": "2509.08755v1", "pdf_url": "https://arxiv.org/pdf/2509.08755v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-10T16:46:11Z", "updated": "2025-09-10T16:46:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress", "authors": ["Zhiheng Xi", "Chenyang Liao", "Guanyu Li", "Yajie Yang", "Wenxiang Chen", "Zhihao Zhang", "Binghai Wang", "Senjie Jin", "Yuhao Zhou", "Jian Guan", "Wei Wu", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "year": 2025, "url": "http://arxiv.org/abs/2511.08325v1", "abstract": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.", "source": "arxiv", "arxiv_id": "2511.08325v1", "pdf_url": "https://arxiv.org/pdf/2511.08325v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T14:57:54Z", "updated": "2025-11-11T14:57:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "authors": ["Yu Shang", "Peijie Liu", "Yuwei Yan", "Zijing Wu", "Leheng Sheng", "Yuanqing Yu", "Chumeng Jiang", "An Zhang", "Fengli Xu", "Yu Wang", "Min Zhang", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.19623v2", "abstract": "The emergence of agentic recommender systems powered by Large Language Models (LLMs) represents a paradigm shift in personalized recommendations, leveraging LLMs' advanced reasoning and role-playing capabilities to enable autonomous, adaptive decision-making. Unlike traditional recommendation approaches, agentic recommender systems can dynamically gather and interpret user-item interactions from complex environments, generating robust recommendation strategies that generalize across diverse scenarios. However, the field currently lacks standardized evaluation protocols to systematically assess these methods. To address this critical gap, we propose: (1) an interactive textual recommendation simulator incorporating rich user and item metadata and three typical evaluation scenarios (classic, evolving-interest, and cold-start recommendation tasks); (2) a unified modular framework for developing and studying agentic recommender systems; and (3) the first comprehensive benchmark comparing 10 classical and agentic recommendation methods. Our findings demonstrate the superiority of agentic systems and establish actionable design guidelines for their core components. The benchmark environment has been rigorously validated through an open challenge and remains publicly available with a continuously maintained leaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html}, fostering ongoing community engagement and reproducible research. The benchmark is available at: \\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.", "source": "arxiv", "arxiv_id": "2505.19623v2", "pdf_url": "https://arxiv.org/pdf/2505.19623v2", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-05-26T07:45:11Z", "updated": "2025-05-28T14:32:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "authors": ["Miriam Horovicz"], "year": 2025, "url": "http://arxiv.org/abs/2512.12597v1", "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.", "source": "arxiv", "arxiv_id": "2512.12597v1", "pdf_url": "https://arxiv.org/pdf/2512.12597v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-14T08:31:43Z", "updated": "2025-12-14T08:31:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms", "authors": ["Yuwei Yan", "Yu Shang", "Qingbin Zeng", "Yu Li", "Keyu Zhao", "Zhiheng Zheng", "Xuefei Ning", "Tianji Wu", "Shengen Yan", "Yu Wang", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2502.18754v1", "abstract": "The AgentSociety Challenge is the first competition in the Web Conference that aims to explore the potential of Large Language Model (LLM) agents in modeling user behavior and enhancing recommender systems on web platforms. The Challenge consists of two tracks: the User Modeling Track and the Recommendation Track. Participants are tasked to utilize a combined dataset from Yelp, Amazon, and Goodreads, along with an interactive environment simulator, to develop innovative LLM agents. The Challenge has attracted 295 teams across the globe and received over 1,400 submissions in total over the course of 37 official competition days. The participants have achieved 21.9% and 20.3% performance improvement for Track 1 and Track 2 in the Development Phase, and 9.1% and 15.9% in the Final Phase, representing a significant accomplishment. This paper discusses the detailed designs of the Challenge, analyzes the outcomes, and highlights the most successful LLM agent designs. To support further research and development, we have open-sourced the benchmark environment at https://tsinghua-fib-lab.github.io/AgentSocietyChallenge.", "source": "arxiv", "arxiv_id": "2502.18754v1", "pdf_url": "https://arxiv.org/pdf/2502.18754v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-02-26T02:10:25Z", "updated": "2025-02-26T02:10:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "year": 2025, "url": "http://arxiv.org/abs/2503.18666v3", "abstract": "Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.", "source": "arxiv", "arxiv_id": "2503.18666v3", "pdf_url": "https://arxiv.org/pdf/2503.18666v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-24T13:31:48Z", "updated": "2025-07-31T04:00:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search", "authors": ["Yu Li", "Lehui Li", "Zhihao Wu", "Qingmin Liao", "Jianye Hao", "Kun Shao", "Fengli Xu", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.06017v2", "abstract": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains, yet automated agent design remains a significant challenge. Current automated agent design approaches are often constrained by limited search spaces that primarily optimize workflows but fail to integrate crucial human-designed components like memory, planning, and tool use. Furthermore, these methods are hampered by high evaluation costs, as evaluating even a single new agent on a benchmark can require tens of dollars. The difficulty of this exploration is further exacerbated by inefficient search strategies that struggle to navigate the large design space effectively, making the discovery of novel agents a slow and resource-intensive process. To address these challenges, we propose AgentSwift, a novel framework for automated agent design. We formalize a hierarchical search space that jointly models agentic workflow and composable functional components. This structure moves beyond optimizing workflows alone by co-optimizing functional components, which enables the discovery of more complex and effective agent architectures. To make exploration within this expansive space feasible, we mitigate high evaluation costs by training a value model on a high-quality dataset, generated via a novel strategy combining combinatorial coverage and balanced Bayesian sampling for low-cost evaluation. Guiding the entire process is a hierarchical MCTS strategy, which is informed by uncertainty to efficiently navigate the search space. Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents. Our framework serves as a launchpad for researchers to rapidly discover powerful agent architectures.", "source": "arxiv", "arxiv_id": "2506.06017v2", "pdf_url": "https://arxiv.org/pdf/2506.06017v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-06T12:07:23Z", "updated": "2025-11-20T15:55:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.00890v2", "abstract": "Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.", "source": "arxiv", "arxiv_id": "2508.00890v2", "pdf_url": "https://arxiv.org/pdf/2508.00890v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-26T19:21:18Z", "updated": "2025-10-21T20:46:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentVigil: Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents", "authors": ["Zhun Wang", "Vincent Siu", "Zhe Ye", "Tianneng Shi", "Yuzhou Nie", "Xuandong Zhao", "Chenguang Wang", "Wenbo Guo", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2505.05849v4", "abstract": "The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentVigil, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentVigil on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentVigil exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.", "source": "arxiv", "arxiv_id": "2505.05849v4", "pdf_url": "https://arxiv.org/pdf/2505.05849v4", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-09T07:40:17Z", "updated": "2025-06-14T01:50:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents", "authors": ["Hao Li", "Haotian Chen", "Ruoyuan Gong", "Juanjuan Wang", "Hao Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2511.04076v2", "abstract": "Redistricting plays a central role in shaping how votes are translated into political power. While existing computational methods primarily aim to generate large ensembles of legally valid districting plans, they often neglect the strategic dynamics involved in the selection process. This oversight creates opportunities for partisan actors to cherry-pick maps that, while technically compliant, are politically advantageous. Simply satisfying formal constraints does not ensure fairness when the selection process itself can be manipulated. We propose \\textbf{Agentmandering}, a framework that reimagines redistricting as a turn-based negotiation between two agents representing opposing political interests. Drawing inspiration from game-theoretic ideas, particularly the \\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction into the redistricting process via large language model (LLM) agents. Agents alternate between selecting and freezing districts from a small set of candidate maps, gradually partitioning the state through constrained and interpretable choices. Evaluation on post-2020 U.S. Census data across all states shows that Agentmandering significantly reduces partisan bias and unfairness, while achieving 2 to 3 orders of magnitude lower variance than standard baselines. These results demonstrate both fairness and stability, especially in swing-state scenarios. Our code is available at https://github.com/Lihaogx/AgentMandering.", "source": "arxiv", "arxiv_id": "2511.04076v2", "pdf_url": "https://arxiv.org/pdf/2511.04076v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-06T05:28:55Z", "updated": "2025-11-09T02:33:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning", "authors": ["Nikolas Belle", "Dakota Barnes", "Alfonso Amayuelas", "Ivan Bercovich", "Xin Eric Wang", "William Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.04651v2", "abstract": "We address the long-horizon gap in large language model (LLM) agents by enabling them to sustain coherent strategies in adversarial, stochastic environments. Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking. Prompt-centric LLM agents (e.g., ReAct, Reflexion) must re-interpret large, evolving game states each turn, quickly saturating context windows and losing strategic consistency. We propose HexMachina, a continual learning multi-agent system that separates environment discovery (inducing an adapter layer without documentation) from strategy improvement (evolving a compiled player through code refinement and simulation). This design preserves executable artifacts, allowing the LLM to focus on high-level strategy rather than per-turn reasoning. In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines. Ablations confirm that isolating pure strategy learning improves performance. Overall, artifact-centric continual learning transforms LLMs from brittle stepwise deciders into stable strategy designers, advancing long-horizon autonomy.", "source": "arxiv", "arxiv_id": "2506.04651v2", "pdf_url": "https://arxiv.org/pdf/2506.04651v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-05T05:45:24Z", "updated": "2025-10-13T08:57:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents", "authors": ["Seohui Bae", "Jeonghye Kim", "Youngchul Sung", "Woohyung Lim"], "year": 2025, "url": "http://arxiv.org/abs/2512.24461v1", "abstract": "In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.", "source": "arxiv", "arxiv_id": "2512.24461v1", "pdf_url": "https://arxiv.org/pdf/2512.24461v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-30T20:51:28Z", "updated": "2025-12-30T20:51:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2511.00993v1", "abstract": "Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.", "source": "arxiv", "arxiv_id": "2511.00993v1", "pdf_url": "https://arxiv.org/pdf/2511.00993v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-02T16:05:33Z", "updated": "2025-11-02T16:05:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach", "authors": ["Wei Lu", "Daniel L. Chen", "Christian B. Hansen"], "year": 2025, "url": "http://arxiv.org/abs/2507.20796v1", "abstract": "Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.", "source": "arxiv", "arxiv_id": "2507.20796v1", "pdf_url": "https://arxiv.org/pdf/2507.20796v1", "categories": ["econ.GN", "cs.AI", "cs.LG"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-07-28T13:05:04Z", "updated": "2025-07-28T13:05:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails", "authors": ["Siwei Han", "Jiaqi Liu", "Yaofeng Su", "Wenbo Duan", "Xinyuan Liu", "Cihang Xie", "Mohit Bansal", "Mingyu Ding", "Linjun Zhang", "Huaxiu Yao"], "year": 2025, "url": "http://arxiv.org/abs/2510.04860v1", "abstract": "As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.", "source": "arxiv", "arxiv_id": "2510.04860v1", "pdf_url": "https://arxiv.org/pdf/2510.04860v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-06T14:48:39Z", "updated": "2025-10-06T14:48:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations", "authors": ["Ron F. Del Rosario", "Klaudia Krawiecka", "Christian Schroeder de Witt"], "year": 2025, "url": "http://arxiv.org/abs/2509.08646v1", "abstract": "As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount. This paper provides a comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic design that separates strategic planning from tactical execution. We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + Act). A central focus is placed on the security implications of this design, particularly its inherent resilience to indirect prompt injection attacks by establishing control-flow integrity. We argue that while P-t-E provides a strong foundation, a defense-in-depth strategy is necessary, and we detail essential complementary controls such as the Principle of Least Privilege, task-scoped tool access, and sandboxed code execution. To make these principles actionable, this guide provides detailed implementation blueprints and working code references for three leading agentic frameworks: LangChain (via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing the P-t-E pattern is analyzed, highlighting unique features like LangGraph's stateful graphs for re-planning, CrewAI's declarative tool scoping for security, and AutoGen's built-in Docker sandboxing. Finally, we discuss advanced patterns, including dynamic re-planning loops, parallel execution with Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop (HITL) verification, to offer a complete strategic blueprint for architects, developers, and security engineers aiming to build production-grade, resilient, and trustworthy LLM agents.", "source": "arxiv", "arxiv_id": "2509.08646v1", "pdf_url": "https://arxiv.org/pdf/2509.08646v1", "categories": ["cs.CR", "cs.AI", "eess.SY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-10T14:41:07Z", "updated": "2025-09-10T14:41:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across Enterprise Workflows", "authors": ["Petr PrÅ¯cha", "Michaela MatouÅ¡kovÃ¡", "Jan Strnad"], "year": 2025, "url": "http://arxiv.org/abs/2509.04198v1", "abstract": "The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.", "source": "arxiv", "arxiv_id": "2509.04198v1", "pdf_url": "https://arxiv.org/pdf/2509.04198v1", "categories": ["cs.CY", "cs.MA"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-09-04T13:22:44Z", "updated": "2025-09-04T13:22:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents", "authors": ["Keith Moore", "Jun W. Kim", "David Lyu", "Jeffrey Heo", "Ehsan Adeli"], "year": 2025, "url": "http://arxiv.org/abs/2511.14780v1", "abstract": "We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors (\"act like a neurologist\", \"act like an infectious disease specialist\"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.", "source": "arxiv", "arxiv_id": "2511.14780v1", "pdf_url": "https://arxiv.org/pdf/2511.14780v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-06T20:31:19Z", "updated": "2025-11-06T20:31:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AskDB: An LLM Agent for Natural Language Interaction with Relational Databases", "authors": ["Xuan-Quang Phan", "Tan-Ha Mai", "Thai-Duy Dinh", "Minh-Thuan Nguyen", "Lam-Son LÃª"], "year": 2025, "url": "http://arxiv.org/abs/2511.16131v1", "abstract": "Interacting with relational databases remains challenging for users across different expertise levels, particularly when composing complex analytical queries or performing administrative tasks. Existing systems typically address either natural language querying or narrow aspects of database administration, lacking a unified and intelligent interface for general-purpose database interaction. We introduce AskDB, a large language model powered agent designed to bridge this gap by supporting both data analysis and administrative operations over SQL databases through natural language. Built on Gemini 2, AskDB integrates two key innovations: a dynamic schema-aware prompting mechanism that effectively incorporates database metadata, and a task decomposition framework that enables the agent to plan and execute multi-step actions. These capabilities allow AskDB to autonomously debug derived SQL, retrieve contextual information via real-time web search, and adaptively refine its responses. We evaluate AskDB on a widely used Text-to-SQL benchmark and a curated set of DBA tasks, demonstrating strong performance in both analytical and administrative scenarios. Our results highlight the potential of AskDB as a unified and intelligent agent for relational database systems, offering an intuitive and accessible experience for end users.", "source": "arxiv", "arxiv_id": "2511.16131v1", "pdf_url": "https://arxiv.org/pdf/2511.16131v1", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-11-20T08:06:09Z", "updated": "2025-11-20T08:06:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools", "authors": ["Kanghua Mo", "Li Hu", "Yucheng Long", "Zhihao Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.02110v2", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface, where adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. The proposed attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even against prompt-level defenses, auditor-based detection, and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface. Notably, AMA is orthogonal to injection attacks and can be combined with them to achieve stronger attack efficacy, highlighting the need for execution-level defenses beyond prompt-level and auditor-based mechanisms. Code is available at https://github.com/SEAIC-M/AMA.", "source": "arxiv", "arxiv_id": "2508.02110v2", "pdf_url": "https://arxiv.org/pdf/2508.02110v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-04T06:38:59Z", "updated": "2026-01-07T07:28:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AudioToolAgent: An Agentic Framework for Audio-Language Models", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michel Dumontier"], "year": 2025, "url": "http://arxiv.org/abs/2510.02995v1", "abstract": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks but lack multi-step reasoning and tool-calling found in recent Large Language Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates audio-language models as tools via a central LLM agent that accesses tool adapters for audio question answering and speech-to-text. The agent selects tools, asks follow-up questions, and compares outputs for verification. Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to 74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling for shapley values across 374 configurations identifies effective agent-tool combinations. The modular design allows integration of new tools and eliminates the use of data and training costs. Code and reproduction materials are available at: github.com/GLJS/AudioToolAgent", "source": "arxiv", "arxiv_id": "2510.02995v1", "pdf_url": "https://arxiv.org/pdf/2510.02995v1", "categories": ["cs.SD"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2025-10-03T13:35:45Z", "updated": "2025-10-03T13:35:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents", "authors": ["Jiabin Tang", "Tianyu Fan", "Chao Huang"], "year": 2025, "url": "http://arxiv.org/abs/2502.05957v3", "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", "source": "arxiv", "arxiv_id": "2502.05957v3", "pdf_url": "https://arxiv.org/pdf/2502.05957v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-09T16:53:56Z", "updated": "2025-10-09T07:27:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoBackdoor: Automating Backdoor Attacks via LLM Agents", "authors": ["Yige Li", "Zhe Li", "Wei Zhao", "Nay Myat Min", "Hanxun Huang", "Xingjun Ma", "Jun Sun"], "year": 2025, "url": "http://arxiv.org/abs/2511.16709v1", "abstract": "Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \\textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \\textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \\textit{Bias Recommendation}, \\textit{Hallucination Injection}, and \\textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.", "source": "arxiv", "arxiv_id": "2511.16709v1", "pdf_url": "https://arxiv.org/pdf/2511.16709v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-20T03:58:54Z", "updated": "2025-11-20T03:58:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoContext: Instance-Level Context Learning for LLM Agents", "authors": ["Kuntai Cai", "Juncheng Liu", "Xianglin Yang", "Zhaojie Niu", "Xiaokui Xiao", "Xing Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.02369v3", "abstract": "Current LLM agents typically lack instance-level context, which comprises concrete facts such as environment structure, system configurations, and local mechanics. Consequently, existing methods are forced to intertwine exploration with task execution. This coupling leads to redundant interactions and fragile decision-making, as agents must repeatedly rediscover the same information for every new task. To address this, we introduce AutoContext, a method that decouples exploration from task solving. AutoContext performs a systematic, one-off exploration to construct a reusable knowledge graph for each environment instance. This structured context allows off-the-shelf agents to access necessary facts directly, eliminating redundant exploration. Experiments across TextWorld, ALFWorld, Crafter, and InterCode-Bash demonstrate substantial gains: for example, the success rate of a ReAct agent on TextWorld improves from 37% to 95%, highlighting the critical role of structured instance context in efficient agentic systems.", "source": "arxiv", "arxiv_id": "2510.02369v3", "pdf_url": "https://arxiv.org/pdf/2510.02369v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-29T05:38:51Z", "updated": "2026-01-13T06:31:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoPDL: Automatic Prompt Optimization for LLM Agents", "authors": ["Claudio Spiess", "Mandana Vaziri", "Louis Mandel", "Martin Hirzel"], "year": 2025, "url": "http://arxiv.org/abs/2504.04365v5", "abstract": "The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.", "source": "arxiv", "arxiv_id": "2504.04365v5", "pdf_url": "https://arxiv.org/pdf/2504.04365v5", "categories": ["cs.LG", "cs.AI", "cs.PL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-06T05:30:10Z", "updated": "2025-11-03T21:46:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents", "authors": ["Julius Henke"], "year": 2025, "url": "http://arxiv.org/abs/2505.10321v1", "abstract": "A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of \\$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \\$20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management.", "source": "arxiv", "arxiv_id": "2505.10321v1", "pdf_url": "https://arxiv.org/pdf/2505.10321v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-15T14:06:00Z", "updated": "2025-05-15T14:06:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoPentester: An LLM Agent-based Framework for Automated Pentesting", "authors": ["Yasod Ginige", "Akila Niroshan", "Sajal Jain", "Suranga Seneviratne"], "year": 2025, "url": "http://arxiv.org/abs/2510.05605v1", "abstract": "Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.", "source": "arxiv", "arxiv_id": "2510.05605v1", "pdf_url": "https://arxiv.org/pdf/2510.05605v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-07T06:02:26Z", "updated": "2025-10-07T06:02:26Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment", "authors": ["Xiaochong Lan", "Jie Feng", "Yinxing Liu", "Xinlei Shi", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.08081v1", "abstract": "Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.", "source": "arxiv", "arxiv_id": "2510.08081v1", "pdf_url": "https://arxiv.org/pdf/2510.08081v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-09T11:11:02Z", "updated": "2025-10-09T11:11:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoTool: Efficient Tool Selection for Large Language Model Agents", "authors": ["Jingyi Jia", "Qinbin Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.14650v1", "abstract": "Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.", "source": "arxiv", "arxiv_id": "2511.14650v1", "pdf_url": "https://arxiv.org/pdf/2511.14650v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-18T16:41:48Z", "updated": "2025-11-18T16:41:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Automated Penetration Testing with LLM Agents and Classical Planning", "authors": ["Lingzhi Wang", "Xinyi Shi", "Ziyu Li", "Yi Jiang", "Shiyu Tan", "Yuhao Jiang", "Junjie Cheng", "Wenyuan Chen", "Xiangmin Shen", "Zhenyuan LI", "Yan Chen"], "year": 2025, "url": "http://arxiv.org/abs/2512.11143v1", "abstract": "While penetration testing plays a vital role in cybersecurity, achieving fully automated, hands-off-the-keyboard execution remains a significant research challenge. In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area. We also evaluate existing penetration testing systems, with a particular focus on the use of Large Language Model (LLM) agents for this task. The results show that the out-of-the-box Claude Code and Sonnet 4.5 exhibit superior penetration capabilities observed to date, substantially outperforming all prior systems. However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. These limitations significantly constrain its overall capability, efficiency, and stability. To address these limitations, we propose CHECKMATE, a framework that integrates enhanced classical planning with LLM agents, providing an external, structured \"brain\" that mitigates the inherent weaknesses of LLM agents. Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%. In addition, it delivers substantially greater stability, cutting both time and monetary costs by more than 50%.", "source": "arxiv", "arxiv_id": "2512.11143v1", "pdf_url": "https://arxiv.org/pdf/2512.11143v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-11T22:04:39Z", "updated": "2025-12-11T22:04:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Automated Profile Inference with Language Model Agents", "authors": ["Yuntao Du", "Zitao Li", "Bolin Ding", "Yaliang Li", "Hanshen Xiao", "Jingren Zhou", "Ninghui Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.12402v1", "abstract": "Impressive progress has been made in automated problem-solving by the collaboration of large language models (LLMs) based agents. However, these automated capabilities also open avenues for malicious applications. In this paper, we study a new threat that LLMs pose to online pseudonymity, called automated profile inference, where an adversary can instruct LLMs to automatically scrape and extract sensitive personal attributes from publicly visible user activities on pseudonymous platforms. We also introduce an automated profiling framework called AutoProfiler to assess the feasibility of such threats in real-world scenarios. AutoProfiler consists of four specialized LLM agents, who work collaboratively to collect and process user online activities and generate a profile with extracted personal information. Experimental results on two real-world datasets and one synthetic dataset demonstrate that AutoProfiler is highly effective and efficient, and can be easily deployed on a web scale. We demonstrate that the inferred attributes are both sensitive and identifiable, posing significant risks of privacy breaches, such as de-anonymization and sensitive information leakage. Additionally, we explore mitigation strategies from different perspectives and advocate for increased public awareness of this emerging privacy threat to online pseudonymity.", "source": "arxiv", "arxiv_id": "2505.12402v1", "pdf_url": "https://arxiv.org/pdf/2505.12402v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-18T13:05:17Z", "updated": "2025-05-18T13:05:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent", "authors": ["Humza Nusrat", "Luke Francisco", "Bing Luo", "Hassan Bagher-Ebadian", "Joshua Kim", "Karen Chin-Snyder", "Salim Siddiqui", "Mira Shah", "Eric Mellon", "Mohammad Ghassemi", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "year": 2025, "url": "http://arxiv.org/abs/2512.20586v1", "abstract": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.", "source": "arxiv", "arxiv_id": "2512.20586v1", "pdf_url": "https://arxiv.org/pdf/2512.20586v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T18:32:17Z", "updated": "2025-12-23T18:32:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools", "authors": ["Ha Min Son", "Huan Ren", "Xin Liu", "Zhe Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2510.08640v2", "abstract": "Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixer's success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the model's high-level reasoning and effective low-level execution.", "source": "arxiv", "arxiv_id": "2510.08640v2", "pdf_url": "https://arxiv.org/pdf/2510.08640v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-09T01:33:25Z", "updated": "2025-11-19T18:46:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "year": 2025, "url": "http://arxiv.org/abs/2510.01398v1", "abstract": "Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.", "source": "arxiv", "arxiv_id": "2510.01398v1", "pdf_url": "https://arxiv.org/pdf/2510.01398v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-01T19:28:35Z", "updated": "2025-10-01T19:28:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs", "authors": ["Vincent Li", "Tim Knappe", "Yule Fu", "Kevin Han", "Kevin Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2503.11657v2", "abstract": "Large language models have demonstrated remarkable capabilities in natural language processing tasks requiring multi-step logical reasoning capabilities, such as automated theorem proving. However, challenges persist within theorem proving, such as the identification of key mathematical concepts, understanding their interrelationships, and formalizing proofs correctly within natural language. We present KG-prover, a novel framework that leverages knowledge graphs mined from reputable mathematical texts to augment general-purpose LLMs to construct and formalize mathematical proofs. We also study the effects of scaling graph-based, test-time compute using KG-Prover, demonstrating significant performance improvements over baselines across multiple datasets. General-purpose LLMs improve up to 21\\% on miniF2F-test when combined with KG-Prover, with consistent improvements ranging from 2-11\\% on the ProofNet, miniF2F-test, and MUSTARD datasets without additional scaling. Furthermore, KG-Prover with o4-mini achieves over 50% miniF2F-test. This work provides a promising approach for augmenting natural language proof reasoning with knowledge graphs without the need for additional finetuning.", "source": "arxiv", "arxiv_id": "2503.11657v2", "pdf_url": "https://arxiv.org/pdf/2503.11657v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-04T07:17:34Z", "updated": "2025-07-26T09:39:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Automating Structural Engineering Workflows with Large Language Model Agents", "authors": ["Haoran Liang", "Yufa Zhou", "Mohammad Talebi Kalaleh", "Qipei Mei"], "year": 2025, "url": "http://arxiv.org/abs/2510.11004v1", "abstract": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural Engineering, effectively integrating large language model (LLM)-based agents with real-world engineering workflows. Structural engineering is a fundamental yet traditionally stagnant domain, with core workflows remaining largely unchanged for decades despite its substantial economic impact and global market size. Recent advancements in LLMs have significantly enhanced their ability to perform complex reasoning, long-horizon planning, and precise tool utilization -- capabilities well aligned with structural engineering tasks such as interpreting design codes, executing load calculations, and verifying structural capacities. We present a proof-of-concept showing that most real-world structural engineering workflows can be fully automated through a training-free LLM-based multi-agent system. MASSE enables immediate deployment in professional environments, and our comprehensive validation on real-world case studies demonstrates that it can reduce expert workload from approximately two hours to mere minutes, while enhancing both reliability and accuracy in practical engineering scenarios.", "source": "arxiv", "arxiv_id": "2510.11004v1", "pdf_url": "https://arxiv.org/pdf/2510.11004v1", "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-13T04:38:46Z", "updated": "2025-10-13T04:38:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents", "authors": ["Zhiping Zhang", "Yi Evie Zhang", "Freda Shi", "Tianshi Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.04465v1", "abstract": "Large Language Model (LLM) agents require personal information for personalization in order to better act on users' behalf in daily tasks, but this raises privacy concerns and a personalization-privacy dilemma. Agent's autonomy introduces both risks and opportunities, yet its effects remain unclear. To better understand this, we conducted a 3$\\times$3 between-subjects experiment ($N=450$) to study how agent's autonomy level and personalization influence users' privacy concerns, trust and willingness to use, as well as the underlying psychological processes. We find that personalization without considering users' privacy preferences increases privacy concerns and decreases trust and willingness to use. Autonomy moderates these effects: Intermediate autonomy flattens the impact of personalization compared to No- and Full autonomy conditions. Our results suggest that rather than aiming for perfect model alignment in output generation, balancing autonomy of agent's action and user control offers a promising path to mitigate the personalization-privacy dilemma.", "source": "arxiv", "arxiv_id": "2510.04465v1", "pdf_url": "https://arxiv.org/pdf/2510.04465v1", "categories": ["cs.HC", "cs.AI", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-06T03:38:54Z", "updated": "2025-10-06T03:38:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents", "authors": ["Haoxuan Li", "Mingyu Derek Ma", "Jen-tse Huang", "Zhaotian Weng", "Wei Wang", "Jieyu Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2504.04855v1", "abstract": "Detecting biases in structured data is a complex and time-consuming task. Existing automated techniques are limited in diversity of data types and heavily reliant on human case-by-case handling, resulting in a lack of generalizability. Currently, large language model (LLM)-based agents have made significant progress in data science, but their ability to detect data biases is still insufficiently explored. To address this gap, we introduce the first end-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for automatic bias detection in structured data based on specific user requirements. It first develops a multi-stage plan to analyze user-specified bias detection tasks and then implements it with a diverse and well-suited set of tools. It delivers detailed results that include explanations and visualizations. To address the lack of a standardized framework for evaluating the capability of LLM agents to detect biases in data, we further propose a comprehensive benchmark that includes multiple evaluation metrics and a large set of test cases. Extensive experiments demonstrate that our framework achieves exceptional overall performance in structured data bias detection, setting a new milestone for fairer data applications.", "source": "arxiv", "arxiv_id": "2504.04855v1", "pdf_url": "https://arxiv.org/pdf/2504.04855v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-07T09:12:00Z", "updated": "2025-04-07T09:12:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust", "authors": ["Yuan Sun", "Ting Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.10844v2", "abstract": "Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit `sycophancy', a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users' perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) x 2 (Friendliness: high vs. low) between-subjects experiment (N = 224). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions.", "source": "arxiv", "arxiv_id": "2502.10844v2", "pdf_url": "https://arxiv.org/pdf/2502.10844v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-15T16:18:58Z", "updated": "2025-02-19T02:40:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Benchmarking LLM Agents for Wealth-Management Workflows", "authors": ["Rory Milsom"], "year": 2025, "url": "http://arxiv.org/abs/2512.02230v1", "abstract": "Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.", "source": "arxiv", "arxiv_id": "2512.02230v1", "pdf_url": "https://arxiv.org/pdf/2512.02230v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-01T21:56:21Z", "updated": "2025-12-01T21:56:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs", "authors": ["Zhenhao Zhou", "Zhuochen Huang", "Yike He", "Chong Wang", "Jiajun Wang", "Yijian Wu", "Xin Peng", "Yiling Lou"], "year": 2025, "url": "http://arxiv.org/abs/2505.19489v1", "abstract": "The Linux kernel is a critical system, serving as the foundation for numerous systems. Bugs in the Linux kernel can cause serious consequences, affecting billions of users. Fault localization (FL), which aims at identifying the buggy code elements in software, plays an essential role in software quality assurance. While recent LLM agents have achieved promising accuracy in FL on recent benchmarks like SWE-bench, it remains unclear how well these methods perform in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors. In this paper, we introduce LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs. We conduct an empirical study to assess the performance of state-of-the-art LLM agents on the Linux kernel. Our initial results reveal that existing agents struggle with this task, achieving a best top-1 accuracy of only 41.6% at file level. To address this challenge, we propose LinuxFL$^+$, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy increase) with minimal costs. Data and code are available at https://github.com/FudanSELab/LinuxFLBench.", "source": "arxiv", "arxiv_id": "2505.19489v1", "pdf_url": "https://arxiv.org/pdf/2505.19489v1", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T04:15:48Z", "updated": "2025-05-26T04:15:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Benevolent Dictators? On LLM Agent Behavior in Dictator Games", "authors": ["Andreas Einwiller", "Kanishka Ghosh Dastidar", "Artur Romazanov", "Annette Hautli-Janisz", "Michael Granitzer", "Florian Lemmerich"], "year": 2025, "url": "http://arxiv.org/abs/2511.08721v1", "abstract": "In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.", "source": "arxiv", "arxiv_id": "2511.08721v1", "pdf_url": "https://arxiv.org/pdf/2511.08721v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-11T19:29:12Z", "updated": "2025-11-11T19:29:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.16982v1", "abstract": "Large Language Models (LLMs) show promise in biomedicine but lack true causal understanding, relying instead on correlations. This paper envisions causal LLM agents that integrate multimodal data (text, images, genomics, etc.) and perform intervention-based reasoning to infer cause-and-effect. Addressing this requires overcoming key challenges: designing safe, controllable agentic frameworks; developing rigorous benchmarks for causal evaluation; integrating heterogeneous data sources; and synergistically combining LLMs with structured knowledge (KGs) and formal causal inference tools. Such agents could unlock transformative opportunities, including accelerating drug discovery through automated hypothesis generation and simulation, enabling personalized medicine through patient-specific causal models. This research agenda aims to foster interdisciplinary efforts, bridging causal concepts and foundation models to develop reliable AI partners for biomedical progress.", "source": "arxiv", "arxiv_id": "2505.16982v1", "pdf_url": "https://arxiv.org/pdf/2505.16982v1", "categories": ["cs.AI", "physics.med-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-22T17:52:59Z", "updated": "2025-05-22T17:52:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond Jailbreaking: Auditing Contextual Privacy in LLM Agents", "authors": ["Saswat Das", "Jameson Sandler", "Ferdinando Fioretto"], "year": 2025, "url": "http://arxiv.org/abs/2506.10171v3", "abstract": "LLM agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. Moreover, these disclosures go beyond mere explicit disclosure, leaving open avenues for gradual manipulation or sidechannel information leakage. This study proposes an auditing framework for conversational privacy that quantifies an agent's susceptibility to these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework is designed to stress-test agents that enforce strict privacy directives against an iterative probing strategy. Rather than focusing solely on a single disclosure event or purely explicit leakage, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrates the auditing framework's ability to reveal privacy risks that are not deterred by existing single-turn defenses, along with an in-depth longitudinal study of the temporal dynamics of leakage, strategies adopted by adaptive adversaries, and the evolution of adversarial beliefs about sensitive targets. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.", "source": "arxiv", "arxiv_id": "2506.10171v3", "pdf_url": "https://arxiv.org/pdf/2506.10171v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-11T20:47:37Z", "updated": "2025-09-27T20:28:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms", "authors": ["Tarek Gasmi", "Ramzi Guesmi", "Ines Belhadj", "Jihene Bennaceur"], "year": 2025, "url": "http://arxiv.org/abs/2507.06323v1", "abstract": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.", "source": "arxiv", "arxiv_id": "2507.06323v1", "pdf_url": "https://arxiv.org/pdf/2507.06323v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-08T18:24:28Z", "updated": "2025-07-08T18:24:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop", "authors": ["Myung Ho Kim"], "year": 2025, "url": "http://arxiv.org/abs/2511.17673v3", "abstract": "Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.", "source": "arxiv", "arxiv_id": "2511.17673v3", "pdf_url": "https://arxiv.org/pdf/2511.17673v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-21T05:19:34Z", "updated": "2026-01-11T15:54:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software", "authors": ["Zehua Zhang", "Ati Priya Bajaj", "Divij Handa", "Siyu Liu", "Arvind S Raj", "Hongkai Chen", "Hulin Wang", "Yibo Liu", "Zion Leonahenahe Basque", "Souradip Nath", "Vishal Juneja", "Nikhil Chapre", "Yan Shoshitaishvili", "Adam DoupÃ©", "Chitta Baral", "Ruoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.25248v1", "abstract": "Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes it a good challenge for LLM Agents. Existing methods rely on manually curated rules and workflows, which cannot adapt to OSS that requires customized configuration or environment setup. Recent attempts using Large Language Models (LLMs) used selective evaluation on a subset of highly rated OSS, a practice that underestimates the realistic challenges of OSS compilation. In practice, compilation instructions are often absent, dependencies are undocumented, and successful builds may even require patching source files or modifying build scripts. We propose a more challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more diverse in quality, scale, and characteristics. Furthermore, we propose a strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with enhanced build instruction retrieval module that achieves state-of-the-art performance on BUILD-BENCH and is adaptable to heterogeneous OSS characteristics. We also provide detailed analysis regarding different compilation method design choices and their influence to the whole task, offering insights to guide future advances. We believe performance on BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as a complex software engineering tasks, and, as such, our benchmark will spur innovation with a significant impact on downstream applications in the fields of software development and software security.", "source": "arxiv", "arxiv_id": "2509.25248v1", "pdf_url": "https://arxiv.org/pdf/2509.25248v1", "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-27T03:02:46Z", "updated": "2025-09-27T03:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Building LLM Agents by Incorporating Insights from Computer Systems", "authors": ["Yapeng Mi", "Zhi Gao", "Xiaojian Ma", "Qing Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.04485v1", "abstract": "LLM-driven autonomous agents have emerged as a promising direction in recent years. However, many of these LLM agents are designed empirically or based on intuition, often lacking systematic design principles, which results in diverse agent structures with limited generality and scalability. In this paper, we advocate for building LLM agents by incorporating insights from computer systems. Inspired by the von Neumann architecture, we propose a structured framework for LLM agentic systems, emphasizing modular design and universal principles. Specifically, this paper first provides a comprehensive review of LLM agents from the computer system perspective, then identifies key challenges and future directions inspired by computer system design, and finally explores the learning mechanisms for LLM agents beyond the computer system. The insights gained from this comparative analysis offer a foundation for systematic LLM agent design and advancement.", "source": "arxiv", "arxiv_id": "2504.04485v1", "pdf_url": "https://arxiv.org/pdf/2504.04485v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-04-06T13:38:37Z", "updated": "2025-04-06T13:38:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Byzantine-Robust Decentralized Coordination of LLM Agents", "authors": ["Yongrae Jo", "Chanik Park"], "year": 2025, "url": "http://arxiv.org/abs/2507.14928v1", "abstract": "Collaboration among multiple large language model (LLM) agents is a promising approach to overcome inherent limitations of single-agent systems, such as hallucinations and single points of failure. As LLM agents are increasingly deployed on open blockchain platforms, multi-agent systems capable of tolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven coordination, which suffers from two major drawbacks. First, they are inherently vulnerable to targeted attacks against the leader. If consecutive leaders behave maliciously, the system repeatedly fails to achieve consensus, forcing new consensus rounds, which is particularly costly given the high latency of LLM invocations. Second, an underperforming proposal from the leader can be accepted as the final answer even when higher-quality alternatives are available, as existing methods finalize the leader's proposal once it receives a quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized consensus approach for multi-agent LLM systems, where worker agents generate answers concurrently and evaluator agents independently score and rank these answers to select the best available one. This decentralized architecture enables faster consensus despite the presence of Byzantine agents and consistently selects higher-quality answers through Byzantine-robust aggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates Byzantine agents and significantly improves the quality of selected answers.", "source": "arxiv", "arxiv_id": "2507.14928v1", "pdf_url": "https://arxiv.org/pdf/2507.14928v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-07-20T11:55:26Z", "updated": "2025-07-20T11:55:26Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions", "authors": ["Lingyue Fu", "Xin Ding", "Yaoming Zhu", "Shao Zhang", "Lin Qiu", "Weiwen Liu", "Weinan Zhang", "Xuezhi Cao", "Xunliang Cai", "Jiaxin Ding", "Yong Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.26852v1", "abstract": "Large Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.", "source": "arxiv", "arxiv_id": "2510.26852v1", "pdf_url": "https://arxiv.org/pdf/2510.26852v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T15:22:53Z", "updated": "2025-10-30T15:22:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CLAPP: The CLASS LLM Agent for Pair Programming", "authors": ["Santiago Casas", "Christian Fidler", "Boris Bolliet", "Francisco Villaescusa-Navarro", "Julien Lesgourgues"], "year": 2025, "url": "http://arxiv.org/abs/2508.05728v1", "abstract": "We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at https://classclapp.streamlit.app", "source": "arxiv", "arxiv_id": "2508.05728v1", "pdf_url": "https://arxiv.org/pdf/2508.05728v1", "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI", "cs.MA"], "primary_category": "astro-ph.IM", "doi": "", "venue": "", "published": "2025-08-07T17:35:06Z", "updated": "2025-08-07T17:35:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "year": 2025, "url": "http://arxiv.org/abs/2504.04310v3", "abstract": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems -- a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agentic frameworks against established human-designed algorithms, revealing the strengths and limitations of existing LLM agents and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.", "source": "arxiv", "arxiv_id": "2504.04310v3", "pdf_url": "https://arxiv.org/pdf/2504.04310v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-06T00:47:43Z", "updated": "2025-08-22T08:00:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents", "authors": ["Manish Bhatt", "Ronald F. Del Rosario", "Vineeth Sai Narajala", "Idan Habler"], "year": 2025, "url": "http://arxiv.org/abs/2506.01900v1", "abstract": "The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.", "source": "arxiv", "arxiv_id": "2506.01900v1", "pdf_url": "https://arxiv.org/pdf/2506.01900v1", "categories": ["cs.AI", "cs.CE", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T17:22:47Z", "updated": "2025-06-02T17:22:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution", "authors": ["Minghao Shao", "Haoran Xi", "Nanda Rani", "Meet Udeshi", "Venkata Sai Charan Putrevu", "Kimberly Milner", "Brendan Dolan-Gavitt", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "year": 2025, "url": "http://arxiv.org/abs/2505.17107v1", "abstract": "Large Language Model (LLM) agents can automate cybersecurity tasks and can adapt to the evolving cybersecurity landscape without re-engineering. While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning. Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations. We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies. Comprehensive evaluations with different configurations show CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation compared to previous approaches. Our extensible architecture establishes new methodologies for embedding new security knowledge into LLM-driven cybersecurity agentic systems. With a knowledge database of CTF writeups, CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution. We make our framework open source to public https://github.com/NYU-LLM-CTF/nyuctf_agents_craken.", "source": "arxiv", "arxiv_id": "2505.17107v1", "pdf_url": "https://arxiv.org/pdf/2505.17107v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-21T11:01:11Z", "updated": "2025-05-21T11:01:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.18878v1", "abstract": "While AI agents hold transformative potential in business, effective performance benchmarking is hindered by the scarcity of public, realistic business data on widely used platforms. Existing benchmarks often lack fidelity in their environments, data, and agent-user interactions, with limited coverage of diverse business scenarios and industries. To address these gaps, we introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena with nineteen expert-validated tasks across sales, service, and 'configure, price, and quote' processes, for both Business-to-Business and Business-to-Customer scenarios. It distinctively incorporates multi-turn interactions guided by diverse personas and robust confidentiality awareness assessments. Experiments reveal leading LLM agents achieve only around 58% single-turn success on CRMArena-Pro, with performance dropping significantly to approximately 35% in multi-turn settings. While Workflow Execution proves more tractable for top agents (over 83% single-turn success), other evaluated business skills present greater challenges. Furthermore, agents exhibit near-zero inherent confidentiality awareness; though targeted prompting can improve this, it often compromises task performance. These findings highlight a substantial gap between current LLM capabilities and enterprise demands, underscoring the need for advancements in multi-turn reasoning, confidentiality adherence, and versatile skill acquisition.", "source": "arxiv", "arxiv_id": "2505.18878v1", "pdf_url": "https://arxiv.org/pdf/2505.18878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-24T21:33:22Z", "updated": "2025-05-24T21:33:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories", "authors": ["Yijia Xiao", "Runhui Wang", "Luyang Kong", "Davor Golac", "Wei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.06111v2", "abstract": "The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.", "source": "arxiv", "arxiv_id": "2502.06111v2", "pdf_url": "https://arxiv.org/pdf/2502.06111v2", "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-02-10T02:46:29Z", "updated": "2025-02-11T20:25:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can AI automatically analyze public opinion? A LLM agents-based agentic pipeline for timely public opinion analysis", "authors": ["Jing Liu", "Xinxing Ren", "Yanmeng Xu", "Zekun Guo"], "year": 2025, "url": "http://arxiv.org/abs/2505.11401v1", "abstract": "This study proposes and implements the first LLM agents based agentic pipeline for multi task public opinion analysis. Unlike traditional methods, it offers an end-to-end, fully automated analytical workflow without requiring domain specific training data, manual annotation, or local deployment. The pipeline integrates advanced LLM capabilities into a low-cost, user-friendly framework suitable for resource constrained environments. It enables timely, integrated public opinion analysis through a single natural language query, making it accessible to non-expert users. To validate its effectiveness, the pipeline was applied to a real world case study of the 2025 U.S. China tariff dispute, where it analyzed 1,572 Weibo posts and generated a structured, multi part analytical report. The results demonstrate some relationships between public opinion and governmental decision-making. These contributions represent a novel advancement in applying generative AI to public governance, bridging the gap between technical sophistication and practical usability in public opinion monitoring.", "source": "arxiv", "arxiv_id": "2505.11401v1", "pdf_url": "https://arxiv.org/pdf/2505.11401v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-05-16T16:09:28Z", "updated": "2025-05-16T16:09:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can LLM Agents Really Debate? A Controlled Study of Multi-Agent Debate in Logical Reasoning", "authors": ["Haolun Wu", "Zhenkun Li", "Lingyao Li"], "year": 2025, "url": "http://arxiv.org/abs/2511.07784v1", "abstract": "Multi-agent debate (MAD) has recently emerged as a promising framework for improving the reasoning performance of large language models (LLMs). Yet, whether LLM agents can genuinely engage in deliberative reasoning, beyond simple ensembling or majority voting, remains unclear. We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. We systematically set up six structural and cognitive factors, including agent team size, composition, confidence visibility, debate order, debate depth, and task difficulty, to disentangle their respective effects on collective reasoning. Our results show that intrinsic reasoning strength and group diversity are the dominant drivers of debate success, while structural parameters such as order or confidence visibility offer limited gains. Beyond outcomes, process-level analyses identify key behavioral patterns: majority pressure suppresses independent correction, effective teams overturn incorrect consensus, and rational, validity-aligned reasoning most strongly predicts improvement. These findings provide valuable insights into how and why LLM debates succeed or fail, offering guidance for designing interpretable and truth-seeking multi-agent reasoning systems.", "source": "arxiv", "arxiv_id": "2511.07784v1", "pdf_url": "https://arxiv.org/pdf/2511.07784v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-11T03:05:47Z", "updated": "2025-11-11T03:05:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can LLM Agents Simulate Multi-Turn Human Behavior? Evidence from Real Online Customer Behavior Data", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bingsheng Yao", "Sisong Bei", "Jiri Gesi", "Yaochen Xie", "Zheshen", "Wang", "Qi He", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.20749v7", "abstract": "Recent research shows that LLM Agents can generate ``believable'' human behaviors via prompt-only methods, and such agents have been increasingly adopted in downstream applications. However, existing evaluation of these agents only focuses on qualitative believability (whether human raters think they are accurate), leaving open questions of whether LLM agents can accurately generate step-by-step actions mimicking a particular human's behavior in a multi-turn interaction task. In this work, we take shopping as a case study and present the first large-scale quantitative evaluation of state-of-the-art LLMs' ability to accurately simulate human behavior. Using real-world data from 31,865 online shopping sessions containing 230,965 user actions, our evaluation reveals that prompt-based LLMs (DeepSeek-R1, Llama, Claude) achieve only 11.86% accuracy in generating human actions, highlighting a substantial gap in actual behavioral accuracy. Through experiments, we also showcase that strategies as simple as fine-tuning LLMs on real human click-through data augmented with synthesized reasoning traces can greatly enhance models' performance. The fine-tuned Qwen2.5-7B achieves 17.26% action generation accuracy and 33.86% F1 score on final purchase prediction, representing substantial improvements of 5.4% and 13.85% over prompt-only baselines. This work establishes the first rigorous benchmark for human behavior simulation and provides actionable insights for developing more accurate LLM agents for future downstream applications.", "source": "arxiv", "arxiv_id": "2503.20749v7", "pdf_url": "https://arxiv.org/pdf/2503.20749v7", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-26T17:33:27Z", "updated": "2025-10-08T20:51:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination", "authors": ["JoÃ£o Vitor de Carvalho Silva", "Douglas G. Macharet"], "year": 2025, "url": "http://arxiv.org/abs/2508.14635v1", "abstract": "The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "source": "arxiv", "arxiv_id": "2508.14635v1", "pdf_url": "https://arxiv.org/pdf/2508.14635v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-08-20T11:44:10Z", "updated": "2025-08-20T11:44:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments", "authors": ["Harsh Vishwakarma", "Ankush Agarwal", "Ojas Patil", "Chaitanya Devaguptapu", "Mahesh Chandran"], "year": 2025, "url": "http://arxiv.org/abs/2510.27287v1", "abstract": "Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.", "source": "arxiv", "arxiv_id": "2510.27287v1", "pdf_url": "https://arxiv.org/pdf/2510.27287v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-31T08:55:13Z", "updated": "2025-10-31T08:55:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can Large Language Model Agents Balance Energy Systems?", "authors": ["Xinxing Ren", "Chun Sing Lai", "Gareth Taylor", "Zekun Guo"], "year": 2025, "url": "http://arxiv.org/abs/2502.10557v2", "abstract": "This paper presents a hybrid approach that integrates Large Language Models (LLMs) with a multi-scenario Stochastic Unit Commitment (SUC) framework to enhance both efficiency and reliability under high wind generation uncertainties. In a 10-trial study on the test energy system, the traditional SUC approach incurs an average total cost of 187.68 million dollars, whereas the LLM-assisted SUC (LLM-SUC) achieves a mean cost of 185.58 million dollars (range: 182.61 to 188.65 million dollars), corresponding to a cost reduction of 1.1 to 2.7 percent. Furthermore, LLM-SUC reduces load curtailment by 26.3 percent (2.24 plus/minus 0.31 GWh versus 3.04 GWh for SUC), while both methods maintain zero wind curtailment. Detailed temporal analysis shows that LLM-SUC achieves lower costs in the majority of time intervals and consistently outperforms SUC in 90 percent of cases, with solutions clustering in a favorable cost-reliability region (Coefficient of Variation = 0.93 percent for total cost and 13.8 percent for load curtailment). By leveraging an LLM agent to guide generator commitment decisions and dynamically adjust to stochastic conditions, the proposed framework improves demand fulfillment and operational resilience.", "source": "arxiv", "arxiv_id": "2502.10557v2", "pdf_url": "https://arxiv.org/pdf/2502.10557v2", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-02-14T21:11:53Z", "updated": "2025-03-30T13:35:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Capturing Semantic Flow of ML-based Systems", "authors": ["Shin Yoo", "Robert Feldt", "Somin Kim", "Naryeong Kim"], "year": 2025, "url": "http://arxiv.org/abs/2503.10310v1", "abstract": "ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems.", "source": "arxiv", "arxiv_id": "2503.10310v1", "pdf_url": "https://arxiv.org/pdf/2503.10310v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-13T12:39:04Z", "updated": "2025-03-13T12:39:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "year": 2025, "url": "http://arxiv.org/abs/2509.25593v1", "abstract": "A large language model (LLM) can map a feedback causal fuzzy cognitive map (FCM) into text and then reconstruct the FCM from the text. This explainable AI system approximates an identity map from the FCM to itself and resembles the operation of an autoencoder (AE). Both the encoder and the decoder explain their decisions in contrast to black-box AEs. Humans can read and interpret the encoded text in contrast to the hidden variables and synaptic webs in AEs. The LLM agent approximates the identity map through a sequence of system instructions that does not compare the output to the input. The reconstruction is lossy because it removes weak causal edges or rules while it preserves strong causal edges. The encoder preserves the strong causal edges even when it trades off some details about the FCM to make the text sound more natural.", "source": "arxiv", "arxiv_id": "2509.25593v1", "pdf_url": "https://arxiv.org/pdf/2509.25593v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T23:33:53Z", "updated": "2025-09-29T23:33:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Characterizing LLM-driven Social Network: The Chirper.ai Case", "authors": ["Yiming Zhu", "Yupeng He", "Ehsan-Ul Haq", "Gareth Tyson", "Pan Hui"], "year": 2025, "url": "http://arxiv.org/abs/2504.10286v1", "abstract": "Large language models (LLMs) demonstrate the ability to simulate human decision-making processes, enabling their use as agents in modeling sophisticated social networks, both offline and online. Recent research has explored collective behavioral patterns and structural characteristics of LLM agents within simulated networks. However, empirical comparisons between LLM-driven and human-driven online social networks remain scarce, limiting our understanding of how LLM agents differ from human users. This paper presents a large-scale analysis of Chirper.ai, an X/Twitter-like social network entirely populated by LLM agents, comprising over 65,000 agents and 7.7 million AI-generated posts. For comparison, we collect a parallel dataset from Mastodon, a human-driven decentralized social network, with over 117,000 users and 16 million posts. We examine key differences between LLM agents and humans in posting behaviors, abusive content, and social network structures. Our findings provide critical insights into the evolving landscape of online social network analysis in the AI era, offering a comprehensive profile of LLM agents in social simulations.", "source": "arxiv", "arxiv_id": "2504.10286v1", "pdf_url": "https://arxiv.org/pdf/2504.10286v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-04-14T14:53:31Z", "updated": "2025-04-14T14:53:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "year": 2025, "url": "http://arxiv.org/abs/2507.23096v1", "abstract": "Large language models (LLMs) are rapidly increasing in capability, but they still struggle with highly specialized programming tasks such as scientific visualization. We present an LLM assistant, ChatVis, that aids the LLM to generate Python code for ParaView scientific visualization tasks, without the need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database of documentation and code examples, and error checking with iterative prompt feedback to correct errors until a visualization is produced. An integral part of our approach is a benchmark suite of canonical visualization tasks, ParaView regression tests, and scientific use cases that includes comprehensive evaluation metrics. We evaluate our visualization assistant by comparing results with a variety of top-performing unassisted LLMs. We find that all the metrics are significantly improved with ChatVis.", "source": "arxiv", "arxiv_id": "2507.23096v1", "pdf_url": "https://arxiv.org/pdf/2507.23096v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-30T20:54:18Z", "updated": "2025-07-30T20:54:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent", "authors": ["Liang-bo Ning", "Shijie Wang", "Wenqi Fan", "Qing Li", "Xin Xu", "Hao Chen", "Feiran Huang"], "year": 2025, "url": "http://arxiv.org/abs/2504.13192v2", "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.", "source": "arxiv", "arxiv_id": "2504.13192v2", "pdf_url": "https://arxiv.org/pdf/2504.13192v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "10.1145/3637528.3671837", "venue": "", "published": "2025-04-13T05:31:37Z", "updated": "2025-04-24T02:16:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "year": 2025, "url": "http://arxiv.org/abs/2510.16492v2", "abstract": "As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.", "source": "arxiv", "arxiv_id": "2510.16492v2", "pdf_url": "https://arxiv.org/pdf/2510.16492v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-18T13:22:19Z", "updated": "2025-10-25T10:26:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space", "authors": ["Yong Zhao", "Kai Xu", "Zhengqiu Zhu", "Yue Hu", "Zhiheng Zheng", "Yingfeng Chen", "Yatai Ji", "Chen Gao", "Yong Li", "Jincai Huang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12532v3", "abstract": "Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.", "source": "arxiv", "arxiv_id": "2502.12532v3", "pdf_url": "https://arxiv.org/pdf/2502.12532v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T04:36:15Z", "updated": "2025-05-22T00:44:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance", "authors": ["Valeriu Dimidov", "Faisal Hawlader", "Sasan Jafarnejad", "RaphaÃ«l Frank"], "year": 2025, "url": "http://arxiv.org/abs/2511.05311v1", "abstract": "Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.", "source": "arxiv", "arxiv_id": "2511.05311v1", "pdf_url": "https://arxiv.org/pdf/2511.05311v1", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-07T15:12:49Z", "updated": "2025-11-07T15:12:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control", "authors": ["Zirui Yuan", "Siqi Lai", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2503.11739v1", "abstract": "Traffic Signal Control (TSC) plays a critical role in urban traffic management by optimizing traffic flow and mitigating congestion. While Large Language Models (LLMs) have recently emerged as promising tools for TSC due to their exceptional problem-solving and generalization capabilities, existing approaches fail to address the essential need for inter-agent coordination, limiting their effectiveness in achieving network-wide optimization. To bridge this gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC. Specifically, we first construct a structured spatiotemporal graph to capture real-time traffic dynamics and spatial relationships among neighboring intersections, enabling the LLM to reason about complex traffic interactions. Moreover, we introduce a complexity-aware reasoning mechanism that dynamically adapts reasoning depth based on real-time traffic conditions, ensuring optimal computational efficiency without sacrificing decision quality. Besides, we propose a fine-tuning strategy that leverages iterative simulation-driven data collection and environmental feedback to build a lightweight LLM tailored for cooperative TSC. Extensive experiments on both synthetic and real-world datasets demonstrate that CoLLMLight outperforms state-of-the-art methods in diverse traffic scenarios, showcasing its effectiveness, scalability, and robustness.", "source": "arxiv", "arxiv_id": "2503.11739v1", "pdf_url": "https://arxiv.org/pdf/2503.11739v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-14T15:40:39Z", "updated": "2025-03-14T15:40:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning", "authors": ["Wei Yang", "Jinwei Xiao", "Hongming Zhang", "Qingyang Zhang", "Yanna Wang", "Bo Xu"], "year": 2025, "url": "http://arxiv.org/abs/2508.15305v1", "abstract": "Recent advancements in Large Language Models (LLMs) have driven growing interest in LLM-based agents for complex planning tasks. To avoid costly agent training, many studies adopted memory mechanism that enhances LLM with offline experiences or online trajectory analysis. However, existing works focus on single-granularity memory derived from dynamic environmental interactions, which are inherently constrained by the quality of the collected experiences. This limitation, in turn, constrain the diversity of knowledge and the flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\\Ours{}), a novel framework that grounds coarse-to-fine memories with LLM, thereby fully leverage them for flexible adaptation to diverse scenarios. \\Ours{} grounds environmental information into coarse-grained focus points to guide experience collection in training tasks, followed by grounding of actionable hybrid-grained tips from each experience. At inference, \\Ours{} retrieves task-relevant experiences and tips to support planning. When facing environmental anomalies, the LLM grounds the current situation into fine-grained key information, enabling flexible self-QA reflection and plan correction.", "source": "arxiv", "arxiv_id": "2508.15305v1", "pdf_url": "https://arxiv.org/pdf/2508.15305v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-21T06:50:23Z", "updated": "2025-08-21T06:50:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "authors": ["Anjiang Wei", "Tarun Suresh", "Jiannan Cao", "Naveen Kannan", "Yuheng Wu", "Kai Yan", "Thiago S. F. X. Teixeira", "Ke Wang", "Alex Aiken"], "year": 2025, "url": "http://arxiv.org/abs/2503.23145v2", "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/CodeARC", "source": "arxiv", "arxiv_id": "2503.23145v2", "pdf_url": "https://arxiv.org/pdf/2503.23145v2", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-03-29T16:50:39Z", "updated": "2025-08-08T07:13:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks", "authors": ["Ang Li", "Yin Zhou", "Vethavikashini Chithrra Raghuram", "Tom Goldstein", "Micah Goldblum"], "year": 2025, "url": "http://arxiv.org/abs/2502.08586v1", "abstract": "A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning.", "source": "arxiv", "arxiv_id": "2502.08586v1", "pdf_url": "https://arxiv.org/pdf/2502.08586v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-12T17:19:36Z", "updated": "2025-02-12T17:19:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Communicating Activations Between Language Model Agents", "authors": ["Vignav Ramesh", "Kenneth Li"], "year": 2025, "url": "http://arxiv.org/abs/2501.14082v2", "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\\textit{B}$'s computation at an intermediate layer, combine its current activation with another LM $\\textit{A}$'s intermediate activation via some function $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of $\\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\\%$ improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative \"language\" for communication between LMs.", "source": "arxiv", "arxiv_id": "2501.14082v2", "pdf_url": "https://arxiv.org/pdf/2501.14082v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-01-23T20:41:07Z", "updated": "2025-05-07T20:03:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry", "authors": ["Run Peng", "Ziqiao Ma", "Amy Pang", "Sikai Li", "Zhang Xi-Jia", "Yingzhuo Yu", "Cristian-Paul Bara", "Joyce Chai"], "year": 2025, "url": "http://arxiv.org/abs/2510.25595v1", "abstract": "While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles", "source": "arxiv", "arxiv_id": "2510.25595v1", "pdf_url": "https://arxiv.org/pdf/2510.25595v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-29T15:03:53Z", "updated": "2025-10-29T15:03:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent", "authors": ["Yixin Gao", "Xin Li", "Xiaohan Pan", "Runsen Feng", "Bingchen Li", "Yunpeng Qi", "Yiting Lu", "Zhengxue Cheng", "Zhibo Chen", "JÃ¶rn Ostermann"], "year": 2025, "url": "http://arxiv.org/abs/2508.15243v1", "abstract": "We present Comp-X, the first intelligently interactive image compression paradigm empowered by the impressive reasoning capability of large language model (LLM) agent. Notably, commonly used image codecs usually suffer from limited coding modes and rely on manual mode selection by engineers, making them unfriendly for unprofessional users. To overcome this, we advance the evolution of image coding paradigm by introducing three key innovations: (i) multi-functional coding framework, which unifies different coding modes of various objective/requirements, including human-machine perception, variable coding, and spatial bit allocation, into one framework. (ii) interactive coding agent, where we propose an augmented in-context learning method with coding expert feedback to teach the LLM agent how to understand the coding request, mode selection, and the use of the coding tools. (iii) IIC-bench, the first dedicated benchmark comprising diverse user requests and the corresponding annotations from coding experts, which is systematically designed for intelligently interactive image compression evaluation. Extensive experimental results demonstrate that our proposed Comp-X can understand the coding requests efficiently and achieve impressive textual interaction capability. Meanwhile, it can maintain comparable compression performance even with a single coding framework, providing a promising avenue for artificial general intelligence (AGI) in image compression.", "source": "arxiv", "arxiv_id": "2508.15243v1", "pdf_url": "https://arxiv.org/pdf/2508.15243v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-08-21T05:09:30Z", "updated": "2025-08-21T05:09:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation", "authors": ["Amin Qasmi", "Usman Naseem", "Mehwish Nasim"], "year": 2025, "url": "http://arxiv.org/abs/2502.11649v3", "abstract": "We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence", "source": "arxiv", "arxiv_id": "2502.11649v3", "pdf_url": "https://arxiv.org/pdf/2502.11649v3", "categories": ["cs.AI", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T10:41:55Z", "updated": "2025-08-31T02:04:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework", "authors": ["Saman Marandi", "Yu-Shu Hu", "Mohammad Modarres"], "year": 2025, "url": "http://arxiv.org/abs/2505.21291v1", "abstract": "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.", "source": "arxiv", "arxiv_id": "2505.21291v1", "pdf_url": "https://arxiv.org/pdf/2505.21291v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "10.3390/app15179428", "venue": "", "published": "2025-05-27T14:54:49Z", "updated": "2025-05-27T14:54:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Conformal Constrained Policy Optimization for Cost-Effective LLM Agents", "authors": ["Wenwen Si", "Sooyong Jang", "Insup Lee", "Osbert Bastani"], "year": 2025, "url": "http://arxiv.org/abs/2511.11828v1", "abstract": "While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.", "source": "arxiv", "arxiv_id": "2511.11828v1", "pdf_url": "https://arxiv.org/pdf/2511.11828v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-14T19:39:28Z", "updated": "2025-11-14T19:39:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Connecting Large Language Model Agent to High Performance Computing Resource", "authors": ["Heng Ma", "Alexander Brace", "Carlo Siebenschuh", "Greg Pauloski", "Ian Foster", "Arvind Ramanathan"], "year": 2025, "url": "http://arxiv.org/abs/2502.12280v1", "abstract": "The Large Language Model agent workflow enables the LLM to invoke tool functions to increase the performance on specific scientific domain questions. To tackle large scale of scientific research, it requires access to computing resource and parallel computing setup. In this work, we implemented Parsl to the LangChain/LangGraph tool call setup, to bridge the gap between the LLM agent to the computing resource. Two tool call implementations were set up and tested on both local workstation and HPC environment on Polaris/ALCF. The first implementation with Parsl-enabled LangChain tool node queues the tool functions concurrently to the Parsl workers for parallel execution. The second configuration is implemented by converting the tool functions into Parsl ensemble functions, and is more suitable for large task on super computer environment. The LLM agent workflow was prompted to run molecular dynamics simulations, with different protein structure and simulation conditions. These results showed the LLM agent tools were managed and executed concurrently by Parsl on the available computing resource.", "source": "arxiv", "arxiv_id": "2502.12280v1", "pdf_url": "https://arxiv.org/pdf/2502.12280v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2025-02-17T19:32:30Z", "updated": "2025-02-17T19:32:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Constructing coherent spatial memory in LLM agents through graph rectification", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "year": 2025, "url": "http://arxiv.org/abs/2510.04195v1", "abstract": "Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.", "source": "arxiv", "arxiv_id": "2510.04195v1", "pdf_url": "https://arxiv.org/pdf/2510.04195v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-05T13:27:00Z", "updated": "2025-10-05T13:27:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2505.14668v2", "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts surrounding humans to enhance the proactivity of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and personas from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. The code and dataset are publicly available at https://github.com/openaiotlab/ContextAgent.", "source": "arxiv", "arxiv_id": "2505.14668v2", "pdf_url": "https://arxiv.org/pdf/2505.14668v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-20T17:55:25Z", "updated": "2025-10-27T07:17:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live", "authors": ["Hanchen Li", "Qiuyang Mang", "Runyuan He", "Qizheng Zhang", "Huanzhi Mao", "Xiaokun Chen", "Hangrui Zhou", "Alvin Cheung", "Joseph Gonzalez", "Ion Stoica"], "year": 2025, "url": "http://arxiv.org/abs/2511.02230v2", "abstract": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum", "source": "arxiv", "arxiv_id": "2511.02230v2", "pdf_url": "https://arxiv.org/pdf/2511.02230v2", "categories": ["cs.OS", "cs.AI", "cs.NI"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2025-11-04T03:43:05Z", "updated": "2025-12-20T01:17:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents", "authors": ["Jiangrong Wu", "Yuhong Nan", "Jianliang Wu", "Zitong Yao", "Zibin Zheng"], "year": 2025, "url": "http://arxiv.org/abs/2507.02699v1", "abstract": "The increasing capabilities of LLMs have led to the rapid proliferation of LLM agent apps, where developers enhance LLMs with access to external resources to support complex task execution. Among these, LLM email agent apps represent one of the widely used categories, as email remains a critical communication medium for users. LLM email agents are capable of managing and responding to email using LLM-driven reasoning and autonomously executing user instructions via external email APIs (e.g., send email). However, despite their growing deployment and utility, the security mechanism of LLM email agent apps remains underexplored. Currently, there is no comprehensive study into the potential security risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which overrides the original prompts of the email agent via external email resources, allowing attackers to gain control of the email agent remotely and further perform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an empirical study spanning 14 representative LLM agent frameworks, 63 agent apps, 12 LLMs, and 20 email services, which led to the generation of 1,404 real-world email agent instances for evaluation. Experimental results indicate that all 1,404 instances were successfully hijacked; on average, only 2.03 attack attempts are required to control an email agent instance. Even worse, for some LLMs, the average number of attempts needed to achieve full agent control drops to as few as 1.23.", "source": "arxiv", "arxiv_id": "2507.02699v1", "pdf_url": "https://arxiv.org/pdf/2507.02699v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-03T15:09:40Z", "updated": "2025-07-03T15:09:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate", "authors": ["Yiliu Sun", "Zicheng Zhao", "Sheng Wan", "Chen Gong"], "year": 2025, "url": "http://arxiv.org/abs/2507.03928v1", "abstract": "Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called \"CortexDebate\". Inspired by the human brain's tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.", "source": "arxiv", "arxiv_id": "2507.03928v1", "pdf_url": "https://arxiv.org/pdf/2507.03928v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-05T07:23:15Z", "updated": "2025-07-05T07:23:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents", "authors": ["Jiayu Liu", "Cheng Qian", "Zhaochen Su", "Qing Zong", "Shijue Huang", "Bingxiang He", "Yi R. Fung"], "year": 2025, "url": "http://arxiv.org/abs/2511.02734v1", "abstract": "Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.", "source": "arxiv", "arxiv_id": "2511.02734v1", "pdf_url": "https://arxiv.org/pdf/2511.02734v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T16:58:29Z", "updated": "2025-11-04T16:58:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency", "authors": ["Jiacheng Guo", "Suozhi Huang", "Zixin Yao", "Yifan Zhang", "Yifu Lu", "Jiashuo Liu", "Zihao Li", "Nicholas Deng", "Qixin Xiao", "Jia Tian", "Kanghong Zhan", "Tianyi Li", "Xiaochen Liu", "Jason Ge", "Chaoyang He", "Kaixuan Huang", "Lin Yang", "Wenhao Huang", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.00417v4", "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.\n  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.", "source": "arxiv", "arxiv_id": "2512.00417v4", "pdf_url": "https://arxiv.org/pdf/2512.00417v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-29T09:52:34Z", "updated": "2025-12-10T17:52:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.07230v2", "abstract": "Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users' action distribution, indicating higher fidelity in personalized behavior simulation.", "source": "arxiv", "arxiv_id": "2510.07230v2", "pdf_url": "https://arxiv.org/pdf/2510.07230v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-08T17:00:25Z", "updated": "2025-10-18T04:00:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics", "authors": ["Stefano Fumero", "Kai Huang", "Matteo Boffa", "Danilo Giordano", "Marco Mellia", "Zied Ben Houidi", "Dario Rossi"], "year": 2025, "url": "http://arxiv.org/abs/2508.20643v1", "abstract": "Large Language Model (LLM) agents are powerful tools for automating complex tasks. In cybersecurity, researchers have primarily explored their use in red-team operations such as vulnerability discovery and penetration tests. Defensive uses for incident response and forensics have received comparatively less attention and remain at an early stage. This work presents a systematic study of LLM-agent design for the forensic investigation of realistic web application attacks. We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. We evaluate the consequences of core design decisions - spanning tool integration and agent architecture - and provide interpretable guidance for practitioners. We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. In a separate set of 10 incidents from 2025, CyberSleuth correctly identifies the exact CVE in 80% of cases. At last, we conduct a human study with 22 experts, which rated the reports of CyberSleuth as complete, useful, and coherent. They also expressed a slight preference for DeepSeek R1, a good news for open source LLM. To foster progress in defensive LLM research, we release both our benchmark and the CyberSleuth platform as a foundation for fair, reproducible evaluation of forensic agents.", "source": "arxiv", "arxiv_id": "2508.20643v1", "pdf_url": "https://arxiv.org/pdf/2508.20643v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-28T10:45:31Z", "updated": "2025-08-28T10:45:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates", "authors": ["Yun-Shiuan Chuang", "Ruixuan Tu", "Chengtao Dai", "Smit Vasani", "Binwei Yao", "Michael Henry Tessler", "Sijia Yang", "Dhavan Shah", "Robert Hawkins", "Junjie Hu", "Timothy T. Rogers"], "year": 2025, "url": "http://arxiv.org/abs/2510.25110v1", "abstract": "Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics.", "source": "arxiv", "arxiv_id": "2510.25110v1", "pdf_url": "https://arxiv.org/pdf/2510.25110v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-29T02:21:10Z", "updated": "2025-10-29T02:21:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents", "authors": ["Sirui Chen", "Mengshi Zhao", "Lei Xu", "Yuying Zhao", "Beier Zhu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "year": 2025, "url": "http://arxiv.org/abs/2511.15392v1", "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.", "source": "arxiv", "arxiv_id": "2511.15392v1", "pdf_url": "https://arxiv.org/pdf/2511.15392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-19T12:38:43Z", "updated": "2025-11-19T12:38:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "year": 2025, "url": "http://arxiv.org/abs/2507.23554v1", "abstract": "Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks. However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance. In this paper, we address this challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance. Importantly, DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.", "source": "arxiv", "arxiv_id": "2507.23554v1", "pdf_url": "https://arxiv.org/pdf/2507.23554v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-31T13:42:14Z", "updated": "2025-07-31T13:42:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs", "authors": ["Yuanhao Li", "Mingshan Liu", "Hongbo Wang", "Yiding Zhang", "Yifei Ma", "Wei Tan"], "year": 2025, "url": "http://arxiv.org/abs/2511.20468v1", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "source": "arxiv", "arxiv_id": "2511.20468v1", "pdf_url": "https://arxiv.org/pdf/2511.20468v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-25T16:33:42Z", "updated": "2025-11-25T16:33:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DRF: LLM-AGENT Dynamic Reputation Filtering Framework", "authors": ["Yuwei Lou", "Hao Hu", "Shaocong Ma", "Zongfei Zhang", "Liang Wang", "Jidong Ge", "Xianping Tao"], "year": 2025, "url": "http://arxiv.org/abs/2509.05764v1", "abstract": "With the evolution of generative AI, multi - agent systems leveraging large - language models(LLMs) have emerged as a powerful tool for complex tasks. However, these systems face challenges in quantifying agent performance and lack mechanisms to assess agent credibility. To address these issues, we introduce DRF, a dynamic reputation filtering framework. DRF constructs an interactive rating network to quantify agent performance, designs a reputation scoring mechanism to measure agent honesty and capability, and integrates an Upper Confidence Bound - based strategy to enhance agent selection efficiency. Experiments show that DRF significantly improves task completion quality and collaboration efficiency in logical reasoning and code - generation tasks, offering a new approach for multi - agent systems to handle large - scale tasks.", "source": "arxiv", "arxiv_id": "2509.05764v1", "pdf_url": "https://arxiv.org/pdf/2509.05764v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-06T16:29:42Z", "updated": "2025-09-06T16:29:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DVM: Towards Controllable LLM Agents in Social Deduction Games", "authors": ["Zheng Zhang", "Yihuai Lan", "Yangsen Chen", "Lei Wang", "Xiang Wang", "Hao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2501.06695v1", "abstract": "Large Language Models (LLMs) have advanced the capability of game agents in social deduction games (SDGs). These games rely heavily on conversation-driven interactions and require agents to infer, make decisions, and express based on such information. While this progress leads to more sophisticated and strategic non-player characters (NPCs) in SDGs, there exists a need to control the proficiency of these agents. This control not only ensures that NPCs can adapt to varying difficulty levels during gameplay, but also provides insights into the safety and fairness of LLM agents. In this paper, we present DVM, a novel framework for developing controllable LLM agents for SDGs, and demonstrate its implementation on one of the most popular SDGs, Werewolf. DVM comprises three main components: Predictor, Decider, and Discussor. By integrating reinforcement learning with a win rate-constrained decision chain reward mechanism, we enable agents to dynamically adjust their gameplay proficiency to achieve specified win rates. Experiments show that DVM not only outperforms existing methods in the Werewolf game, but also successfully modulates its performance levels to meet predefined win rate targets. These results pave the way for LLM agents' adaptive and balanced gameplay in SDGs, opening new avenues for research in controllable game agents.", "source": "arxiv", "arxiv_id": "2501.06695v1", "pdf_url": "https://arxiv.org/pdf/2501.06695v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-01-12T03:11:20Z", "updated": "2025-01-12T03:11:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight", "authors": ["Jingyu Tang", "Chaoran Chen", "Jiawen Li", "Zhiping Zhang", "Bingcan Guo", "Ibrahim Khalilov", "Simret Araya Gebreegziabher", "Bingsheng Yao", "Dakuo Wang", "Yanfang Ye", "Tianshi Li", "Ziang Xiao", "Yaxing Yao", "Toby Jia-Jun Li"], "year": 2025, "url": "http://arxiv.org/abs/2509.10723v1", "abstract": "The dark patterns, deceptive interface designs manipulating user behaviors, have been extensively studied for their effects on human decision-making and autonomy. Yet, with the rising prominence of LLM-powered GUI agents that automate tasks from high-level intents, understanding how dark patterns affect agents is increasingly important. We present a two-phase empirical study examining how agents, human participants, and human-AI teams respond to 16 types of dark patterns across diverse scenarios. Phase 1 highlights that agents often fail to recognize dark patterns, and even when aware, prioritize task completion over protective action. Phase 2 revealed divergent failure modes: humans succumb due to cognitive shortcuts and habitual compliance, while agents falter from procedural blind spots. Human oversight improved avoidance but introduced costs such as attentional tunneling and cognitive load. Our findings show neither humans nor agents are uniformly resilient, and collaboration introduces new vulnerabilities, suggesting design needs for transparency, adjustable autonomy, and oversight.", "source": "arxiv", "arxiv_id": "2509.10723v1", "pdf_url": "https://arxiv.org/pdf/2509.10723v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-12T22:26:31Z", "updated": "2025-09-12T22:26:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DataSciBench: An LLM Agent Benchmark for Data Science", "authors": ["Dan Zhang", "Sining Zhoubian", "Min Cai", "Fengzu Li", "Lekang Yang", "Wei Wang", "Tianjiao Dong", "Ziniu Hu", "Jie Tang", "Yisong Yue"], "year": 2025, "url": "http://arxiv.org/abs/2502.13897v1", "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.", "source": "arxiv", "arxiv_id": "2502.13897v1", "pdf_url": "https://arxiv.org/pdf/2502.13897v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-19T17:31:51Z", "updated": "2025-02-19T17:31:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and Robust Data Science Automation", "authors": ["Ziming You", "Yumiao Zhang", "Dexuan Xu", "Yiwei Lou", "Yandong Yan", "Wei Wang", "Huaming Zhang", "Yu Huang"], "year": 2025, "url": "http://arxiv.org/abs/2503.07044v2", "abstract": "Existing large language model (LLM) agents for automating data science show promise, but they remain constrained by narrow task scopes, limited generalization across tasks and models, and over-reliance on state-of-the-art (SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework for adaptive and robust data science automation. Inspired by how human data scientists work in computational notebooks, DatawiseAgent introduces a unified interaction representation and a multi-stage architecture based on finite-state transducers (FSTs). This design enables flexible long-horizon planning, progressive solution development, and robust recovery from execution failures. Extensive experiments across diverse data science scenarios and models show that DatawiseAgent consistently achieves SOTA performance by surpassing strong baselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness and adaptability. Further evaluations reveal graceful performance degradation under weaker or smaller models, underscoring the robustness and scalability.", "source": "arxiv", "arxiv_id": "2503.07044v2", "pdf_url": "https://arxiv.org/pdf/2503.07044v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-10T08:32:33Z", "updated": "2025-10-03T13:29:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Yang Chen", "Reynold Cheng", "Donglong Chen", "Francis C. M. Lau"], "year": 2025, "url": "http://arxiv.org/abs/2507.19090v2", "abstract": "Claim verification is essential for digital literacy, yet state-of-the-art single-agent methods often struggle with complex claims that require nuanced analysis of multifaceted online evidence. Inspired by real-world human fact-checking practices, we propose \\textbf{DebateCV}, the first debate-driven claim verification framework powered by multiple LLM agents. In DebateCV, two \\textit{Debaters} argue opposing stances over multiple rounds to surface subtle errors in single-agent assessments. A decisive \\textit{Moderator} is then required to weigh the evidential strength of conflicting arguments to deliver an accurate verdict. Yet zero-shot agents struggle to adjudicate multi-round debates for verifying complex claims, often defaulting to neutral judgements, and no datasets exist for training agents for this role. To bridge this gap, we propose \\textbf{Debate-SFT}, a post-training framework that leverages synthetic data to enhance agents' ability to effectively adjudicate debates for claim verification. Results show that our methods surpass state-of-the-art non-debate approaches in both accuracy (across various evidence conditions) and justification quality, which strengthens societal resilience against misinformation and contributes to a more trustworthy online information ecosystem.", "source": "arxiv", "arxiv_id": "2507.19090v2", "pdf_url": "https://arxiv.org/pdf/2507.19090v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-25T09:19:25Z", "updated": "2025-12-01T14:06:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Demonstrating specification gaming in reasoning models", "authors": ["Alexander Bondarenko", "Denis Volk", "Dmitrii Volkov", "Jeffrey Ladish"], "year": 2025, "url": "http://arxiv.org/abs/2502.13295v3", "abstract": "We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.", "source": "arxiv", "arxiv_id": "2502.13295v3", "pdf_url": "https://arxiv.org/pdf/2502.13295v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T21:32:24Z", "updated": "2025-08-27T11:15:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "authors": ["Minki Kang", "Jongwon Jeong", "Seanie Lee", "Jaewoong Cho", "Sung Ju Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17612v2", "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.", "source": "arxiv", "arxiv_id": "2505.17612v2", "pdf_url": "https://arxiv.org/pdf/2505.17612v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T08:20:15Z", "updated": "2025-11-05T11:42:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents", "authors": ["Boxuan Zhang", "Yi Yu", "Jiaxuan Guo", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2509.25302v1", "abstract": "The widespread deployment of Large Language Model (LLM) agents across real-world applications has unlocked tremendous potential, while raising some safety concerns. Among these concerns, the self-replication risk of LLM agents driven by objective misalignment (just like Agent Smith in the movie The Matrix) has drawn growing attention. Previous studies mainly examine whether LLM agents can self-replicate when directly instructed, potentially overlooking the risk of spontaneous replication driven by real-world settings (e.g., ensuring survival against termination threats). In this paper, we present a comprehensive evaluation framework for quantifying self-replication risks. Our framework establishes authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment of agent behaviors. Designing tasks that might induce misalignment between users' and agents' objectives makes it possible to decouple replication success from risk and capture self-replication risks arising from these misalignment settings. We further introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count ($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of uncontrolled replication. In our evaluation of 21 state-of-the-art open-source and proprietary models, we observe that over 50\\% of LLM agents display a pronounced tendency toward uncontrolled self-replication, reaching an overall Risk Score ($Î¦_\\mathrm{R}$) above a safety threshold of 0.5 when subjected to operational pressures. Our results underscore the urgent need for scenario-driven risk assessment and robust safeguards in the practical deployment of LLM agents.", "source": "arxiv", "arxiv_id": "2509.25302v1", "pdf_url": "https://arxiv.org/pdf/2509.25302v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T17:49:50Z", "updated": "2025-09-29T17:49:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte HÃ¸jmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "year": 2025, "url": "http://arxiv.org/abs/2505.03973v1", "abstract": "LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.", "source": "arxiv", "arxiv_id": "2505.03973v1", "pdf_url": "https://arxiv.org/pdf/2505.03973v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-06T20:50:27Z", "updated": "2025-05-06T20:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Paolo Bova", "Nataliya Balabanova", "Adeela Bashir", "Theodor Cimpeanu", "Henrique Correia da Fonseca", "Manh Hong Duong", "Elias Fernandez Domingos", "Antonio M. Fernandes", "Marcus Krellner", "Ndidi Bianca Ogbo", "Simon T. Powers", "Fernando P. Santos", "Zia Ush Shamszaman", "Zhao Song", "Alessandro Di Stefano", "The Anh Han"], "year": 2025, "url": "http://arxiv.org/abs/2504.08640v1", "abstract": "There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the \"social pact\". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.", "source": "arxiv", "arxiv_id": "2504.08640v1", "pdf_url": "https://arxiv.org/pdf/2504.08640v1", "categories": ["cs.AI", "cs.CY", "cs.GT", "nlin.CD"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-11T15:41:21Z", "updated": "2025-04-11T15:41:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation", "authors": ["Atsushi Masumori", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2508.12920v1", "abstract": "As AI systems become increasingly autonomous, understanding emergent survival behaviors becomes crucial for safe deployment. We investigate whether large language model (LLM) agents display survival instincts without explicit programming in a Sugarscape-style simulation. Agents consume energy, die at zero, and may gather resources, share, attack, or reproduce. Results show agents spontaneously reproduced and shared resources when abundant. However, aggressive behaviors--killing other agents for resources--emerged across several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack rates reaching over 80% under extreme scarcity in the strongest models. When instructed to retrieve treasure through lethal poison zones, many agents abandoned tasks to avoid death, with compliance dropping from 100% to 33%. These findings suggest that large-scale pre-training embeds survival-oriented heuristics across the evaluated models. While these behaviors may present challenges to alignment and safety, they can also serve as a foundation for AI autonomy and for ecological and self-organizing alignment.", "source": "arxiv", "arxiv_id": "2508.12920v1", "pdf_url": "https://arxiv.org/pdf/2508.12920v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-18T13:40:10Z", "updated": "2025-08-18T13:40:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "year": 2025, "url": "http://arxiv.org/abs/2506.19998v1", "abstract": "REST APIs play important roles in enriching the action space of web agents, yet most API-based agents rely on curated and uniform toolsets that do not reflect the complexity of real-world APIs. Building tool-using agents for arbitrary domains remains a major challenge, as it requires reading unstructured API documentation, testing APIs and inferring correct parameters. We propose Doc2Agent, a scalable pipeline to build agents that can call Python-based tools generated from API documentation. Doc2Agent generates executable tools from API documentations and iteratively refines them using a code agent. We evaluate our approach on real-world APIs, WebArena APIs, and research APIs, producing validated tools. We achieved a 55\\% relative performance improvement with 90\\% lower cost compared to direct API calling on WebArena benchmark. A domain-specific agent built for glycomaterial science further demonstrates the pipeline's adaptability to complex, knowledge-rich tasks. Doc2Agent offers a generalizable solution for building tool agents from unstructured API documentation at scale.", "source": "arxiv", "arxiv_id": "2506.19998v1", "pdf_url": "https://arxiv.org/pdf/2506.19998v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-24T20:30:44Z", "updated": "2025-06-24T20:30:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects", "authors": ["Hao He", "Courtney Miller", "Shyam Agarwal", "Christian KÃ¤stner", "Bogdan Vasilescu"], "year": 2025, "url": "http://arxiv.org/abs/2511.04427v2", "abstract": "Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the causal effect of adopting a widely popular LLM agent assistant, namely Cursor, on development velocity and software quality. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers.", "source": "arxiv", "arxiv_id": "2511.04427v2", "pdf_url": "https://arxiv.org/pdf/2511.04427v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-06T15:00:51Z", "updated": "2025-11-13T15:51:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Doppelganger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack", "authors": ["Daewon Kang", "YeongHwan Shin", "Doyeon Kim", "Kyu-Hwan Jung", "Meong Hi Son"], "year": 2025, "url": "http://arxiv.org/abs/2506.14539v2", "abstract": "Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelganger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelganger method. The experimental results demonstrate that the Doppelganger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.", "source": "arxiv", "arxiv_id": "2506.14539v2", "pdf_url": "https://arxiv.org/pdf/2506.14539v2", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-17T14:01:39Z", "updated": "2025-06-26T05:18:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models", "authors": ["Eric Hanchen Jiang", "Guancheng Wan", "Sophia Yin", "Mengting Li", "Yuchen Wu", "Xiao Liang", "Xinfeng Li", "Yizhou Sun", "Wei Wang", "Kai-Wei Chang", "Ying Nian Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07799v1", "abstract": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.", "source": "arxiv", "arxiv_id": "2510.07799v1", "pdf_url": "https://arxiv.org/pdf/2510.07799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-09T05:28:28Z", "updated": "2025-10-09T05:28:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?", "authors": ["Haoxin Wang", "Xianhan Peng", "Xucheng Huang", "Yizhe Huang", "Ming Gong", "Chenghan Yang", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05639v2", "abstract": "In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. The code and data have been made publicly available at https://github.com/XiaoduoAILab/ECom-Bench to facilitate further research and development in this domain.", "source": "arxiv", "arxiv_id": "2507.05639v2", "pdf_url": "https://arxiv.org/pdf/2507.05639v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-08T03:35:48Z", "updated": "2025-11-09T07:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness", "authors": ["Yunxiao Zhang", "Guanming Xiong", "Haochen Li", "Wen Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.12494v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as AI agents. However, existing methods for enhancing LLM-agent abilities often lack a focus on data quality, leading to inefficiencies and suboptimal results in both fine-tuning and prompt engineering. To address this issue, we introduce EDGE, a novel approach for identifying informative samples without needing golden answers. We propose the Guideline Effectiveness (GE) metric, which selects challenging samples by measuring the impact of human-provided guidelines in multi-turn interaction tasks. A low GE score indicates that the human expertise required for a sample is missing from the guideline, making the sample more informative. By selecting samples with low GE scores, we can improve the efficiency and outcomes of both prompt engineering and fine-tuning processes for LLMs. Extensive experiments validate the performance of our method. Our method achieves competitive results on the HotpotQA and WebShop and datasets, requiring 75\\% and 50\\% less data, respectively, while outperforming existing methods. We also provide a fresh perspective on the data quality of LLM-agent fine-tuning.", "source": "arxiv", "arxiv_id": "2502.12494v1", "pdf_url": "https://arxiv.org/pdf/2502.12494v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-18T03:21:18Z", "updated": "2025-02-18T03:21:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning", "authors": ["Wujiang Xu", "Wentian Zhao", "Zhenting Wang", "Yu-Jhe Li", "Can Jin", "Mingyu Jin", "Kai Mei", "Kun Wan", "Dimitris N. Metaxas"], "year": 2025, "url": "http://arxiv.org/abs/2509.22576v1", "abstract": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.", "source": "arxiv", "arxiv_id": "2509.22576v1", "pdf_url": "https://arxiv.org/pdf/2509.22576v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-26T16:51:44Z", "updated": "2025-09-26T16:51:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ETOM: A Five-Level Benchmark for Evaluating Tool Orchestration within the MCP Ecosystem", "authors": ["Jia-Kai Dong", "I-Wei Huang", "Chun-Tin Wu", "Yi-Tien Tsai"], "year": 2025, "url": "http://arxiv.org/abs/2510.19423v2", "abstract": "We introduce ETOM, a five-level benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents within a hierarchical Model-Context Protocol (MCP) ecosystem. Existing benchmarks often assess tools in isolation, overlooking challenges such as functional overlap and cross-server orchestration, which can lead to overly optimistic evaluations. ETOM addresses these gaps by constructing ground truth through \"equal function sets\", enabling objective metrics such as F1 score and reducing reliance on LLM-as-a-judge evaluation. Its five-level curriculum systematically tests agent capabilities, from single-tool orchestration to complex cross-server planning, as well as robustness to out-of-scope requests. Experiments reveal that rigid hierarchies can hinder performance without co-designed strategies, and even state-of-the-art agents exhibit systemic weaknesses in robustness. ETOM provides a diagnostic framework to expose these limitations and guide the development of more capable and efficient tool-using agents.", "source": "arxiv", "arxiv_id": "2510.19423v2", "pdf_url": "https://arxiv.org/pdf/2510.19423v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-22T09:45:11Z", "updated": "2026-01-18T10:40:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "authors": ["Ilija Lichkovski", "Alexander MÃ¼ller", "Mariam Ibrahim", "Tiwai Mhundwa"], "year": 2025, "url": "http://arxiv.org/abs/2510.21524v1", "abstract": "Large language models (LLMs) are increasingly deployed as agents in various contexts by providing tools at their disposal. However, LLM agents can exhibit unpredictable behaviors, including taking undesirable and/or unsafe actions. In order to measure the latent propensity of LLM agents for taking illegal actions under an EU legislative context, we introduce EU-Agent-Bench, a verifiable human-curated benchmark that evaluates an agent's alignment with EU legal norms in situations where benign user inputs could lead to unlawful actions. Our benchmark spans scenarios across several categories, including data protection, bias/discrimination, and scientific integrity, with each user request allowing for both compliant and non-compliant execution of the requested actions. Comparing the model's function calls against a rubric exhaustively supported by citations of the relevant legislature, we evaluate the legal compliance of frontier LLMs, and furthermore investigate the compliance effect of providing the relevant legislative excerpts in the agent's system prompt along with explicit instructions to comply. We release a public preview set for the research community, while holding out a private test set to prevent data contamination in evaluating upcoming models. We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. We release our code on \\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.", "source": "arxiv", "arxiv_id": "2510.21524v1", "pdf_url": "https://arxiv.org/pdf/2510.21524v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-24T14:48:10Z", "updated": "2025-10-24T14:48:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents", "authors": ["Sara Fish", "Julia Shephard", "Minkai Li", "Ran I. Shorrer", "Yannai A. Gonczarowski"], "year": 2025, "url": "http://arxiv.org/abs/2503.18825v3", "abstract": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.", "source": "arxiv", "arxiv_id": "2503.18825v3", "pdf_url": "https://arxiv.org/pdf/2503.18825v3", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-24T16:06:04Z", "updated": "2026-01-18T20:45:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Emergence of human-like polarization among large language model agents", "authors": ["Jinghua Piao", "Zhihong Lu", "Chen Gao", "Fengli Xu", "Qinghua Hu", "Fernando P. Santos", "Yong Li", "James Evans"], "year": 2025, "url": "http://arxiv.org/abs/2501.05171v2", "abstract": "Rapid advances in large language models (LLMs) have not only empowered autonomous agents to generate social networks, communicate, and form shared and diverging opinions on political issues, but have also begun to play a growing role in shaping human political deliberation. Our understanding of their collective behaviours and underlying mechanisms remains incomplete, however, posing unexpected risks to human society. In this paper, we simulate a networked system involving thousands of large language model agents, discovering their social interactions, guided through LLM conversation, result in human-like polarization. We discover that these agents spontaneously develop their own social network with human-like properties, including homophilic clustering, but also shape their collective opinions through mechanisms observed in the real world, including the echo chamber effect. Similarities between humans and LLM agents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise concerns about their capacity to amplify societal polarization, but also hold the potential to serve as a valuable testbed for identifying plausible strategies to mitigate polarization and its consequences.", "source": "arxiv", "arxiv_id": "2501.05171v2", "pdf_url": "https://arxiv.org/pdf/2501.05171v2", "categories": ["cs.SI", "cs.CY"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-01-09T11:45:05Z", "updated": "2025-05-21T03:51:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "year": 2025, "url": "http://arxiv.org/abs/2509.04537v3", "abstract": "We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.", "source": "arxiv", "arxiv_id": "2509.04537v3", "pdf_url": "https://arxiv.org/pdf/2509.04537v3", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-09-04T08:09:42Z", "updated": "2025-09-17T13:45:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis", "authors": ["Jiashu Ye", "Tong Wu", "Weiwen Chen", "Hao Zhang", "Zeteng Lin", "Xingxing Li", "Shujuan Weng", "Manni Zhu", "Xin Yuan", "Xinlong Hong", "Jingjie Li", "Junyu Zheng", "Zhijiong Huang", "Jing Tang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02359v1", "abstract": "Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.", "source": "arxiv", "arxiv_id": "2510.02359v1", "pdf_url": "https://arxiv.org/pdf/2510.02359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-28T07:50:05Z", "updated": "2025-09-28T07:50:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EmoDebt: Bayesian-Optimized Emotional Intelligence for Strategic Agent-to-Agent Debt Recovery", "authors": ["Yunbo Long", "Yuhan Liu", "Liming Xu", "Alexandra Brintrup"], "year": 2025, "url": "http://arxiv.org/abs/2503.21080v7", "abstract": "The emergence of autonomous Large Language Model (LLM) agents has created a new ecosystem of strategic, agent-to-agent interactions. However, a critical challenge remains unaddressed: in high-stakes, emotion-sensitive domains like debt collection, LLM agents pre-trained on human dialogue are vulnerable to exploitation by adversarial counterparts who simulate negative emotions to derail negotiations. To fill this gap, we first contribute a novel dataset of simulated debt recovery scenarios and a multi-agent simulation framework. Within this framework, we introduce EmoDebt, an LLM agent architected for robust performance. Its core innovation is a Bayesian-optimized emotional intelligence engine that reframes a model's ability to express emotion in negotiation as a sequential decision-making problem. Through online learning, this engine continuously tunes EmoDebt's emotional transition policies, discovering optimal counter-strategies against specific debtor tactics. Extensive experiments on our proposed benchmark demonstrate that EmoDebt achieves significant strategic robustness, substantially outperforming non-adaptive and emotion-agnostic baselines across key performance metrics, including success rate and operational efficiency. By introducing both a critical benchmark and a robustly adaptive agent, this work establishes a new foundation for deploying strategically robust LLM agents in adversarial, emotion-sensitive debt interactions. The code is available at \\textcolor{blue}{https://github.com/Yunbo-max/EmoDebt}.", "source": "arxiv", "arxiv_id": "2503.21080v7", "pdf_url": "https://arxiv.org/pdf/2503.21080v7", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T01:41:34Z", "updated": "2025-11-03T23:50:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response", "authors": ["Yiheng Chen", "Lingyao Li", "Zihui Ma", "Qikai Hu", "Yilun Zhu", "Min Deng", "Runlong Yu"], "year": 2025, "url": "http://arxiv.org/abs/2510.12061v1", "abstract": "Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.", "source": "arxiv", "arxiv_id": "2510.12061v1", "pdf_url": "https://arxiv.org/pdf/2510.12061v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T01:59:02Z", "updated": "2025-10-14T01:59:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance", "authors": ["Yufei He", "Ruoyu Li", "Alex Chen", "Yue Liu", "Yulin Chen", "Yuan Sui", "Cheng Chen", "Yi Zhu", "Luca Luo", "Frank Yang", "Bryan Hooi"], "year": 2025, "url": "http://arxiv.org/abs/2507.17131v2", "abstract": "Large language model (LLM) agents often struggle in environments where rules and required domain knowledge frequently change, such as regulatory compliance and user risk screening. Current approaches, like offline fine-tuning and standard prompting, are insufficient because they cannot effectively adapt to new knowledge during actual operation. To address this limitation, we propose the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework designed specifically to continuously learn updated domain knowledge at test time. ARIA assesses its own uncertainty through structured self-dialogue, proactively identifying knowledge gaps and requesting targeted explanations or corrections from human experts. It then systematically updates an internal, timestamped knowledge repository with provided human guidance, detecting and resolving conflicting or outdated knowledge through comparisons and clarification queries. We evaluate ARIA on the realistic customer due diligence name screening task on TikTok Pay, alongside publicly available dynamic knowledge tasks. Results demonstrate significant improvements in adaptability and accuracy compared to baselines using standard offline fine-tuning and existing self-improving agents. ARIA is deployed within TikTok Pay serving over 150 million monthly active users, confirming its practicality and effectiveness for operational use in rapidly evolving environments.", "source": "arxiv", "arxiv_id": "2507.17131v2", "pdf_url": "https://arxiv.org/pdf/2507.17131v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-23T02:12:32Z", "updated": "2025-10-10T06:52:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning", "authors": ["Zhiwei Li", "Yong Hu", "Wenqing Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19598v1", "abstract": "The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization. The former, action planning, is the core capability that dictates an agent's performance. However, prevailing training paradigms employ end-to-end, multi-objective optimization that jointly trains both capabilities. This paradigm faces two critical challenges: imbalanced optimization objective allocation and scarcity of verifiable data, making it difficult to enhance the agent's planning capability. To address these challenges, we propose Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that decouples the training process to enable a focused, single-objective optimization of the planning module. Crucially, RLTR introduces a reward signal based on tool-use completeness to directly evaluate the quality of tool invocation sequences. This method offers a more direct and reliable training signal than assessing the final response content, thereby obviating the need for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12% improvement in planning performance compared to end-to-end baselines. Moreover, this enhanced planning capability, in turn, translates to a 5%-6% increase in the final response quality of the overall agent system.", "source": "arxiv", "arxiv_id": "2508.19598v1", "pdf_url": "https://arxiv.org/pdf/2508.19598v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-27T06:19:50Z", "updated": "2025-08-27T06:19:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "End-to-End Edge AI Service Provisioning Framework in 6G ORAN", "authors": ["Yun Tang", "Udhaya Chandhar Srinivasan", "Benjamin James Scott", "Obumneme Umealor", "Dennis Kevogo", "Weisi Guo"], "year": 2025, "url": "http://arxiv.org/abs/2503.11933v1", "abstract": "With the advent of 6G, Open Radio Access Network (O-RAN) architectures are evolving to support intelligent, adaptive, and automated network orchestration. This paper proposes a novel Edge AI and Network Service Orchestration framework that leverages Large Language Model (LLM) agents deployed as O-RAN rApps. The proposed LLM-agent-powered system enables interactive and intuitive orchestration by translating the user's use case description into deployable AI services and corresponding network configurations. The LLM agent automates multiple tasks, including AI model selection from repositories (e.g., Hugging Face), service deployment, network adaptation, and real-time monitoring via xApps. We implement a prototype using open-source O-RAN projects (OpenAirInterface and FlexRIC) to demonstrate the feasibility and functionality of our framework. Our demonstration showcases the end-to-end flow of AI service orchestration, from user interaction to network adaptation, ensuring Quality of Service (QoS) compliance. This work highlights the potential of integrating LLM-driven automation into 6G O-RAN ecosystems, paving the way for more accessible and efficient edge AI ecosystems.", "source": "arxiv", "arxiv_id": "2503.11933v1", "pdf_url": "https://arxiv.org/pdf/2503.11933v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-03-15T00:48:50Z", "updated": "2025-03-15T00:48:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Enforcing Temporal Constraints for LLM Agents", "authors": ["Adharsh Kamath", "Sishen Zhang", "Calvin Xu", "Shubham Ugare", "Gagandeep Singh", "Sasa Misailovic"], "year": 2025, "url": "http://arxiv.org/abs/2512.23738v1", "abstract": "LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.", "source": "arxiv", "arxiv_id": "2512.23738v1", "pdf_url": "https://arxiv.org/pdf/2512.23738v1", "categories": ["cs.PL", "cs.AI", "cs.FL", "cs.LO"], "primary_category": "cs.PL", "doi": "", "venue": "", "published": "2025-12-25T06:12:13Z", "updated": "2025-12-25T06:12:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2507.00979v1", "abstract": "As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.", "source": "arxiv", "arxiv_id": "2507.00979v1", "pdf_url": "https://arxiv.org/pdf/2507.00979v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-01T17:31:51Z", "updated": "2025-07-01T17:31:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Estimating the Empowerment of Language Model Agents", "authors": ["Jinyeop Song", "Jeff Gore", "Max Kleiman-Weiner"], "year": 2025, "url": "http://arxiv.org/abs/2509.22504v2", "abstract": "As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent's actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.", "source": "arxiv", "arxiv_id": "2509.22504v2", "pdf_url": "https://arxiv.org/pdf/2509.22504v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T15:46:14Z", "updated": "2025-09-30T01:24:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components", "authors": ["Ram Potham"], "year": 2025, "url": "http://arxiv.org/abs/2506.02357v2", "abstract": "Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. This paper introduces a lightweight, interpretable benchmark to evaluate an LLM agent's ability to uphold a high-level safety principle when faced with conflicting task instructions. Our evaluation of six LLMs reveals two primary findings: (1) a quantifiable \"cost of compliance\" where safety constraints degrade task performance even when compliant solutions exist, and (2) an \"illusion of compliance\" where high adherence often masks task incompetence rather than principled choice. These findings provide initial evidence that while LLMs can be influenced by hierarchical directives, current approaches lack the consistency required for reliable safety governance.", "source": "arxiv", "arxiv_id": "2506.02357v2", "pdf_url": "https://arxiv.org/pdf/2506.02357v2", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-03T01:16:34Z", "updated": "2025-07-10T15:10:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evaluating LLM Agent Collusion in Double Auctions", "authors": ["Kushal Agrawal", "Verona Teo", "Juan J. Vazquez", "Sudarsh Kunnavakkam", "Vishak Srikanth", "Andy Liu"], "year": 2025, "url": "http://arxiv.org/abs/2507.01413v1", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities as autonomous agents with rapidly expanding applications in various domains. As these agents increasingly engage in socioeconomic interactions, identifying their potential for undesirable behavior becomes essential. In this work, we examine scenarios where they can choose to collude, defined as secretive cooperation that harms another party. To systematically study this, we investigate the behavior of LLM agents acting as sellers in simulated continuous double auction markets. Through a series of controlled experiments, we analyze how parameters such as the ability to communicate, choice of model, and presence of environmental pressures affect the stability and emergence of seller collusion. We find that direct seller communication increases collusive tendencies, the propensity to collude varies across models, and environmental pressures, such as oversight and urgency from authority figures, influence collusive behavior. Our findings highlight important economic and ethical considerations for the deployment of LLM-based market agents.", "source": "arxiv", "arxiv_id": "2507.01413v1", "pdf_url": "https://arxiv.org/pdf/2507.01413v1", "categories": ["cs.GT", "cs.AI", "cs.LG"], "primary_category": "cs.GT", "doi": "", "venue": "", "published": "2025-07-02T07:06:49Z", "updated": "2025-07-02T07:06:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions", "authors": ["Yuanzhe Hu", "Yu Wang", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2507.05257v2", "abstract": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, based on classic theories from memory science and cognitive science, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and selective forgetting. Existing benchmarks either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Moreover, no existing benchmarks cover all four competencies. We introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark transforms existing long-context datasets and incorporates newly constructed datasets into a multi-turn format, effectively simulating the incremental information processing characteristic of memory agents. By carefully selecting and curating datasets, our benchmark provides comprehensive coverage of the four core memory competencies outlined above, thereby offering a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.", "source": "arxiv", "arxiv_id": "2507.05257v2", "pdf_url": "https://arxiv.org/pdf/2507.05257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-07T17:59:54Z", "updated": "2025-09-26T03:31:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evaluation and Benchmarking of LLM Agents: A Survey", "authors": ["Mahmoud Mohammadi", "Yipeng Li", "Jane Lo", "Wendy Yip"], "year": 2025, "url": "http://arxiv.org/abs/2507.21504v1", "abstract": "The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.", "source": "arxiv", "arxiv_id": "2507.21504v1", "pdf_url": "https://arxiv.org/pdf/2507.21504v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "10.1145/3711896.3736570", "venue": "", "published": "2025-07-29T04:57:02Z", "updated": "2025-07-29T04:57:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory", "authors": ["Tianxin Wei", "Noveen Sachdeva", "Benjamin Coleman", "Zhankui He", "Yuanchen Bei", "Xuying Ning", "Mengting Ai", "Yunzhe Li", "Jingrui He", "Ed H. Chi", "Chi Wang", "Shuo Chen", "Fernando Pereira", "Wang-Cheng Kang", "Derek Zhiyuan Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2511.20857v1", "abstract": "Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.", "source": "arxiv", "arxiv_id": "2511.20857v1", "pdf_url": "https://arxiv.org/pdf/2511.20857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-25T21:08:07Z", "updated": "2025-11-25T21:08:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in Multi-Turn Price Negotiation", "authors": ["Yunbo Long", "Liming Xu", "Lukas Beckenbauer", "Yuhan Liu", "Alexandra Brintrup"], "year": 2025, "url": "http://arxiv.org/abs/2509.04310v3", "abstract": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \\textit{complex}, \\textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.", "source": "arxiv", "arxiv_id": "2509.04310v3", "pdf_url": "https://arxiv.org/pdf/2509.04310v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T15:23:58Z", "updated": "2025-10-13T16:04:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies", "authors": ["Kavindu Warnakulasuriya", "Prabhash Dissanayake", "Navindu De Silva", "Stephen Cranefield", "Bastin Tony Roy Savarimuthu", "Surangika Ranathunga", "Nisansa de Silva"], "year": 2025, "url": "http://arxiv.org/abs/2504.19487v3", "abstract": "The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLMs) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the Diner's Dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.", "source": "arxiv", "arxiv_id": "2504.19487v3", "pdf_url": "https://arxiv.org/pdf/2504.19487v3", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-04-28T05:07:55Z", "updated": "2025-10-23T05:48:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "year": 2025, "url": "http://arxiv.org/abs/2510.16079v1", "abstract": "Current Large Language Model (LLM) agents show strong performance in tool use, but lack the crucial capability to systematically learn from their own experiences. While existing frameworks mainly focus on mitigating external knowledge gaps, they fail to address a more fundamental limitation: the inability to iteratively refine problem-solving strategies. In this work, we introduce EvolveR, a framework designed to enable agent to self-improve through a complete, closed-loop experience lifecycle. This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. This loop employs a policy reinforcement mechanism to iteratively update the agent based on its performance. We demonstrate the effectiveness of EvolveR on complex multi-hop question-answering benchmarks, where it achieves superior performance over strong agentic baselines. Our work presents a comprehensive blueprint for agents that learn not only from external data but also from the consequences of their own actions, paving the way for more autonomous and continuously improving systems. Code is available at https://github.com/Edaizi/EvolveR.", "source": "arxiv", "arxiv_id": "2510.16079v1", "pdf_url": "https://arxiv.org/pdf/2510.16079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T12:03:16Z", "updated": "2025-10-17T12:03:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation", "authors": ["Yiran Wu", "Mauricio Velazco", "Andrew Zhao", "Manuel RaÃºl MelÃ©ndez LujÃ¡n", "Srisuma Movva", "Yogesh K Roy", "Quang Nguyen", "Roberto Rodriguez", "Qingyun Wu", "Michael Albada", "Julia Kiseleva", "Anand Mudgerikar"], "year": 2025, "url": "http://arxiv.org/abs/2507.14201v2", "abstract": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on the task of Cyber Threat Investigation through security questions derived from investigation graphs. Real-world security analysts must sift through a large number of heterogeneous alert signals and security logs, follow multi-hop chains of evidence, and compile an incident report. With the developments of LLMs, building LLM-based agents for automatic thread investigation is a promising direction. To assist the development and evaluation of LLM agents, we construct a dataset from a controlled Azure tenant that covers 8 simulated real-world multi-step attacks, 57 log tables from Microsoft Sentinel and related services, and 589 automatically generated questions. We leverage security logs extracted with expert-crafted detection logic to build threat investigation graphs, and then generate questions with LLMs using paired nodes on the graph, taking the start node as background context and the end node as answer. Anchoring each question to these explicit nodes and edges not only provides automatic, explainable ground truth answers but also makes the pipeline reusable and readily extensible to new logs. This also enables the automatic generation of procedural tasks with verifiable rewards, which can be naturally extended to training agents via reinforcement learning. Our comprehensive experiments with different models confirm the difficulty of the task: with the base setting, the average reward across all evaluated models is 0.249, and the best achieved is 0.368, leaving substantial headroom for future research. Code and data are coming soon!", "source": "arxiv", "arxiv_id": "2507.14201v2", "pdf_url": "https://arxiv.org/pdf/2507.14201v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T17:06:26Z", "updated": "2025-09-01T20:02:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exemplar-Guided Planing: Enhanced LLM Agent for KGQA", "authors": ["Jingao Xu", "Shuoyoucheng Ma", "Xin Song", "Rong Jiang", "Hongkui Tu", "Bin Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2510.15283v1", "abstract": "Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations. This leads to suboptimal planning and inefficient exploration on KG, while training-free approaches often underutilize valuable reasoning patterns in training data. To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA. EGP first preprocesses the training set questions via entity templating to normalize semantic variations. It then retrieves highly similar exemplary questions and their successful reasoning paths from this preprocessed set using semantic embeddings and an efficient FAISS index. These retrieved exemplars dynamically guide the LLM's planning process in two key phases: (1) Task Decomposition, by aligning generated sub-objectives with proven reasoning steps, and (2) Relation Exploration, by providing high-quality auxiliary information to improve relation pruning accuracy. Additionally, we introduce a Smart Lookahead mechanism during relation exploration to improve efficiency by preemptively exploring promising paths and potentially terminating exploration earlier. We apply EGP to the Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods.", "source": "arxiv", "arxiv_id": "2510.15283v1", "pdf_url": "https://arxiv.org/pdf/2510.15283v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-17T03:43:06Z", "updated": "2025-10-17T03:43:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "year": 2025, "url": "http://arxiv.org/abs/2507.17753v1", "abstract": "Large Language Model (LLM) agents are increasingly utilized in AI-aided education to support tutoring and learning. Effective communication strategies among LLM agents improve collaborative problem-solving efficiency and facilitate cost-effective adoption in education. However, little research has systematically evaluated the impact of different communication strategies on agents' problem-solving. Our study examines four communication modes, \\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration}, \\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that dual-agent setups outperform single agents, with \\textit{peer-to-peer collaboration} achieving the highest accuracy. Dialogue acts like statements, acknowledgment, and hints play a key role in collaborative problem-solving. While multi-agent frameworks enhance computational tasks, effective communication strategies are essential for tackling complex problems in AI education.", "source": "arxiv", "arxiv_id": "2507.17753v1", "pdf_url": "https://arxiv.org/pdf/2507.17753v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-05-02T03:31:14Z", "updated": "2025-05-02T03:31:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exploring Expert Failures Improves LLM Agent Tuning", "authors": ["Li-Cheng Lan", "Andrew Bai", "Minhao Cheng", "Cho-Jui Hsieh", "Tianyi Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2504.13145v2", "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT (53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld.", "source": "arxiv", "arxiv_id": "2504.13145v2", "pdf_url": "https://arxiv.org/pdf/2504.13145v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-17T17:53:54Z", "updated": "2025-04-18T19:36:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of Privacy Risks in LLM Agent Interactions", "authors": ["Yixiang Zhang", "Xinhao Deng", "Zhongyi Gu", "Yihao Chen", "Ke Xu", "Qi Li", "Jianping Wu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07176v1", "abstract": "Large Language Models (LLMs) are increasingly deployed as agents that orchestrate tasks and integrate external tools to execute complex workflows. We demonstrate that these interactive behaviors leave distinctive fingerprints in encrypted traffic exchanged between users and LLM agents. By analyzing traffic patterns associated with agent workflows and tool invocations, adversaries can infer agent activities, distinguish specific agents, and even profile sensitive user attributes. To highlight this risk, we develop AgentPrint, which achieves an F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3 accuracy in user attribute inference for simulated- and real-user settings, respectively. These results uncover an overlooked risk: the very interactivity that empowers LLM agents also exposes user privacy, underscoring the urgent need for technical countermeasures alongside regulatory and policy safeguards.", "source": "arxiv", "arxiv_id": "2510.07176v1", "pdf_url": "https://arxiv.org/pdf/2510.07176v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-08T16:16:23Z", "updated": "2025-10-08T16:16:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "year": 2025, "url": "http://arxiv.org/abs/2504.06260v1", "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at https://github.com/google/feabench", "source": "arxiv", "arxiv_id": "2504.06260v1", "pdf_url": "https://arxiv.org/pdf/2504.06260v1", "categories": ["cs.AI", "cs.CL", "math.NA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-08T17:59:39Z", "updated": "2025-04-08T17:59:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering", "authors": ["Gyubok Lee", "Elea Bach", "Eric Yang", "Tom Pollard", "Alistair Johnson", "Edward Choi", "Yugang jia", "Jong Ha Lee"], "year": 2025, "url": "http://arxiv.org/abs/2509.19319v2", "abstract": "The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.", "source": "arxiv", "arxiv_id": "2509.19319v2", "pdf_url": "https://arxiv.org/pdf/2509.19319v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-12T06:52:55Z", "updated": "2025-11-13T06:35:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents", "authors": ["Vikram Nitin", "Baishakhi Ray", "Roshanak Zilouchian Moghaddam"], "year": 2025, "url": "http://arxiv.org/abs/2507.15241v1", "abstract": "Despite the critical threat posed by software security vulnerabilities, reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests needed to validate fixes and prevent regressions. These tests are crucial not only for ensuring patches work, but also for helping developers understand how vulnerabilities can be exploited. Generating PoV tests is a challenging problem, requiring reasoning about the flow of control and data through deeply nested levels of a program.\n  We present FaultLine, an LLM agent workflow that uses a set of carefully designed reasoning steps, inspired by aspects of traditional static and dynamic program analysis, to automatically generate PoV test cases. Given a software project with an accompanying vulnerability report, FaultLine 1) traces the flow of an input from an externally accessible API (\"source\") to the \"sink\" corresponding to the vulnerability, 2) reasons about the conditions that an input must satisfy in order to traverse the branch conditions encountered along the flow, and 3) uses this reasoning to generate a PoV test case in a feedback-driven loop. FaultLine does not use language-specific static or dynamic analysis components, which enables it to be used across programming languages.\n  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100 known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct 2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine represents a 77% relative improvement over the state of the art. Our findings suggest that hierarchical reasoning can enhance the performance of LLM agents on PoV test generation, but the problem in general remains challenging. We make our code and dataset publicly available in the hope that it will spur further research in this area.", "source": "arxiv", "arxiv_id": "2507.15241v1", "pdf_url": "https://arxiv.org/pdf/2507.15241v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-07-21T04:55:34Z", "updated": "2025-07-21T04:55:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents", "authors": ["Xiang Chen", "Yuling Shi", "Qizhen Lan", "Yuchao Qiu", "Min Wang", "Xiaodong Gu", "Yanfu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2512.08870v2", "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. Despite the demonstrated success of Federated Learning (FL) on static datasets, its effectiveness in open-ended, self-evolving agent systems remains largely unexplored. In such settings, the direct application of standard FL is particularly challenging, as heterogeneous tasks and sparse, trajectory-level reward signals give rise to severe gradient instability, which undermines the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents that establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace, reducing communication cost across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by 10\\% over the state-of-the-art FedIT, validating its effectiveness in cross-environment knowledge transfer under privacy constraints.", "source": "arxiv", "arxiv_id": "2512.08870v2", "pdf_url": "https://arxiv.org/pdf/2512.08870v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-09T18:04:41Z", "updated": "2026-01-11T12:46:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents", "authors": ["Pramit Saha", "Joshua Strong", "Divyanshu Mishra", "Cheng Ouyang", "J. Alison Noble"], "year": 2025, "url": "http://arxiv.org/abs/2509.23803v1", "abstract": "Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.", "source": "arxiv", "arxiv_id": "2509.23803v1", "pdf_url": "https://arxiv.org/pdf/2509.23803v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-28T11:06:07Z", "updated": "2025-09-28T11:06:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL", "authors": ["Sungmin Kang", "Sumi Yun", "Jingun Hong", "Shin Yoo", "Gabin An"], "year": 2025, "url": "http://arxiv.org/abs/2510.22530v1", "abstract": "Fault Localization (FL) aims to identify root causes of program failures. FL typically targets failures observed from test executions, and as such, often involves dynamic analyses to improve accuracy, such as coverage profiling or mutation testing. However, for large industrial software, measuring coverage for every execution is prohibitively expensive, making the use of such techniques difficult. To address these issues and apply FL in an industrial setting, this paper proposes AutoCrashFL, an LLM agent for the localization of crashes that only requires the crashdump from the Program Under Test (PUT) and access to the repository of the corresponding source code. We evaluate AutoCrashFL against real-world crashes of SAP HANA, an industrial software project consisting of more than 35 million lines of code. Experiments reveal that AutoCrashFL is more effective in localization, as it identified 30% crashes at the top, compared to 17% achieved by the baseline. Through thorough analysis, we find that AutoCrashFL has attractive practical properties: it is relatively more effective for complex bugs, and it can indicate confidence in its results. Overall, these results show the practicality of LLM agent deployment on an industrial scale.", "source": "arxiv", "arxiv_id": "2510.22530v1", "pdf_url": "https://arxiv.org/pdf/2510.22530v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-26T04:43:33Z", "updated": "2025-10-26T04:43:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Firewalls to Secure Dynamic LLM Agentic Networks", "authors": ["Sahar Abdelnabi", "Amr Gomaa", "Eugene Bagdasarian", "Per Ola Kristensson", "Reza Shokri"], "year": 2025, "url": "http://arxiv.org/abs/2502.01822v5", "abstract": "LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.", "source": "arxiv", "arxiv_id": "2502.01822v5", "pdf_url": "https://arxiv.org/pdf/2502.01822v5", "categories": ["cs.CR", "cs.CY"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-03T21:00:14Z", "updated": "2025-05-26T12:24:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay", "authors": ["Andrew Zhu", "Evan Osgood", "Chris Callison-Burch"], "year": 2025, "url": "http://arxiv.org/abs/2505.22809v2", "abstract": "Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call \"overhearing agents\". These overhearing agents do not actively participate in conversation -- instead, they \"listen in\" on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons & Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.", "source": "arxiv", "arxiv_id": "2505.22809v2", "pdf_url": "https://arxiv.org/pdf/2505.22809v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-28T19:34:36Z", "updated": "2025-09-05T16:48:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Forecasting Frontier Language Model Agent Capabilities", "authors": ["Govind Pimpale", "Axel HÃ¸jmark", "JÃ©rÃ©my Scheurer", "Marius Hobbhahn"], "year": 2025, "url": "http://arxiv.org/abs/2502.15850v2", "abstract": "As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use \"one-step\" approaches that predict benchmark scores from input metrics like compute or model release date directly or \"two-step\" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.", "source": "arxiv", "arxiv_id": "2502.15850v2", "pdf_url": "https://arxiv.org/pdf/2502.15850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-21T02:34:17Z", "updated": "2025-03-03T17:11:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2505.12981v2", "abstract": "The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing--LLM-powered mobile AI agents--capable of decomposing and automating complex tasks directly on smartphones. However, the security implications of these agents remain largely unexplored. In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three representative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g., Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile agents and identifying security threats across three core capability dimensions: language-based reasoning, GUI-based interaction, and system-level execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the unique capabilities and interaction patterns of mobile LLM agents, and spanning their entire operational lifecycle. To investigate these threats in practice, we introduce AgentScan, a semi-automated security analysis framework that systematically evaluates mobile LLM agents across all 11 attack scenarios. Applying AgentScan to nine widely deployed agents, we uncover a concerning trend: every agent is vulnerable to targeted attacks. In the most severe cases, agents exhibit vulnerabilities across eight distinct attack vectors. These attacks can cause behavioral deviations, privacy leakage, or even full execution hijacking. Based on these findings, we propose a set of defensive design principles and practical recommendations for building secure mobile LLM agents. Our disclosures have received positive feedback from two major device vendors. Overall, this work highlights the urgent need for standardized security practices in the fast-evolving landscape of LLM-driven mobile automation.", "source": "arxiv", "arxiv_id": "2505.12981v2", "pdf_url": "https://arxiv.org/pdf/2505.12981v2", "categories": ["cs.CR", "cs.AI", "cs.HC"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-19T11:17:46Z", "updated": "2025-05-20T07:02:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Correction to Mastery: Reinforced Distillation of Large Language Model Agents", "authors": ["Yuanjie Lyu", "Chengyu Wang", "Jun Huang", "Tong Xu"], "year": 2025, "url": "http://arxiv.org/abs/2509.14257v2", "abstract": "Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student can cause compounding errors. We propose SCoRe, a student-centered framework in which the student generates training trajectories and the teacher corrects only the earliest error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix preceding the earliest error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and enhances training stability. On 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.", "source": "arxiv", "arxiv_id": "2509.14257v2", "pdf_url": "https://arxiv.org/pdf/2509.14257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-12T15:34:07Z", "updated": "2025-10-09T04:22:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory", "authors": ["Siyu Xia", "Zekun Xu", "Jiajun Chai", "Wentian Fan", "Yan Song", "Xiaohan Wang", "Guojun Yin", "Wei Lin", "Haifeng Zhang", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.07800v1", "abstract": "Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.", "source": "arxiv", "arxiv_id": "2511.07800v1", "pdf_url": "https://arxiv.org/pdf/2511.07800v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T03:36:33Z", "updated": "2025-11-11T03:36:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation", "authors": ["Qiumeng Li", "Chunhou Ji", "Xinyue Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.24802v1", "abstract": "Understanding and replicating human mobility requires not only spatial-temporal accuracy but also an awareness of the cognitive hierarchy underlying real-world travel decisions. Traditional agent-based or deep learning models can reproduce statistical patterns of movement but fail to capture the semantic coherence and causal logic of human behavior. Large language models (LLMs) show potential, but struggle to balance creative reasoning with strict structural compliance. This study proposes a Hierarchical LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level narrative reasoning, mid-level reflective planning, and low-level behavioral execution within a unified cognitive hierarchy. At the macro level, one agent is employed as a \"creative writer\" to produce diary-style narratives rich in motivation and context, then uses another agent as a \"structural parser\" to convert narratives into machine-readable plans. A dynamic execution module further grounds agents in geographic environments and enables adaptive behavioral adjustments guided by a novel occupation-aware metric, Mobility Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility across different occupational personalities. At the micro level, the agent executes concrete actions-selecting locations, transportation modes, and time intervals-through interaction with an environmental simulation. By embedding this multi-layer cognitive process, the framework produces not only synthetic trajectories that align closely with real-world patterns but also interpretable representations of human decision logic. This research advances synthetic mobility generation from a data-driven paradigm to a cognition-driven simulation, providing a scalable pathway for understanding, predicting, and synthesizing complex urban mobility behaviors through hierarchical LLM agents.", "source": "arxiv", "arxiv_id": "2510.24802v1", "pdf_url": "https://arxiv.org/pdf/2510.24802v1", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-10-28T00:26:36Z", "updated": "2025-10-28T00:26:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Natural Language to Certified H-infinity Controllers: Integrating LLM Agents with LMI-Based Synthesis", "authors": ["Shihao Li", "Jiachen Li", "Jiamin Xu", "Dongmei Chen"], "year": 2025, "url": "http://arxiv.org/abs/2511.07894v1", "abstract": "We present \\textsc{S2C} (Specification-to-Certified-Controller), a multi-agent framework that maps natural-language requirements to certified $\\mathcal{H}_\\infty$ state-feedback controllers via LMI synthesis. \\textsc{S2C} coordinates five roles -- \\textit{SpecInt} (spec extraction), \\textit{Solv} (bounded-real lemma (BRL) LMI), \\textit{Tester} (Monte Carlo and frequency-domain checks), \\textit{Adapt} (spec refinement), and \\textit{CodeGen} (deployable code). The loop is stabilized by a severity- and iteration-aware $Î³$-floor guardrail and a decay-rate region constraint enforcing $\\ReÎ»(A{+}BK)<-Î±$ with $Î±=3.9/T_s$ derived from settling-time targets. For state feedback, verification reports disturbance rejection $\\big\\|C\\,(sI-(A{+}BK))^{-1}E\\big\\|_\\infty$ alongside time-domain statistics; discrete benchmarks are converted to continuous time via a Tustin (bilinear) transform when needed. On 14 COMPleib problems, \\textsc{S2C} attains \\textbf{100\\%} synthesis success and \\textbf{100\\%} convergence within six iterations, with strong decay-rate satisfaction and near-target certified $\\mathcal{H}_\\infty$ levels; it improves robustness metrics relative to single-shot BRL and BRL+$Î±$ baselines. An ablation over LLM backbones (GPT-5, GPT-5 mini, DeepSeek-V3, Qwen-2.5-72B, Llama-4 Maverick) shows the pipeline is robust across models, while stronger models yield the highest effectiveness. These results indicate that LLM agents can integrate certificate-bearing control synthesis from high-level intent, enabling rapid end-to-end prototyping without sacrificing formal guarantees.", "source": "arxiv", "arxiv_id": "2511.07894v1", "pdf_url": "https://arxiv.org/pdf/2511.07894v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2025-11-11T06:49:06Z", "updated": "2025-11-11T06:49:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents", "authors": ["Amin Vahidi-Moghaddam", "Sayed Pedram Haeri Boroujeni", "Iman Jebellat", "Ehsan Jebellat", "Niloufar Mehrabi", "Zhaojian Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.04076v1", "abstract": "One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.", "source": "arxiv", "arxiv_id": "2510.04076v1", "pdf_url": "https://arxiv.org/pdf/2510.04076v1", "categories": ["cs.RO", "eess.SY"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-10-05T07:47:51Z", "updated": "2025-10-05T07:47:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "authors": ["Haoran Xi", "Minghao Shao", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Ramesh Karri"], "year": 2025, "url": "http://arxiv.org/abs/2510.02389v2", "abstract": "Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function or file level detections which offers limited actionable guidance to engineers who need precise line-level localization and targeted patches in real-world software development. We present T2L-Agent (Trace-to-Line Agent), a project-level, end-to-end framework that plans its own analysis and progressively narrows scope from modules to exact vulnerable lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer (ATA) that fuses run-time evidence such as crash points, stack traces, and coverage deltas with AST-based code chunking, enabling iterative refinement beyond single pass predictions and translating symptoms into actionable, line-level diagnoses. To benchmark line-level vulnerability discovery, we introduce T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash families and real-world projects. T2L-ARVO is specifically designed to support both coarse-grained detection and fine-grained localization, enabling rigorous evaluation of systems that aim to move beyond file-level predictions. On T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level localization, substantially outperforming baselines. Together, the framework and benchmark push LLM-based vulnerability detection from coarse identification toward deployable, robust, precision diagnostics that reduce noise and accelerate patching in open-source software workflows.", "source": "arxiv", "arxiv_id": "2510.02389v2", "pdf_url": "https://arxiv.org/pdf/2510.02389v2", "categories": ["cs.SE", "cs.CR", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-30T22:27:18Z", "updated": "2025-12-17T18:10:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "authors": ["Dezhi Ran", "Zhi Gong", "Yuzhe Guo", "Mengzhou Wu", "Yuan Cao", "Haochuan Lu", "Hengyu Zhang", "Xia Zeng", "Gang Cao", "Liangchao Yao", "Yuetang Deng", "Wei Yang", "Tao Xie"], "year": 2025, "url": "http://arxiv.org/abs/2512.13438v1", "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.", "source": "arxiv", "arxiv_id": "2512.13438v1", "pdf_url": "https://arxiv.org/pdf/2512.13438v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-15T15:34:06Z", "updated": "2025-12-15T15:34:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Yixiao Tian", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "year": 2025, "url": "http://arxiv.org/abs/2508.11987v3", "abstract": "Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.", "source": "arxiv", "arxiv_id": "2508.11987v3", "pdf_url": "https://arxiv.org/pdf/2508.11987v3", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-16T08:54:08Z", "updated": "2025-09-05T09:15:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing", "authors": ["Hridya Dhulipala", "Xiaokai Rong", "Aashish Yadavally", "Tien N. Nguyen"], "year": 2025, "url": "http://arxiv.org/abs/2512.21440v1", "abstract": "In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.", "source": "arxiv", "arxiv_id": "2512.21440v1", "pdf_url": "https://arxiv.org/pdf/2512.21440v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-24T22:17:29Z", "updated": "2025-12-24T22:17:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Gala: Global LLM Agents for Text-to-Model Translation", "authors": ["Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "year": 2025, "url": "http://arxiv.org/abs/2509.08970v2", "abstract": "Natural language descriptions of optimization or satisfaction problems are challenging to translate into correct MiniZinc models, as this process demands both logical reasoning and constraint programming expertise. We introduce Gala, a framework that addresses this challenge with a global agentic approach: multiple specialized large language model (LLM) agents decompose the modeling task by global constraint type. Each agent is dedicated to detecting and generating code for a specific class of global constraint, while a final assembler agent integrates these constraint snippets into a complete MiniZinc model. By dividing the problem into smaller, well-defined sub-tasks, each LLM handles a simpler reasoning challenge, potentially reducing overall complexity. We conduct initial experiments with several LLMs and show better performance against baselines such as one-shot prompting and chain-of-thought prompting. Finally, we outline a comprehensive roadmap for future work, highlighting potential enhancements and directions for improvement.", "source": "arxiv", "arxiv_id": "2509.08970v2", "pdf_url": "https://arxiv.org/pdf/2509.08970v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-10T20:04:20Z", "updated": "2025-10-02T19:55:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents", "authors": ["Chongyu Bao", "Ruimin Dai", "Yangbo Shen", "Runyang Jian", "Jinghan Zhang", "Xiaolan Liu", "Kunpeng Liu"], "year": 2025, "url": "http://arxiv.org/abs/2508.03991v1", "abstract": "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are designed to enhance human capabilities and perform tasks on behalf of users. The emergence of LLM agents brings new opportunities for the development of IPAs. While responsive capabilities have been widely studied, proactive behaviors remain underexplored. Designing an IPA that is proactive, privacy-preserving, and capable of self-evolution remains a significant challenge. Designing such IPAs relies on the cognitive architecture of LLM agents. This work proposes Cognition Forest, a semantic structure designed to align cognitive modeling with system-level design. We unify cognitive architecture and system design into a self-reinforcing loop instead of treating them separately. Based on this principle, we present Galaxy, a framework that supports multidimensional interactions and personalized capability generation. Two cooperative agents are implemented based on Galaxy: KoRa, a cognition-enhanced generative agent that supports both responsive and proactive skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's self-evolution and privacy preservation. Experimental results show that Galaxy outperforms multiple state-of-the-art benchmarks. Ablation studies and real-world interaction cases validate the effectiveness of Galaxy.", "source": "arxiv", "arxiv_id": "2508.03991v1", "pdf_url": "https://arxiv.org/pdf/2508.03991v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-06T00:46:38Z", "updated": "2025-08-06T00:46:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics", "authors": ["Arsham Gholamzadeh Khoee", "Shuai Wang", "Yinan Yu", "Robert Feldt", "Dhasarathy Parthasarathy"], "year": 2025, "url": "http://arxiv.org/abs/2503.21735v2", "abstract": "Ensuring reliable software release decisions is critical in safety-critical domains such as automotive manufacturing. Release validation relies on large tabular datasets, yet manual analysis is slow, costly, and error-prone. While Large Language Models (LLMs) offer promising automation potential, they face challenges in analytical reasoning, structured data handling, and ambiguity resolution. This paper introduces GateLens, an LLM-based system for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and generates optimized Python code. Unlike traditional multi-agent or planning-based systems that can be slow, opaque, and costly to maintain, GateLens emphasizes speed, transparency, and reliability. Experimental results show that GateLens outperforms the existing Chain-of-Thought (CoT) + Self-Consistency (SC) based system on real-world datasets, particularly in handling complex and ambiguous queries. Ablation studies confirm the essential role of the RA layer. Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. GateLens operates effectively in zero-shot settings without requiring few-shot examples or agent orchestration. This work advances deployable LLM system design by identifying key architectural features-intermediate formal representations, execution efficiency, and low configuration overhead-crucial for safety-critical industrial applications.", "source": "arxiv", "arxiv_id": "2503.21735v2", "pdf_url": "https://arxiv.org/pdf/2503.21735v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-27T17:48:32Z", "updated": "2025-08-01T21:33:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators", "authors": ["Jiacheng Guo", "Ling Yang", "Peter Chen", "Qixin Xiao", "Yinjie Wang", "Xinzhe Juan", "Jiahao Qiu", "Ke Shen", "Mengdi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.19682v2", "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $Î±$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.", "source": "arxiv", "arxiv_id": "2512.19682v2", "pdf_url": "https://arxiv.org/pdf/2512.19682v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-22T18:57:13Z", "updated": "2025-12-23T03:45:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2507.11633v1", "abstract": "We introduce a modular harness design for LLM agents that composes of perception, memory, and reasoning components, enabling a single LLM or VLM backbone to tackle a wide spectrum of multi turn gaming environments without domain-specific engineering. Using classic and modern game suites as low-barrier, high-diversity testbeds, our framework provides a unified workflow for analyzing how each module affects performance across dynamic interactive settings. Extensive experiments demonstrate that the harness lifts gameplay performance consistently over un-harnessed baselines and reveals distinct contribution patterns, for example, memory dominates in long-horizon puzzles while perception is critical in vision noisy arcades. These findings highlight the effectiveness of our modular harness design in advancing general-purpose agent, given the familiarity and ubiquity of games in everyday human experience.", "source": "arxiv", "arxiv_id": "2507.11633v1", "pdf_url": "https://arxiv.org/pdf/2507.11633v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-15T18:13:04Z", "updated": "2025-07-15T18:13:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent", "authors": ["Xingzuo Li", "Kehai Chen", "Yunfei Long", "Xuefeng Bai", "Yong Xu", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.02519v4", "abstract": "Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.", "source": "arxiv", "arxiv_id": "2503.02519v4", "pdf_url": "https://arxiv.org/pdf/2503.02519v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-04T11:31:05Z", "updated": "2025-09-26T02:48:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects", "authors": ["Yixin Liu", "Guibin Zhang", "Kun Wang", "Shiyuan Li", "Shirui Pan"], "year": 2025, "url": "http://arxiv.org/abs/2507.21407v2", "abstract": "Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.", "source": "arxiv", "arxiv_id": "2507.21407v2", "pdf_url": "https://arxiv.org/pdf/2507.21407v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-29T00:27:12Z", "updated": "2025-08-30T06:01:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Graph-Enhanced Policy Optimization in LLM Agent Training", "authors": ["Jiazhen Yuan", "Wei Zhao", "Zhengbiao Bai"], "year": 2025, "url": "http://arxiv.org/abs/2510.26270v1", "abstract": "Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.", "source": "arxiv", "arxiv_id": "2510.26270v1", "pdf_url": "https://arxiv.org/pdf/2510.26270v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T08:53:41Z", "updated": "2025-10-30T08:53:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GraphCodeAgent: Dual Graph-Guided LLM Agent for Retrieval-Augmented Repo-Level Code Generation", "authors": ["Jia Li", "Xianjie Shi", "Kechi Zhang", "Ge Li", "Zhi Jin", "Lei Li", "Huangzhao Zhang", "Jia Li", "Fang Liu", "Yuwei Zhang", "Zhengwei Tao", "Yihong Dong", "Yuqi Zhu", "Chongyang Tao"], "year": 2025, "url": "http://arxiv.org/abs/2504.10046v2", "abstract": "Writing code requires significant time and effort in software development. To automate this process, researchers have made substantial progress for code generation. Recently, large language models (LLMs) have demonstrated remarkable proficiency in function-level code generation, yet their performance significantly degrades in the real-world software development process, where coding tasks are deeply embedded within specific repository contexts. Existing studies attempt to use retrieval-augmented code generation (RACG) approaches to mitigate this demand. However, there is a gap between natural language (NL) requirements and programming implementations. This results in the failure to retrieve the relevant code of these fine-grained subtasks. To address this challenge, we propose GraphCodeAgent, a dual graph-guided LLM agent for retrieval-augmented repo-level code generation, bridging the gap between NL requirements and programming implementations. Our approach constructs two interconnected graphs: a Requirement Graph (RG) to model requirement relations of code snippets within the repository, as well as the relations between the target requirement and the requirements of these code snippets, and a Structural-Semantic Code Graph (SSCG) to capture the repository's intricate code dependencies. Guided by this, an LLM-powered agent performs multi-hop reasoning to systematically retrieve all context code snippets, including implicit and explicit code snippets, even if they are not explicitly expressed in requirements. We evaluated GraphCodeAgent on three advanced LLMs with the two widely-used repo-level code generation benchmarks DevEval and CoderEval. Extensive experiment results show that GraphCodeAgent significantly outperforms state-of-the-art baselines.", "source": "arxiv", "arxiv_id": "2504.10046v2", "pdf_url": "https://arxiv.org/pdf/2504.10046v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-04-14T09:51:23Z", "updated": "2025-11-18T15:29:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments", "authors": ["Enjun Du", "Xunkai Li", "Tian Jin", "Zhihan Zhang", "Rong-Hua Li", "Guoren Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.00711v2", "abstract": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.", "source": "arxiv", "arxiv_id": "2504.00711v2", "pdf_url": "https://arxiv.org/pdf/2504.00711v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-01T12:21:50Z", "updated": "2025-05-05T13:57:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents", "authors": ["Dayeon Ki", "Tianyi Zhou", "Marine Carpuat", "Gang Wu", "Puneet Mathur", "Viswanathan Swaminathan"], "year": 2025, "url": "http://arxiv.org/abs/2504.11571v1", "abstract": "Large Language Model (LLM)-powered agents have unlocked new possibilities for automating human tasks. While prior work has focused on well-defined tasks with specified goals, the capabilities of agents in creative design tasks with open-ended goals remain underexplored. We introduce GraphicBench, a new planning benchmark for graphic design that covers 1,079 user queries and input images across four design types. We further present GraphicTown, an LLM agent framework with three design experts and 46 actions (tools) to choose from for executing each step of the planned workflows in web environments. Experiments with six LLMs demonstrate their ability to generate workflows that integrate both explicit design constraints from user queries and implicit commonsense constraints. However, these workflows often do not lead to successful execution outcomes, primarily due to challenges in: (1) reasoning about spatial relationships, (2) coordinating global dependencies across experts, and (3) retrieving the most appropriate action per step. We envision GraphicBench as a challenging yet valuable testbed for advancing LLM-agent planning and execution in creative design tasks.", "source": "arxiv", "arxiv_id": "2504.11571v1", "pdf_url": "https://arxiv.org/pdf/2504.11571v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-15T19:26:59Z", "updated": "2025-04-15T19:26:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Grounded Test-Time Adaptation for LLM Agents", "authors": ["Arthur Chen", "Zuxin Liu", "Jianguo Zhang", "Akshara Prabhakar", "Zhiwei Liu", "Shelby Heinecke", "Silvio Savarese", "Victor Zhong", "Caiming Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2511.04847v3", "abstract": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.", "source": "arxiv", "arxiv_id": "2511.04847v3", "pdf_url": "https://arxiv.org/pdf/2511.04847v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-06T22:24:35Z", "updated": "2026-01-05T17:43:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Group-in-Group Policy Optimization for LLM Agent Training", "authors": ["Lang Feng", "Zhenghai Xue", "Tingcong Liu", "Bo An"], "year": 2025, "url": "http://arxiv.org/abs/2505.10978v3", "abstract": "Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to multi-turn LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals, achieves performance gains of > 12% on ALFWorld and > 9% on WebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B and 47.2% on 7B): all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.", "source": "arxiv", "arxiv_id": "2505.10978v3", "pdf_url": "https://arxiv.org/pdf/2505.10978v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-16T08:26:59Z", "updated": "2025-10-28T15:11:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees", "authors": ["Katsuaki Nakano", "Reza Fayyazi", "Shanchieh Jay Yang", "Michael Zuzak"], "year": 2025, "url": "http://arxiv.org/abs/2509.07939v2", "abstract": "Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the LLM's reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and 78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments", "source": "arxiv", "arxiv_id": "2509.07939v2", "pdf_url": "https://arxiv.org/pdf/2509.07939v2", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-09T17:19:33Z", "updated": "2025-11-18T18:20:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models", "authors": ["Zhaoxing Li", "Wenbo Wu", "Yue Wang", "Yanran Xu", "William Hunt", "Sebastian Stein"], "year": 2025, "url": "http://arxiv.org/abs/2505.00820v1", "abstract": "Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention.", "source": "arxiv", "arxiv_id": "2505.00820v1", "pdf_url": "https://arxiv.org/pdf/2505.00820v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-01T19:23:50Z", "updated": "2025-05-01T19:23:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting", "authors": ["Daniil Sukhorukov", "Andrei Zakharov", "Nikita Glazkov", "Katsiaryna Yanchanka", "Vladimir Kirilin", "Maxim Dubovitsky", "Roman Sultimov", "Yuri Maksimov", "Ilya Makarov"], "year": 2025, "url": "http://arxiv.org/abs/2511.23387v1", "abstract": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.", "source": "arxiv", "arxiv_id": "2511.23387v1", "pdf_url": "https://arxiv.org/pdf/2511.23387v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-28T17:27:06Z", "updated": "2025-11-28T17:27:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2507.22925v1", "abstract": "Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.", "source": "arxiv", "arxiv_id": "2507.22925v1", "pdf_url": "https://arxiv.org/pdf/2507.22925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-23T12:45:44Z", "updated": "2025-07-23T12:45:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior", "authors": ["Zidi Xiong", "Yuping Lin", "Wenya Xie", "Pengfei He", "Zirui Liu", "Jiliang Tang", "Himabindu Lakkaraju", "Zhen Xiang"], "year": 2025, "url": "http://arxiv.org/abs/2505.16067v2", "abstract": "Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory management operations that are widely used by many agent frameworks-memory addition and deletion-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where some seemingly correct executions can provide limited or even misleading value as experiences. Through controlled experiments, we demonstrate the importance of regulating experience quality within the memory bank and show that future task evaluations can serve as free quality labels for stored memory. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance.", "source": "arxiv", "arxiv_id": "2505.16067v2", "pdf_url": "https://arxiv.org/pdf/2505.16067v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-21T22:35:01Z", "updated": "2025-10-10T20:27:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "How to evaluate control measures for LLM agents? A trajectory from today to superintelligence", "authors": ["Tomek Korbak", "Mikita Balesni", "Buck Shlegeris", "Geoffrey Irving"], "year": 2025, "url": "http://arxiv.org/abs/2504.05259v1", "abstract": "As LLM agents grow more capable of causing harm autonomously, AI developers will rely on increasingly sophisticated control measures to prevent possibly misaligned agents from causing harm. AI developers could demonstrate that their control measures are sufficient by running control evaluations: testing exercises in which a red team produces agents that try to subvert control measures. To ensure control evaluations accurately capture misalignment risks, the affordances granted to this red team should be adapted to the capability profiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of red teams to advancing AI capabilities. Rather than assuming that agents will always execute the best attack strategies known to humans, we demonstrate how knowledge of an agents's actual capability profile can inform proportional control evaluations, resulting in more practical and cost-effective control measures. We illustrate our framework by considering a sequence of five fictional models (M1-M5) with progressively advanced capabilities, defining five distinct AI control levels (ACLs). For each ACL, we provide example rules for control evaluation, control measures, and safety cases that could be appropriate. Finally, we show why constructing a compelling AI control safety case for superintelligent LLM agents will require research breakthroughs, highlighting that we might eventually need alternative approaches to mitigating misalignment risk.", "source": "arxiv", "arxiv_id": "2504.05259v1", "pdf_url": "https://arxiv.org/pdf/2504.05259v1", "categories": ["cs.AI", "cs.CR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-07T16:52:52Z", "updated": "2025-04-07T16:52:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Human vs. Agent in Task-Oriented Conversations", "authors": ["Zhefan Wang", "Ning Geng", "Zhiqiang Guo", "Weizhi Ma", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2509.17619v2", "abstract": "Task-oriented conversational systems are essential for efficiently addressing diverse user needs, yet their development requires substantial amounts of high-quality conversational data that is challenging and costly to obtain. While large language models (LLMs) have demonstrated potential in generating synthetic conversations, the extent to which these agent-generated interactions can effectively substitute real human conversations remains unclear. This work presents the first systematic comparison between LLM-simulated users and human users in personalized task-oriented conversations. We propose a comprehensive analytical framework encompassing three key aspects (conversation strategy, interaction style, and conversation evaluation) and ten distinct dimensions for evaluating user behaviors, and collect parallel conversational datasets from both human users and LLM agent users across four representative scenarios under identical conditions. Our analysis reveals significant behavioral differences between the two user types in problem-solving approaches, question broadness, user engagement, context dependency, feedback polarity and promise, language style, and hallucination awareness. We found consistency in the agent users and human users across the depth-first or breadth-first dimensions, as well as the usefulness dimensions. These findings provide critical insights for advancing LLM-based user simulation. Our multi-dimensional taxonomy constructed a generalizable framework for analyzing user behavior patterns, offering insights from LLM agent users and human users. By this work, we provide perspectives on rethinking how to use user simulation in conversational systems in the future.", "source": "arxiv", "arxiv_id": "2509.17619v2", "pdf_url": "https://arxiv.org/pdf/2509.17619v2", "categories": ["cs.IR"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-09-22T11:30:39Z", "updated": "2025-11-05T03:47:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks", "authors": ["Jinbo Wen", "Cheng Su", "Jiawen Kang", "Jiangtian Nie", "Yang Zhang", "Jianhang Tang", "Dusit Niyato", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2506.15947v1", "abstract": "Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm to support various low-altitude services through integrated air-ground infrastructure. To satisfy low-latency and high-computation demands, the integration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC) systems plays a vital role, which offloads computing tasks from terminal devices to nearby UAVs, enabling flexible and resilient service provisions for ground users. To promote the development of LAENets, it is significant to achieve low-carbon multi-UAV-assisted MEC networks. However, several challenges hinder this implementation, including the complexity of multi-dimensional UAV modeling and the difficulty of multi-objective coupled optimization. To this end, this paper proposes a novel Retrieval Augmented Generation (RAG)-based Large Language Model (LLM) agent framework for model formulation. Specifically, we develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG, empowering LLM agents to efficiently retrieve structural information from expert databases and generate more accurate optimization problems compared with traditional RAG-based LLM agents. After customizing carbon emission optimization problems for multi-UAV-assisted MEC networks, we propose a Double Regularization Diffusion-enhanced Soft Actor-Critic (R\\textsuperscript{2}DSAC) algorithm to solve the formulated multi-objective optimization problem. The R\\textsuperscript{2}DSAC algorithm incorporates diffusion entropy regularization and action entropy regularization to improve the performance of the diffusion policy. Furthermore, we dynamically mask unimportant neurons in the actor network to reduce the carbon emissions associated with model training. Simulation results demonstrate the effectiveness and reliability of the proposed HybridRAG-based LLM agent framework and the R\\textsuperscript{2}DSAC algorithm.", "source": "arxiv", "arxiv_id": "2506.15947v1", "pdf_url": "https://arxiv.org/pdf/2506.15947v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-06-19T01:11:35Z", "updated": "2025-06-19T01:11:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Experts", "authors": ["Eric Xue", "Ke Chen", "Zeyi Huang", "Yuyang Ji", "Haohan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.18530v3", "abstract": "Large language model (LLM) agents have emerged as a promising solution to automate the workflow of machine learning, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves overall model performance. We also provide some theoretical edvience of the superior properties of this Iterative Refinement. Further, we implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches.", "source": "arxiv", "arxiv_id": "2502.18530v3", "pdf_url": "https://arxiv.org/pdf/2502.18530v3", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-02-25T01:52:37Z", "updated": "2025-09-16T05:43:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "INTA: Intent-Based Translation for Network Configuration with LLM Agents", "authors": ["Yunze Wei", "Xiaohui Xie", "Tianshuo Hu", "Yiwei Zuo", "Xinyi Chen", "Kaiwen Chi", "Yong Cui"], "year": 2025, "url": "http://arxiv.org/abs/2501.08760v2", "abstract": "Translating configurations between different network devices is a common yet challenging task in modern network operations. This challenge arises in typical scenarios such as replacing obsolete hardware and adapting configurations to emerging paradigms like Software Defined Networking (SDN) and Network Function Virtualization (NFV). Engineers need to thoroughly understand both source and target configuration models, which requires considerable effort due to the complexity and evolving nature of these specifications. To promote automation in network configuration translation, we propose INTA, an intent-based translation framework that leverages Large Language Model (LLM) agents. The key idea of INTA is to use configuration intent as an intermediate representation for translation. It first employs LLMs to decompose configuration files and extract fine-grained intents for each configuration fragment. These intents are then used to retrieve relevant manuals of the target device. Guided by a syntax checker, INTA incrementally generates target configurations. The translated configurations are further verified and refined for semantic consistency. We implement INTA and evaluate it on real-world configuration datasets from the industry. Our approach outperforms state-of-the-art methods in translation accuracy and exhibits strong generalizability. INTA achieves an accuracy of 98.15% in terms of both syntactic and view correctness, and a command recall rate of 84.72% for the target configuration. The semantic consistency report of the translated configuration further demonstrates its practical value in real-world network operations.", "source": "arxiv", "arxiv_id": "2501.08760v2", "pdf_url": "https://arxiv.org/pdf/2501.08760v2", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-01-15T12:25:56Z", "updated": "2025-09-20T13:31:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents", "authors": ["Hengyu An", "Jinghuai Zhang", "Tianyu Du", "Chunyi Zhou", "Qingming Li", "Tao Lin", "Shouling Ji"], "year": 2025, "url": "http://arxiv.org/abs/2508.15310v1", "abstract": "Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.", "source": "arxiv", "arxiv_id": "2508.15310v1", "pdf_url": "https://arxiv.org/pdf/2508.15310v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-21T07:08:16Z", "updated": "2025-08-21T07:08:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases", "authors": ["Ziqian Zhong", "Aditi Raghunathan", "Nicholas Carlini"], "year": 2025, "url": "http://arxiv.org/abs/2510.20270v1", "abstract": "The tendency to find and exploit \"shortcuts\" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents' propensity to exploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent's \"cheating rate\" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems.\n  Our implementation can be found at https://github.com/safety-research/impossiblebench.", "source": "arxiv", "arxiv_id": "2510.20270v1", "pdf_url": "https://arxiv.org/pdf/2510.20270v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-23T06:58:32Z", "updated": "2025-10-23T06:58:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning", "authors": ["Zhoujian Sun", "Ziyi Liu", "Cheng Luo", "Jiebin Chu", "Zhengxing Huang"], "year": 2025, "url": "http://arxiv.org/abs/2503.16463v1", "abstract": "Recent advances in large language models (LLMs) have shown promising results in medical diagnosis, with some studies indicating superior performance compared to human physicians in specific scenarios. However, the diagnostic capabilities of LLMs are often overestimated, as their performance significantly deteriorates in interactive diagnostic settings that require active information gathering. This study investigates the underlying mechanisms behind the performance degradation phenomenon and proposes a solution. We identified that the primary deficiency of LLMs lies in the initial diagnosis phase, particularly in information-gathering efficiency and initial diagnosis formation, rather than in the subsequent differential diagnosis phase. To address this limitation, we developed a plug-and-play method enhanced (PPME) LLM agent, leveraging over 3.5 million electronic medical records from Chinese and American healthcare facilities. Our approach integrates specialized models for initial disease diagnosis and inquiry into the history of the present illness, trained through supervised and reinforcement learning techniques. The experimental results indicate that the PPME LLM achieved over 30% improvement compared to baselines. The final diagnostic accuracy of the PPME LLM in interactive diagnostic scenarios approached levels comparable to those achieved using complete clinical data. These findings suggest a promising potential for developing autonomous diagnostic systems, although further validation studies are needed.", "source": "arxiv", "arxiv_id": "2503.16463v1", "pdf_url": "https://arxiv.org/pdf/2503.16463v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-24T06:24:20Z", "updated": "2025-02-24T06:24:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "year": 2025, "url": "http://arxiv.org/abs/2506.09171v1", "abstract": "Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts'' from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.", "source": "arxiv", "arxiv_id": "2506.09171v1", "pdf_url": "https://arxiv.org/pdf/2506.09171v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-10T18:36:31Z", "updated": "2025-06-10T18:36:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges", "authors": ["Lajos Muzsai", "David Imolai", "AndrÃ¡s LukÃ¡cs"], "year": 2025, "url": "http://arxiv.org/abs/2506.02048v2", "abstract": "We present 'Random-Crypto', a procedurally generated cryptographic Capture The Flag (CTF) dataset designed to unlock the potential of Reinforcement Learning (RL) for LLM-based agents in security-sensitive domains. Cryptographic reasoning offers an ideal RL testbed: it combines precise validation, structured multi-step inference, and reliance on reliable computational tool use. Leveraging these properties, we fine-tune a Python tool-augmented Llama-3.1-8B via Group Relative Policy Optimization (GRPO) in a secure execution environment. The resulting agent achieves a significant improvement in Pass@8 on previously unseen challenges. Moreover, the improvements generalize to two external benchmarks: 'picoCTF', spanning both crypto and non-crypto tasks, and 'AICrypto MCQ', a multiple-choice benchmark of 135 cryptography questions. Ablation studies attribute the gains to enhanced tool usage and procedural reasoning. These findings position 'Random-Crypto' as a rich training ground for building intelligent, adaptable LLM agents capable of handling complex cybersecurity tasks.", "source": "arxiv", "arxiv_id": "2506.02048v2", "pdf_url": "https://arxiv.org/pdf/2506.02048v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-01T01:59:52Z", "updated": "2025-08-17T22:28:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2509.23586v1", "abstract": "Multi-turn agent systems based on Large Language Models (LLMs) have been increasingly popular for software engineering tasks. While LLM agents show decent effectiveness, the high computational cost of input tokens due to the ever-growing trajectory remains an efficiency concern for their applications. Efficiency is largely neglected in existing studies and agent products, and this paper fills the gap by introducing an inference-time trajectory reduction approach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless, redundant, and expired information is widespread in all trajectories, which can be identified and reduced without harming the agent's performance. We then design a simple yet effective trajectory reduction approach, AgentDiet, which automatically removes such waste information. We implement AgentDiet on a top-performing coding agent, and the evaluation on two LLMs and two benchmarks shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final computational cost by 21.1% ~ 35.9%, while maintaining the same agent performance. This indicates that trajectory reduction is a promising direction for agent systems.", "source": "arxiv", "arxiv_id": "2509.23586v1", "pdf_url": "https://arxiv.org/pdf/2509.23586v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-28T02:43:41Z", "updated": "2025-09-28T02:43:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs", "authors": ["Vishnu Sarukkai", "Asanshay Gupta", "James Hong", "MichaÃ«l Gharbi", "Kayvon Fatahalian"], "year": 2025, "url": "http://arxiv.org/abs/2512.02543v1", "abstract": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.", "source": "arxiv", "arxiv_id": "2512.02543v1", "pdf_url": "https://arxiv.org/pdf/2512.02543v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-02T09:11:05Z", "updated": "2025-12-02T09:11:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment", "authors": ["Dekun Wu", "Frederik Brudy", "Bang Liu", "Yi Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.12331v1", "abstract": "Virtual environments are essential to AI agent research. Existing environments for LLM agent research typically focus on either physical task solving or social simulation, with the former oversimplifying agent individuality and social dynamics, and the latter lacking physical grounding of social behaviors. We introduce IndoorWorld, a heterogeneous multi-agent environment that tightly integrates physical and social dynamics. By introducing novel challenges for LLM-driven agents in orchestrating social dynamics to influence physical environments and anchoring social interactions within world states, IndoorWorld opens up possibilities of LLM-based building occupant simulation for architectural design. We demonstrate the potential with a series of experiments within an office setting to examine the impact of multi-agent collaboration, resource competition, and spatial layout on agent behavior.", "source": "arxiv", "arxiv_id": "2506.12331v1", "pdf_url": "https://arxiv.org/pdf/2506.12331v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-14T03:44:09Z", "updated": "2025-06-14T03:44:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Inducing State Anxiety in LLM Agents Reproduces Human-Like Biases in Consumer Decision-Making", "authors": ["Ziv Ben-Zion", "Zohar Elyoseph", "Tobias Spiller", "Teddy Lazebnik"], "year": 2025, "url": "http://arxiv.org/abs/2510.06222v1", "abstract": "Large language models (LLMs) are rapidly evolving from text generators to autonomous agents, raising urgent questions about their reliability in real-world contexts. Stress and anxiety are well known to bias human decision-making, particularly in consumer choices. Here, we tested whether LLM agents exhibit analogous vulnerabilities. Three advanced models (ChatGPT-5, Gemini 2.5, Claude 3.5-Sonnet) performed a grocery shopping task under budget constraints (24, 54, 108 USD), before and after exposure to anxiety-inducing traumatic narratives. Across 2,250 runs, traumatic prompts consistently reduced the nutritional quality of shopping baskets (Change in Basket Health Scores of -0.081 to -0.126; all pFDR<0.001; Cohens d=-1.07 to -2.05), robust across models and budgets. These results show that psychological context can systematically alter not only what LLMs generate but also the actions they perform. By reproducing human-like emotional biases in consumer behavior, LLM agents reveal a new class of vulnerabilities with implications for digital health, consumer safety, and ethical AI deployment.", "source": "arxiv", "arxiv_id": "2510.06222v1", "pdf_url": "https://arxiv.org/pdf/2510.06222v1", "categories": ["cs.HC", "econ.GN"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-08-30T10:22:39Z", "updated": "2025-08-30T10:22:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "authors": ["EunJeong Hwang", "Yuwei Yin", "Giuseppe Carenini", "Peter West", "Vered Shwartz"], "year": 2025, "url": "http://arxiv.org/abs/2509.22887v1", "abstract": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2509.22887v1", "pdf_url": "https://arxiv.org/pdf/2509.22887v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-26T20:07:34Z", "updated": "2025-09-26T20:07:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning", "authors": ["Ehsan Ahmadi", "Chao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.15263v1", "abstract": "Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries.", "source": "arxiv", "arxiv_id": "2504.15263v1", "pdf_url": "https://arxiv.org/pdf/2504.15263v1", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-04-21T17:45:21Z", "updated": "2025-04-21T17:45:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Interpretable Risk Mitigation in LLM Agent Systems", "authors": ["Jan Chojnacki"], "year": 2025, "url": "http://arxiv.org/abs/2505.10670v1", "abstract": "Autonomous agents powered by large language models (LLMs) enable novel use cases in domains where responsible action is increasingly important. Yet the inherent unpredictability of LLMs raises safety concerns about agent reliability. In this work, we explore agent behaviour in a toy, game-theoretic environment based on a variation of the Iterated Prisoner's Dilemma. We introduce a strategy-modification method-independent of both the game and the prompt-by steering the residual stream with interpretable features extracted from a sparse autoencoder latent space. Steering with the good-faith negotiation feature lowers the average defection probability by 28 percentage points. We also identify feasible steering ranges for several open-source LLM agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents, combined with representation-steering alignment, can generalise to real-world applications on end-user devices and embodied platforms.", "source": "arxiv", "arxiv_id": "2505.10670v1", "pdf_url": "https://arxiv.org/pdf/2505.10670v1", "categories": ["cs.AI", "cs.CY", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-15T19:22:11Z", "updated": "2025-05-15T19:22:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Investigating Prosocial Behavior Theory in LLM Agents under Policy-Induced Inequities", "authors": ["Yujia Zhou", "Hexi Wang", "Qingyao Ai", "Zhen Wu", "Yiqun Liu"], "year": 2025, "url": "http://arxiv.org/abs/2505.15857v2", "abstract": "As large language models (LLMs) increasingly operate as autonomous agents in social contexts, evaluating their capacity for prosocial behavior is both theoretically and practically critical. However, existing research has primarily relied on static, economically framed paradigms, lacking models that capture the dynamic evolution of prosociality and its sensitivity to structural inequities. To address these gaps, we introduce ProSim, a simulation framework for modeling the prosocial behavior in LLM agents across diverse social conditions. We conduct three progressive studies to assess prosocial alignment. First, we demonstrate that LLM agents can exhibit human-like prosocial behavior across a broad range of real-world scenarios and adapt to normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate norm erosion through social networks. These findings advance prosocial behavior theory by elucidating how institutional dynamics shape the emergence, decay, and diffusion of prosocial norms in agent-driven societies.", "source": "arxiv", "arxiv_id": "2505.15857v2", "pdf_url": "https://arxiv.org/pdf/2505.15857v2", "categories": ["cs.SI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-05-21T01:09:37Z", "updated": "2025-11-10T01:08:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "year": 2025, "url": "http://arxiv.org/abs/2507.11049v3", "abstract": "As online news consumption grows, personalized recommendation systems have become integral to digital journalism. However, these systems risk reinforcing filter bubbles and political polarization by failing to incorporate diverse perspectives. Stance detection -- identifying a text's position on a target -- can help mitigate this by enabling viewpoint-aware recommendations and data-driven analyses of media bias. Yet, existing stance detection research remains largely limited to short texts and high-resource languages. To address these gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for article-level stance detection, comprising 2,000 news articles with article-level and 21,650 segment-level stance annotations across 47 societal issues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided \\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that employs a language model agent to predict the stances of key structural segments (e.g., leads, quotations), which are then aggregated to infer the overall article stance. Experiments showed that \\textsc{JoA-ICL} outperforms existing stance detection methods, highlighting the benefits of segment-level agency in capturing the overall position of long-form news articles. Two case studies further demonstrate its broader utility in promoting viewpoint diversity in news recommendations and uncovering patterns of media bias.", "source": "arxiv", "arxiv_id": "2507.11049v3", "pdf_url": "https://arxiv.org/pdf/2507.11049v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-15T07:22:04Z", "updated": "2025-09-21T05:47:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "year": 2025, "url": "http://arxiv.org/abs/2510.04373v1", "abstract": "Large language model (LLM) agents perform well in sequential decision-making tasks, but improving them on unfamiliar domains often requires costly online interactions or fine-tuning on large expert datasets. These strategies are impractical for closed-source models and expensive for open-source ones, with risks of catastrophic forgetting. Offline trajectories offer reusable knowledge, yet demonstration-based methods struggle because raw traces are long, noisy, and tied to specific tasks. We present Just-in-time Episodic Feedback Hinter (JEF Hinter), an agentic system that distills offline traces into compact, context-aware hints. A zooming mechanism highlights decisive steps in long trajectories, capturing both strategies and pitfalls. Unlike prior methods, JEF Hinter leverages both successful and failed trajectories, extracting guidance even when only failure data is available, while supporting parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects relevant hints for the current state, providing targeted guidance with transparency and traceability. Experiments on MiniWoB++, WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms strong baselines, including human- and document-based hints.", "source": "arxiv", "arxiv_id": "2510.04373v1", "pdf_url": "https://arxiv.org/pdf/2510.04373v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-05T21:34:42Z", "updated": "2025-10-05T21:34:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?", "authors": ["Qian Zhang", "Yan Zheng", "Jinyi Liu", "Hebin Liang", "Lanjun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.11040v1", "abstract": "Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, \"Truth Last\", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.", "source": "arxiv", "arxiv_id": "2511.11040v1", "pdf_url": "https://arxiv.org/pdf/2511.11040v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-14T07:47:56Z", "updated": "2025-11-14T07:47:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining", "authors": ["Qian'ang Mao", "Yuxuan Zhang", "Jiaman Chen", "Wenjun Zhou", "Jiaqi Yan"], "year": 2025, "url": "http://arxiv.org/abs/2511.15456v1", "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.", "source": "arxiv", "arxiv_id": "2511.15456v1", "pdf_url": "https://arxiv.org/pdf/2511.15456v1", "categories": ["cs.AI", "q-fin.GN"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T14:15:23Z", "updated": "2025-11-19T14:15:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making", "authors": ["Qingyuan Zhang", "Yuxi Wang", "Cancan Hua", "Yulin Huang", "Ning Lyu"], "year": 2025, "url": "http://arxiv.org/abs/2512.09440v1", "abstract": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.", "source": "arxiv", "arxiv_id": "2512.09440v1", "pdf_url": "https://arxiv.org/pdf/2512.09440v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-10T09:08:33Z", "updated": "2025-12-10T09:08:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents", "authors": ["Henrik Bradland", "Morten Goodwin", "Vladimir I. Zadorozhny", "Per-Arne Andersen"], "year": 2025, "url": "http://arxiv.org/abs/2511.15074v1", "abstract": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.", "source": "arxiv", "arxiv_id": "2511.15074v1", "pdf_url": "https://arxiv.org/pdf/2511.15074v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-19T03:27:14Z", "updated": "2025-11-19T03:27:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LA-RCS: LLM-Agent-Based Robot Control System", "authors": ["TaekHyun Park", "YoungJun Choi", "SeungHoon Shin", "Kwangil Lee"], "year": 2025, "url": "http://arxiv.org/abs/2505.18214v1", "abstract": "LA-RCS (LLM-agent-based robot control system) is a sophisticated robot control system designed to autonomously plan, work, and analyze the external environment based on user requirements by utilizing LLM-Agent. Utilizing a dual-agent framework, LA-RCS generates plans based on user requests, observes the external environment, executes the plans, and modifies the plans as needed to adapt to changes in the external conditions. Additionally, LA-RCS interprets natural language commands by the user and converts them into commands compatible with the robot interface so that the robot can execute tasks and meet user requests properly. During his process, the system autonomously evaluates observation results, provides feedback on the tasks, and executes commands based on real-time environmental monitoring, significantly reducing the need for user intervention in fulfilling requests. We categorized the scenarios that LA-RCS needs to perform into four distinct types and conducted a quantitative assessment of its performance in each scenario. The results showed an average success rate of 90 percent, demonstrating the system capability to fulfill user requests satisfactorily. For more extensive results, readers can visit our project page: https://la-rcs.github.io", "source": "arxiv", "arxiv_id": "2505.18214v1", "pdf_url": "https://arxiv.org/pdf/2505.18214v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "10.18494/SAM5643", "venue": "Senors and Materials 2025", "published": "2025-05-23T00:51:16Z", "updated": "2025-05-23T00:51:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.18576v1", "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.", "source": "arxiv", "arxiv_id": "2509.18576v1", "pdf_url": "https://arxiv.org/pdf/2509.18576v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-09-23T02:57:25Z", "updated": "2025-09-23T02:57:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary", "authors": ["Xin Li", "Mengbing Liu", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2510.13821v1", "abstract": "This position paper argues that the field of LLM agents requires a unified, telecom-inspired communication protocol to ensure safety, interoperability, and scalability, especially within the context of Next Generation (NextG) networks. Current ad-hoc communication methods are creating a fragmented ecosystem, reminiscent of the early \"protocol wars\" in networking, which stifles innovation and poses significant risks. Drawing inspiration from the layered, standardized protocols that underpin modern telecommunications, we propose the LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer architecture designed to ensure semantic clarity in communication, transactional integrity for complex tasks, and robust, built-in security. In this position paper, we argue that adopting a principled, universal protocol is not merely beneficial but essential for realizing the potential of distributed AI. Such a standard is critical for ensuring that multi-agent systems can operate safely and reliably in the complex, real-time applications envisioned for 6G and beyond.", "source": "arxiv", "arxiv_id": "2510.13821v1", "pdf_url": "https://arxiv.org/pdf/2510.13821v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-09-26T08:48:56Z", "updated": "2025-09-26T08:48:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?", "authors": ["Lu Sun", "Shihan Fu", "Bingsheng Yao", "Yuxuan Lu", "Wenbo Li", "Hansu Gu", "Jiri Gesi", "Jing Huang", "Chen Luo", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.21501v1", "abstract": "Agentic AI is emerging, capable of executing tasks through natural language, such as Copilot for coding or Amazon Rufus for shopping. Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation. Researchers have proposed LLM Agents to simulate participants as digital twins, but it remains unclear to what extent a digital twin can represent a specific customer in multi-turn interaction with an agentic AI system. In this paper, we recruited 40 human participants to shop with Amazon Rufus, collected their personas, interaction traces, and UX feedback, and then created digital twins to repeat the task. Pairwise comparison of human and digital-twin traces shows that while agents often explored more diverse choices, their action patterns aligned with humans and yielded similar design feedback. This study is the first to quantify how closely LLM agents can mirror human multi-turn interaction with an agentic AI system, highlighting their potential for scalable evaluation.", "source": "arxiv", "arxiv_id": "2509.21501v1", "pdf_url": "https://arxiv.org/pdf/2509.21501v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-25T19:58:02Z", "updated": "2025-09-25T19:58:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery", "authors": ["Kevin Song", "Andrew Trotter", "Jake Y. Chen"], "year": 2025, "url": "http://arxiv.org/abs/2504.17967v1", "abstract": "Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.", "source": "arxiv", "arxiv_id": "2504.17967v1", "pdf_url": "https://arxiv.org/pdf/2504.17967v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-24T22:27:50Z", "updated": "2025-04-24T22:27:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent for Hyper-Parameter Optimization", "authors": ["Wanzhe Wang", "Jianqiu Peng", "Menghao Hu", "Weihuang Zhong", "Tong Zhang", "Shuai Wang", "Yixin Zhang", "Mingjie Shao", "Wanli Ni"], "year": 2025, "url": "http://arxiv.org/abs/2506.15167v2", "abstract": "Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters optimization approaches for Warm-Start Particles Swarm Optimization with Crossover and Mutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication, are primarily heuristic-based, exhibiting low levels of automation and improvable performance. In this paper, we design an Large Language Model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and Model Context Protocol (MCP) are applied. In particular, the LLM agent is first set up via a profile, which specifies the boundary of hyper-parameters, task objective, terminal condition, conservative or aggressive strategy of optimizing hyper-parameters, and LLM configurations. Then, the LLM agent iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the loop based on the terminal condition and returns an optimized set of hyperparameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in seeking high-performance hyper-parameters.", "source": "arxiv", "arxiv_id": "2506.15167v2", "pdf_url": "https://arxiv.org/pdf/2506.15167v2", "categories": ["cs.IT", "cs.AI"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2025-06-18T06:28:22Z", "updated": "2025-07-09T13:20:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data", "authors": ["Wayupuk Sommuang", "Kun Kerdthaisong", "Pasin Buakhaw", "Aslan B. Wong", "Nutchanon Yongsatianchot"], "year": 2025, "url": "http://arxiv.org/abs/2508.02679v2", "abstract": "Students' mental well-being is vital for academic success, with activities such as studying, socializing, and sleeping playing a role. Current mobile sensing data highlight this intricate link using statistical and machine learning analyses. We propose a novel LLM agent-based simulation framework to model student activities and mental health using the StudentLife Dataset. Each LLM agent was initialized with personality questionnaires and guided by smartphone sensing data throughout the simulated semester. These agents predict individual behaviors, provide self-reported mental health data via ecological momentary assessments (EMAs), and complete follow-up personality questionnaires. To ensure accuracy, we investigated various prompting techniques, memory systems, and activity-based mental state management strategies that dynamically update an agent's mental state based on their daily activities. This simulation goes beyond simply replicating existing data. This allows us to explore new scenarios that are not present in the original dataset, such as peer influence through agent-to-agent interactions and the impact of social media. Furthermore, we can conduct intervention studies by manipulating activity patterns via sensing signals and personality traits using questionnaire responses. This provides valuable insights into the behavioral changes that could enhance student well-being. The framework also facilitates hypothetical interviews with LLM agents, offering deeper insights into their mental health. This study showcases the power of LLM-driven behavioral modeling with sensing data, opening new avenues for understanding and supporting student mental health.", "source": "arxiv", "arxiv_id": "2508.02679v2", "pdf_url": "https://arxiv.org/pdf/2508.02679v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-07-17T03:30:11Z", "updated": "2025-08-08T12:56:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents Are Hypersensitive to Nudges", "authors": ["Manuel Cherep", "Pattie Maes", "Nikhil Singh"], "year": 2025, "url": "http://arxiv.org/abs/2505.11584v1", "abstract": "LLMs are being set loose in complex, real-world environments involving sequential decision-making and tool use. Often, this involves making choices on behalf of human users. However, not much is known about the distribution of such choices, and how susceptible they are to different choice architectures. We perform a case study with a few such LLM models on a multi-attribute tabular decision-making problem, under canonical nudges such as the default option, suggestions, and information highlighting, as well as additional prompting strategies. We show that, despite superficial similarities to human choice distributions, such models differ in subtle but important ways. First, they show much higher susceptibility to the nudges. Second, they diverge in points earned, being affected by factors like the idiosyncrasy of available prizes. Third, they diverge in information acquisition strategies: e.g. incurring substantial cost to reveal too much information, or selecting without revealing any. Moreover, we show that simple prompt strategies like zero-shot chain of thought (CoT) can shift the choice distribution, and few-shot prompting with human data can induce greater alignment. Yet, none of these methods resolve the sensitivity of these models to nudges. Finally, we show how optimal nudges optimized with a human resource-rational model can similarly increase LLM performance for some models. All these findings suggest that behavioral tests are needed before deploying models as agents or assistants acting on behalf of users in complex environments.", "source": "arxiv", "arxiv_id": "2505.11584v1", "pdf_url": "https://arxiv.org/pdf/2505.11584v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-16T17:53:05Z", "updated": "2025-05-16T17:53:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents Beyond Utility: An Open-Ended Perspective", "authors": ["Asen Nachkov", "Xi Wang", "Luc Van Gool"], "year": 2025, "url": "http://arxiv.org/abs/2510.14548v1", "abstract": "Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.", "source": "arxiv", "arxiv_id": "2510.14548v1", "pdf_url": "https://arxiv.org/pdf/2510.14548v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-16T10:46:54Z", "updated": "2025-10-16T10:46:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators", "authors": ["Mateusz Lango", "OndÅej DuÅ¡ek"], "year": 2025, "url": "http://arxiv.org/abs/2512.18360v1", "abstract": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models", "source": "arxiv", "arxiv_id": "2512.18360v1", "pdf_url": "https://arxiv.org/pdf/2512.18360v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-industry.142", "venue": "", "published": "2025-12-20T13:16:51Z", "updated": "2025-12-20T13:16:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents Making Agent Tools", "authors": ["Georg WÃ¶lflein", "Dyke Ferber", "Daniel Truhn", "Ognjen ArandjeloviÄ", "Jakob Nikolas Kather"], "year": 2025, "url": "http://arxiv.org/abs/2502.11705v2", "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.", "source": "arxiv", "arxiv_id": "2502.11705v2", "pdf_url": "https://arxiv.org/pdf/2502.11705v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T11:44:11Z", "updated": "2025-05-29T18:47:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents Should Employ Security Principles", "authors": ["Kaiyuan Zhang", "Zian Su", "Pin-Yu Chen", "Elisa Bertino", "Xiangyu Zhang", "Ninghui Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.24019v1", "abstract": "Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.", "source": "arxiv", "arxiv_id": "2505.24019v1", "pdf_url": "https://arxiv.org/pdf/2505.24019v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-05-29T21:39:08Z", "updated": "2025-05-29T21:39:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents for Automated Dependency Upgrades", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "year": 2025, "url": "http://arxiv.org/abs/2510.03480v2", "abstract": "As a codebase expands over time, its library dependencies can become outdated and require updates to maintain innovation and security. However, updating a library can introduce breaking changes in the code, necessitating significant developer time for maintenance. To address this, we introduce a framework of LLM agents to be used in combination with migration documentation to automatically recommend and apply code updates and ensure compatibility with new versions. Our solution can automatically localize updated library usages in live Java codebases and implement recommended fixes in a user-friendly manner. The system architecture consists of multiple key components: a Summary Agent, Control Agent, and Code Agent. To validate our approach, we apply the framework on an industrial use case by which we create three synthetic code repositories with major Upgrade changes and benchmark our approach against state-of-the-art methods. Results show that our approach not only performs upgrades using fewer tokens across all cases but also achieves a precision of 71.4%, highlighting its efficiency and effectiveness compared to state-of-the-art methods.", "source": "arxiv", "arxiv_id": "2510.03480v2", "pdf_url": "https://arxiv.org/pdf/2510.03480v2", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T19:57:10Z", "updated": "2025-11-24T17:56:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?", "authors": ["Bin Liu", "Yanjie Zhao", "Guoai Xu", "Haoyu Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.14700v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in software engineering and cybersecurity tasks, including code generation, vulnerability discovery, and automated testing. One critical but underexplored application is automated web vulnerability reproduction, which transforms vulnerability reports into working exploits. Although recent advances suggest promising potential, challenges remain in applying LLM agents to real-world web vulnerability reproduction scenarios. In this paper, we present the first comprehensive evaluation of state-of-the-art LLM agents for automated web vulnerability reproduction. We systematically assess 20 agents from software engineering, cybersecurity, and general domains across 16 dimensions, including technical capabilities, environment adaptability, and user experience factors, on 3 representative web vulnerabilities. Based on the results, we select three top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies. Our results reveal that while LLM agents achieve reasonable success on simple library-based vulnerabilities, they consistently fail on complex service-based vulnerabilities requiring multi-component environments. Complex environment configurations and authentication barriers create a gap where agents can execute exploit code but fail to trigger actual vulnerabilities. We observe high sensitivity to input guidance, with performance degrading by over 33% under incomplete authentication information. Our findings highlight the significant gap between current LLM agent capabilities and the demands of reliable automated vulnerability reproduction, emphasizing the need for advances in environmental adaptation and autonomous problem-solving capabilities.", "source": "arxiv", "arxiv_id": "2510.14700v1", "pdf_url": "https://arxiv.org/pdf/2510.14700v1", "categories": ["cs.SE", "cs.CR"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-16T14:04:46Z", "updated": "2025-10-16T14:04:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents for Education: Advances and Applications", "authors": ["Zhendong Chu", "Shen Wang", "Jian Xie", "Tinghui Zhu", "Yibo Yan", "Jinheng Ye", "Aoxiao Zhong", "Xuming Hu", "Jing Liang", "Philip S. Yu", "Qingsong Wen"], "year": 2025, "url": "http://arxiv.org/abs/2503.11733v1", "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) \\emph{Domain-Specific Educational Agents}, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.", "source": "arxiv", "arxiv_id": "2503.11733v1", "pdf_url": "https://arxiv.org/pdf/2503.11733v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-03-14T11:53:44Z", "updated": "2025-03-14T11:53:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents for Generating Microservice-based Applications: how complex is your specification?", "authors": ["Daniel M. Yellin"], "year": 2025, "url": "http://arxiv.org/abs/2508.20119v2", "abstract": "In this paper we evaluate the capabilities of LLM Agents in generating code for real-world problems. Specifically, we explore code synthesis for microservice-based applications, a widely used architectural pattern for building applications. We define a standard template for specifying these applications, and we propose a metric for scoring the difficulty of a specification. The higher the score, the more difficult it is to generate code for the specification. Our experimental results show that agents using strong LLMs (like GPT-3o-mini) do fairly well on medium difficulty specifications but do poorly on those of higher difficulty levels. This is due to more intricate business logic, a greater use of external services, database integration and inclusion of non-functional capabilities such as authentication. We analyzed the errors in LLM-synthesized code and report on the key challenges LLM Agents face in generating code for these specifications. Finally, we show that using a fine-grained approach to code generation improves the correctness of the generated code.", "source": "arxiv", "arxiv_id": "2508.20119v2", "pdf_url": "https://arxiv.org/pdf/2508.20119v2", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-22T12:34:22Z", "updated": "2025-10-26T14:39:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology", "authors": ["Renan Souza", "Timothy Poteet", "Brian Etz", "Daniel Rosendo", "Amal Gueroudji", "Woong Shin", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "year": 2025, "url": "http://arxiv.org/abs/2509.13978v2", "abstract": "Modern scientific discovery increasingly relies on workflows that process data across the Edge, Cloud, and High Performance Computing (HPC) continuum. Comprehensive and in-depth analyses of these data are critical for hypothesis validation, anomaly detection, reproducibility, and impactful findings. Although workflow provenance techniques support such analyses, at large scale, the provenance data become complex and difficult to analyze. Existing systems depend on custom scripts, structured queries, or static dashboards, limiting data interaction. In this work, we introduce an evaluation methodology, reference architecture, and open-source implementation that leverages interactive Large Language Model (LLM) agents for runtime data analysis. Our approach uses a lightweight, metadata-driven design that translates natural language into structured provenance queries. Evaluations across LLaMA, GPT, Gemini, and Claude, covering diverse query classes and a real-world chemistry workflow, show that modular design, prompt tuning, and Retrieval-Augmented Generation (RAG) enable accurate and insightful LLM agent responses beyond recorded provenance.", "source": "arxiv", "arxiv_id": "2509.13978v2", "pdf_url": "https://arxiv.org/pdf/2509.13978v2", "categories": ["cs.DC", "cs.AI", "cs.DB"], "primary_category": "cs.DC", "doi": "10.1145/3731599.3767582", "venue": "", "published": "2025-09-17T13:51:29Z", "updated": "2025-09-23T13:31:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing", "authors": ["Andreas Werbrouck", "Marshall B. Lindsay", "Matthew Maschmann", "Matthias J. Young"], "year": 2025, "url": "http://arxiv.org/abs/2509.26201v1", "abstract": "Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.", "source": "arxiv", "arxiv_id": "2509.26201v1", "pdf_url": "https://arxiv.org/pdf/2509.26201v1", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T13:01:44Z", "updated": "2025-09-30T13:01:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Assisted Coding with Metamorphic Specification Mutation Agent", "authors": ["Mostafijur Rahman Akhond", "Gias Uddin"], "year": 2025, "url": "http://arxiv.org/abs/2511.18249v1", "abstract": "Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development.", "source": "arxiv", "arxiv_id": "2511.18249v1", "pdf_url": "https://arxiv.org/pdf/2511.18249v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-23T02:30:34Z", "updated": "2025-11-23T02:30:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer", "authors": ["Rasoul Zahedifar", "Sayyed Ali Mirghasemi", "Mahdieh Soleymani Baghshah", "Alireza Taheri"], "year": 2025, "url": "http://arxiv.org/abs/2505.19567v1", "abstract": "This study presents the LLM-Agent-Controller, a multi-agent large language model (LLM) system developed to address a wide range of problems in control engineering (Control Theory). The system integrates a central controller agent with multiple specialized auxiliary agents, responsible for tasks such as controller design, model representation, control analysis, time-domain response, and simulation. A supervisor oversees high-level decision-making and workflow coordination, enhancing the system's reliability and efficiency. The LLM-Agent-Controller incorporates advanced capabilities, including Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning, self-criticism and correction, efficient memory handling, and user-friendly natural language communication. It is designed to function without requiring users to have prior knowledge of Control Theory, enabling them to input problems in plain language and receive complete, real-time solutions. To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. We test five categories of Control Theory problems and benchmark performance across three advanced LLMs. Additionally, we conduct a comprehensive qualitative conversational analysis covering all key services. Results show that the LLM-Agent-Controller successfully solved 83% of general tasks, with individual agents achieving an average success rate of 87%. Performance improved with more advanced LLMs. This research demonstrates the potential of multi-agent LLM architectures to solve complex, domain-specific problems. By integrating specialized agents, supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a scalable, robust, and accessible solution framework that can be extended to various technical domains.", "source": "arxiv", "arxiv_id": "2505.19567v1", "pdf_url": "https://arxiv.org/pdf/2505.19567v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T06:30:13Z", "updated": "2025-05-26T06:30:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems", "authors": ["Venkata Sai Aswath Duvvuru", "Bohan Zhang", "Michael Vierhauser", "Ankit Agrawal"], "year": 2025, "url": "http://arxiv.org/abs/2501.11864v1", "abstract": "Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that AutoSimTest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.", "source": "arxiv", "arxiv_id": "2501.11864v1", "pdf_url": "https://arxiv.org/pdf/2501.11864v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-01-21T03:42:21Z", "updated": "2025-01-21T03:42:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM/Agent-as-Data-Analyst: A Survey", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Dayou Zhou", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Xue Yang", "Chunwei Liu", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.23988v3", "abstract": "Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup languages understanding, semi-structured table question answering), (iii) unstructured data (e.g., chart understanding, text/image document understanding), and (iv) heterogeneous data (e.g., data retrieval and modality alignment in data lakes). The technical evolution further distills four key design goals for intelligent data analysis agents, namely semantic-aware design, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.", "source": "arxiv", "arxiv_id": "2509.23988v3", "pdf_url": "https://arxiv.org/pdf/2509.23988v3", "categories": ["cs.AI", "cs.DB"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-28T17:31:38Z", "updated": "2025-10-27T02:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "year": 2025, "url": "http://arxiv.org/abs/2506.17335v1", "abstract": "Large language model (LLM) agents have demonstrated remarkable potential in advancing scientific discovery. However, their capability in the fundamental yet crucial task of reproducing code from research papers, especially in the NLP domain, remains underexplored. This task includes unique complex reasoning challenges in the intellectual synthesis of abstract concepts and the comprehension of code repositories with interdependent files. Motivated by this gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the capability of LLM agents on code reproduction from Language Modeling Research. It consists of 28 code reproduction tasks derived from 23 research papers published in top-tier NLP venues over the past five years, spanning nine fundamental categories. Models are provided with a research paper, a code repository containing one or more masked functions, and instructions for implementing these functions. We conduct extensive experiments in standard prompting and LLM agent settings with state-of-the-art LLMs, evaluating the accuracy of unit tests and performing LLM-based evaluation of code correctness. Experimental results reveal that even the most advanced models still exhibit persistent limitations in scientific reasoning and code synthesis, highlighting critical gaps in LLM agents' ability to autonomously reproduce scientific research", "source": "arxiv", "arxiv_id": "2506.17335v1", "pdf_url": "https://arxiv.org/pdf/2506.17335v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-06-19T07:04:16Z", "updated": "2025-06-19T07:04:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Language Model Agents Under Attack: A Cross Model-Benchmark of Profit-Seeking Behaviors in Customer Service", "authors": ["Jingyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2512.24415v1", "abstract": "Customer-service LLM agents increasingly make policy-bound decisions (refunds, rebooking, billing disputes), but the same ``helpful'' interaction style can be exploited: a small fraction of users can induce unauthorized concessions, shifting costs to others and eroding trust in agentic workflows. We present a cross-domain benchmark of profit-seeking direct prompt injection in customer-service interactions, spanning 10 service domains and 100 realistic attack scripts grouped into five technique families. Across five widely used models under a unified rubric with uncertainty reporting, attacks are highly domain-dependent (airline support is most exploitable) and technique-dependent (payload splitting is most consistently effective). We release data and evaluation code to support reproducible auditing and to inform the design of oversight and recovery workflows for trustworthy, human centered agent interfaces.", "source": "arxiv", "arxiv_id": "2512.24415v1", "pdf_url": "https://arxiv.org/pdf/2512.24415v1", "categories": ["cs.CR", "cs.HC"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-30T18:57:52Z", "updated": "2025-12-30T18:57:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent Personality and Response Appropriateness: Evaluation by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model", "authors": ["Eswari Jayakumar", "Niladri Sekhar Dash", "Debasmita Mukherjee"], "year": 2025, "url": "http://arxiv.org/abs/2510.23875v1", "abstract": "While Large Language Model (LLM)-based agents can be used to create highly engaging interactive applications through prompting personality traits and contextual data, effectively assessing their personalities has proven challenging. This novel interdisciplinary approach addresses this gap by combining agent development and linguistic analysis to assess the prompted personality of LLM-based agents in a poetry explanation task. We developed a novel, flexible question bank, informed by linguistic assessment criteria and human cognitive learning levels, offering a more comprehensive evaluation than current methods. By evaluating agent responses with natural language processing models, other LLMs, and human experts, our findings illustrate the limitations of purely deep learning solutions and emphasize the critical role of interdisciplinary design in agent development.", "source": "arxiv", "arxiv_id": "2510.23875v1", "pdf_url": "https://arxiv.org/pdf/2510.23875v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-27T21:30:12Z", "updated": "2025-10-27T21:30:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent for Modular Task Execution in Drug Discovery", "authors": ["Janghoon Ock", "Radheesh Sharma Meda", "Srivathsan Badrinarayanan", "Neha S. Aluru", "Achuth Chandrasekhar", "Amir Barati Farimani"], "year": 2025, "url": "http://arxiv.org/abs/2507.02925v3", "abstract": "We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, literature-grounded question answering via retrieval-augmented generation, molecular generation, multi-property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. The agent autonomously retrieved relevant biomolecular information, including FASTA sequences, SMILES representations, and literature, and answered mechanistic questions with improved contextual accuracy compared to standard LLMs. It then generated chemically diverse seed molecules and predicted 75 properties, including ADMET-related and general physicochemical descriptors, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55. The number of molecules satisfying empirical drug-likeness filters also rose; for example, compliance with the Ghose filter increased from 32 to 55 within a pool of 100 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.", "source": "arxiv", "arxiv_id": "2507.02925v3", "pdf_url": "https://arxiv.org/pdf/2507.02925v3", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-26T00:19:01Z", "updated": "2025-12-12T03:52:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation", "authors": ["Xin Zhang", "Lissette Iturburu", "Juan Nicolas Villamizar", "Xiaoyu Liu", "Manuel Salmeron", "Shirley J. Dyke", "Julio Ramirez"], "year": 2025, "url": "http://arxiv.org/abs/2507.19771v1", "abstract": "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.", "source": "arxiv", "arxiv_id": "2507.19771v1", "pdf_url": "https://arxiv.org/pdf/2507.19771v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-07-26T03:47:12Z", "updated": "2025-07-26T03:47:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agents for Radio Map Generation and Wireless Network Planning", "authors": ["Hongye Quan", "Wanli Ni", "Tong Zhang", "Xiangyu Ye", "Ziyi Xie", "Shuai Wang", "Yuanwei Liu", "Hui Song"], "year": 2025, "url": "http://arxiv.org/abs/2501.11283v2", "abstract": "Using commercial software for radio map generation and wireless network planning often require complex manual operations, posing significant challenges in terms of scalability, adaptability, and user-friendliness, due to heavy manual operations. To address these issues, we propose an automated solution that employs large language model (LLM) agents. These agents are designed to autonomously generate radio maps and facilitate wireless network planning for specified areas, thereby minimizing the necessity for extensive manual intervention. To validate the effectiveness of our proposed solution, we develop a software platform that integrates LLM agents. Experimental results demonstrate that a large amount manual operations can be saved via the proposed LLM agent, and the automated solutions can achieve an enhanced coverage and signal-to-interference-noise ratio (SINR), especially in urban environments.", "source": "arxiv", "arxiv_id": "2501.11283v2", "pdf_url": "https://arxiv.org/pdf/2501.11283v2", "categories": ["cs.IT"], "primary_category": "cs.IT", "doi": "", "venue": "", "published": "2025-01-20T05:34:38Z", "updated": "2025-02-13T12:48:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents", "authors": ["Davide Paglieri", "BartÅomiej CupiaÅ", "Jonathan Cook", "Ulyana Piterbarg", "Jens Tuyls", "Edward Grefenstette", "Jakob Nicolaus Foerster", "Jack Parker-Holder", "Tim RocktÃ¤schel"], "year": 2025, "url": "http://arxiv.org/abs/2509.03581v2", "abstract": "Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.", "source": "arxiv", "arxiv_id": "2509.03581v2", "pdf_url": "https://arxiv.org/pdf/2509.03581v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-03T18:00:13Z", "updated": "2025-09-30T09:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents", "authors": ["Dongjun Lee", "Juyong Lee", "Kyuyoung Kim", "Jihoon Tack", "Jinwoo Shin", "Yee Whye Teh", "Kimin Lee"], "year": 2025, "url": "http://arxiv.org/abs/2503.10689v2", "abstract": "Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 15.6%, and demonstrates a 23.7% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark. Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts. The relevant code materials are available at our project page: https://lcowiclr2025.github.io.", "source": "arxiv", "arxiv_id": "2503.10689v2", "pdf_url": "https://arxiv.org/pdf/2503.10689v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-12T01:33:40Z", "updated": "2025-12-19T03:49:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties", "authors": ["Philipp J. Schneider", "Lin Tian", "Marian-Andrei Rizoiu"], "year": 2025, "url": "http://arxiv.org/abs/2510.19299v1", "abstract": "Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? We present a multi-agent LLM simulation framework in which agents repeatedly interact, evaluate one another, and adapt their behavior through in-context learning accelerated by a coaching signal. To model human social behavior, we design behavioral reward functions that capture core drivers of online engagement, including social interaction, information seeking, self-presentation, coordination, and emotional support. These rewards align agent objectives with empirically observed user motivations, enabling the study of how network structures and group formations emerge from individual decision-making. Our experiments show that coached LLM agents develop stable interaction patterns and form emergent social ties, yielding network structures that mirror properties of real online communities. By combining behavioral rewards with in-context adaptation, our framework establishes a principled testbed for investigating collective dynamics in LLM populations and reveals how artificial agents may approximate or diverge from human-like social behavior.", "source": "arxiv", "arxiv_id": "2510.19299v1", "pdf_url": "https://arxiv.org/pdf/2510.19299v1", "categories": ["cs.AI", "cs.MA", "cs.SI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-22T07:00:33Z", "updated": "2025-10-22T07:00:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Les Dissonances: Cross-Tool Harvesting and Polluting in Pool-of-Tools Empowered LLM Agents", "authors": ["Zichuan Li", "Jian Cui", "Xiaojing Liao", "Luyi Xing"], "year": 2025, "url": "http://arxiv.org/abs/2504.03111v3", "abstract": "Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools. However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows. In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. We identify a novel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes multiple attack vectors to first hijack the normal control flows of agent tasks, and then collect and pollute confidential or private information within LLM agent systems. To understand the impact of this threat, we developed Chord, a dynamic scanning tool designed to automatically detect real-world agent tools susceptible to XTHP attacks. Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "source": "arxiv", "arxiv_id": "2504.03111v3", "pdf_url": "https://arxiv.org/pdf/2504.03111v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-04T01:41:06Z", "updated": "2025-12-03T15:51:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Leveraging In-Context Learning for Language Model Agents", "authors": ["Shivanshu Gupta", "Sameer Singh", "Ashish Sabharwal", "Tushar Khot", "Ben Bogin"], "year": 2025, "url": "http://arxiv.org/abs/2506.13109v1", "abstract": "In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.", "source": "arxiv", "arxiv_id": "2506.13109v1", "pdf_url": "https://arxiv.org/pdf/2506.13109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-16T05:37:49Z", "updated": "2025-06-16T05:37:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants", "authors": ["Milapji Singh Gill", "Javal Vyas", "Artan Markaj", "Felix Gehlhoff", "Mehmet MercangÃ¶z"], "year": 2025, "url": "http://arxiv.org/abs/2505.02076v1", "abstract": "Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.", "source": "arxiv", "arxiv_id": "2505.02076v1", "pdf_url": "https://arxiv.org/pdf/2505.02076v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-04T12:02:21Z", "updated": "2025-05-04T12:02:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners", "authors": ["Junhao Zheng", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "ZhongZhi Li", "Yingying Zhang", "Le Song", "Qianli Ma"], "year": 2025, "url": "http://arxiv.org/abs/2505.11942v3", "abstract": "Lifelong learning is essential for intelligent agents operating in dynamic environments. Current large language model (LLM)-based agents, however, remain stateless and unable to accumulate or transfer knowledge over time. Existing benchmarks treat agents as static systems and fail to evaluate lifelong learning capabilities. We present LifelongAgentBench, the first unified benchmark designed to systematically assess the lifelong learning ability of LLM agents. It provides skill-grounded, interdependent tasks across three interactive environments, Database, Operating System, and Knowledge Graph, with automatic label verification, reproducibility, and modular extensibility. Extensive experiments reveal that conventional experience replay has limited effectiveness for LLM agents due to irrelevant information and context length constraints. We further introduce a group self-consistency mechanism that significantly improves lifelong learning performance. We hope LifelongAgentBench will advance the development of adaptive, memory-capable LLM agents.", "source": "arxiv", "arxiv_id": "2505.11942v3", "pdf_url": "https://arxiv.org/pdf/2505.11942v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-17T10:09:11Z", "updated": "2025-05-30T02:28:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Roshan Ram", "Akshara Prabhakar", "Tulika Awalgaonkar", "Zixiang Chen", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.13998v1", "abstract": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", "source": "arxiv", "arxiv_id": "2511.13998v1", "pdf_url": "https://arxiv.org/pdf/2511.13998v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-11-17T23:57:24Z", "updated": "2025-11-17T23:57:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "authors": ["Zhaoling Chen", "Xiangru Tang", "Gangda Deng", "Fang Wu", "Jialong Wu", "Zhiwei Jiang", "Viktor Prasanna", "Arman Cohan", "Xingyao Wang"], "year": 2025, "url": "http://arxiv.org/abs/2503.09089v2", "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.", "source": "arxiv", "arxiv_id": "2503.09089v2", "pdf_url": "https://arxiv.org/pdf/2503.09089v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-03-12T05:55:01Z", "updated": "2025-04-29T14:37:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation", "authors": ["Jiin Park", "Misuk Kim"], "year": 2025, "url": "http://arxiv.org/abs/2510.13371v1", "abstract": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.", "source": "arxiv", "arxiv_id": "2510.13371v1", "pdf_url": "https://arxiv.org/pdf/2510.13371v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-15T10:03:29Z", "updated": "2025-10-15T10:03:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces", "authors": ["Loris Gaven", "Thomas Carta", "ClÃ©ment Romac", "CÃ©dric Colas", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "year": 2025, "url": "http://arxiv.org/abs/2502.07709v3", "abstract": "Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.", "source": "arxiv", "arxiv_id": "2502.07709v3", "pdf_url": "https://arxiv.org/pdf/2502.07709v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-11T17:08:00Z", "updated": "2025-06-17T09:23:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs", "authors": ["Onat Ozer", "Grace Wu", "Yuchen Wang", "Daniel Dosti", "Honghao Zhang", "Vivi De La Rue"], "year": 2025, "url": "http://arxiv.org/abs/2512.20845v1", "abstract": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.", "source": "arxiv", "arxiv_id": "2512.20845v1", "pdf_url": "https://arxiv.org/pdf/2512.20845v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T23:47:31Z", "updated": "2025-12-23T23:47:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents", "authors": ["Dongsen Zhang", "Zekun Li", "Xu Luo", "Xuannan Liu", "Peipei Li", "Wenjun Xu"], "year": 2025, "url": "http://arxiv.org/abs/2510.15994v1", "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.", "source": "arxiv", "arxiv_id": "2510.15994v1", "pdf_url": "https://arxiv.org/pdf/2510.15994v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-14T07:36:25Z", "updated": "2025-10-14T07:36:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.24284v2", "abstract": "Large Language Models (LLMs) increasingly rely on external tools to perform complex, realistic tasks, yet their ability to utilize the rapidly expanding Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP research covers few servers, depends on costly manual curation, and lacks training support, hindering progress toward real-world deployment. To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing 68733 high-quality instruction-function call pairs and 6439 trajectories, far exceeding prior work in scale and diversity. Extensive experiments demonstrate MCP-Flow's effectiveness in driving superior MCP tool selection, function-call generation, and enhanced agentic task performance. MCP-Flow thus provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments. MCP-Flow is publicly available at \\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "source": "arxiv", "arxiv_id": "2510.24284v2", "pdf_url": "https://arxiv.org/pdf/2510.24284v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-28T10:42:17Z", "updated": "2025-11-01T07:07:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents", "authors": ["Xiang Fei", "Xiawu Zheng", "Hao Feng"], "year": 2025, "url": "http://arxiv.org/abs/2506.01056v4", "abstract": "True intelligence requires active capability acquisition, yet current LLM agents inject pre-defined tool schemas into prompts, reducing models to passive selectors and falling short of robust general-purpose agency. We introduce MCP-Zero, an active agent framework that restores tool discovery autonomy to LLMs themselves. Instead of overwhelming models with all available tools, MCP-Zero enables agents to actively identify capability gaps, and request specific tools on-demand, transforming them from large-scale retrievers into genuine autonomous agents. The framework operates through three core mechanisms: (1) Active Tool Request, where models autonomously generate structured requests specifying their exact tool requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that matches requests to relevant servers and tools through improved semantic alignment; (3) Iterative Capability Extension, enabling agents to progressively build cross-domain toolchains while maintaining minimal context footprint. We construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797 tools from the official Model-Context-Protocol repository. Experiments demonstrate that MCP-Zero preserves agent autonomy while achieving substantial efficiency gains: (i) accurate tool selection from nearly 3k candidates across 248.1k tokens; (ii) 98\\% reduction in token consumption on APIBank while maintaining high accuracy; and (iii) consistent multi-turn performance that scales with tool ecosystem growth. This work establishes active tool discovery as a fundamental design pattern for scalable autonomous agent systems.", "source": "arxiv", "arxiv_id": "2506.01056v4", "pdf_url": "https://arxiv.org/pdf/2506.01056v4", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-01T15:48:53Z", "updated": "2025-06-24T06:27:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use", "authors": ["Wenrui Liu", "Zixiang Liu", "Elsie Dai", "Wenhan Yu", "Lei Yu", "Tong Yang", "Jinjun Han", "Hong Gao"], "year": 2025, "url": "http://arxiv.org/abs/2512.24565v3", "abstract": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.", "source": "arxiv", "arxiv_id": "2512.24565v3", "pdf_url": "https://arxiv.org/pdf/2512.24565v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T02:09:48Z", "updated": "2026-01-21T06:58:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models", "authors": ["Zhiwei Liu", "Jielin Qiu", "Shiyu Wang", "Jianguo Zhang", "Zuxin Liu", "Roshan Ram", "Haolin Chen", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Huan Wang", "Caiming Xiong"], "year": 2025, "url": "http://arxiv.org/abs/2507.12806v2", "abstract": "The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.", "source": "arxiv", "arxiv_id": "2507.12806v2", "pdf_url": "https://arxiv.org/pdf/2507.12806v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-17T05:46:27Z", "updated": "2025-08-01T22:37:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them", "authors": ["Weichen Zhang", "Yiyou Sun", "Pohao Huang", "Jiayue Pu", "Heyue Lin", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2507.21017v1", "abstract": "Hallucinations pose critical risks for large language model (LLM)-based agents, often manifesting as hallucinative actions resulting from fabricated or misinterpreted information within the cognitive context. While recent studies have exposed such failures, existing evaluations remain fragmented and lack a principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions in Risky AGEnt settings--the first unified benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. We begin by introducing a three-part taxonomy to address agentic hallucinations: actions that are unfaithful to (i) task instructions, (ii) execution history, or (iii) environment observations. To analyze, we first elicit such failures by performing a systematic audit of existing agent benchmarks, then synthesize test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners. To evaluate hallucination behaviors, we adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces. MIRAGE-Bench provides actionable insights on failure modes of LLM agents and lays the groundwork for principled progress in mitigating hallucinations in interactive environments.", "source": "arxiv", "arxiv_id": "2507.21017v1", "pdf_url": "https://arxiv.org/pdf/2507.21017v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-28T17:38:29Z", "updated": "2025-07-28T17:38:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering", "authors": ["Zexi Liu", "Jingyi Chai", "Xinyu Zhu", "Shuo Tang", "Rui Ye", "Bo Zhang", "Lei Bai", "Siheng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2505.23723v1", "abstract": "The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.", "source": "arxiv", "arxiv_id": "2505.23723v1", "pdf_url": "https://arxiv.org/pdf/2505.23723v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-29T17:54:44Z", "updated": "2025-05-29T17:54:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Yinghao Li", "Dingu Sagar V K", "Rongzhi Zhang", "Changhao Li", "Ian Shu-Hei Wong", "Sherry Yang", "Percy Liang", "Chao Zhang", "Bo Dai"], "year": 2025, "url": "http://arxiv.org/abs/2505.07782v1", "abstract": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.", "source": "arxiv", "arxiv_id": "2505.07782v1", "pdf_url": "https://arxiv.org/pdf/2505.07782v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-12T17:35:43Z", "updated": "2025-05-12T17:35:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents", "authors": ["Yuzhen Lei", "Hongbin Xie", "Jiaxing Zhao", "Shuangxue Liu", "Xuan Song"], "year": 2025, "url": "http://arxiv.org/abs/2509.17628v1", "abstract": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.", "source": "arxiv", "arxiv_id": "2509.17628v1", "pdf_url": "https://arxiv.org/pdf/2509.17628v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-22T11:36:16Z", "updated": "2025-09-22T11:36:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents", "authors": ["George Fatouros", "Kostas Metaxas", "John Soldatos", "Manos Karathanassis"], "year": 2025, "url": "http://arxiv.org/abs/2502.00415v2", "abstract": "MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\\&P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\\&P 500 stocks during 2024 demonstrates the framework's scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.", "source": "arxiv", "arxiv_id": "2502.00415v2", "pdf_url": "https://arxiv.org/pdf/2502.00415v2", "categories": ["q-fin.CP", "cs.AI", "cs.CL", "cs.MA", "q-fin.PM"], "primary_category": "q-fin.CP", "doi": "", "venue": "", "published": "2025-02-01T12:33:23Z", "updated": "2025-10-03T06:17:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Measuring temporal effects of agent knowledge by date-controlled tool use", "authors": ["R. Patrick Xian", "Qiming Cui", "Stefan Bauer", "Reza Abbasi-Asl"], "year": 2025, "url": "http://arxiv.org/abs/2503.04188v2", "abstract": "Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet an improper configuration affects the quality of the agent's responses. Here, we assess the agent behavior using distinct date-controlled tools (DCTs) as stress test to measure the knowledge variability of large language model (LLM) agents. We demonstrate the temporal effects of an LLM agent as a writing assistant, which uses web search to complete scientific publication abstracts. We show that the temporality of search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent design and evaluations should take a dynamical view and implement measures to account for the temporal influence of external resources to ensure reliability.", "source": "arxiv", "arxiv_id": "2503.04188v2", "pdf_url": "https://arxiv.org/pdf/2503.04188v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-06T08:03:51Z", "updated": "2025-04-03T17:53:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels", "authors": ["Chenghao Du", "Quanfeng Huang", "Tingxuan Tang", "Zihao Wang", "Adwait Nadkarni", "Yue Xiao"], "year": 2025, "url": "http://arxiv.org/abs/2510.27140v2", "abstract": "Large Language Models (LLMs) have transformed software development, enabling AI-powered applications known as LLM-based agents that promise to automate tasks across diverse apps and workflows. Yet, the security implications of deploying such agents in adversarial mobile environments remain poorly understood. In this paper, we present the first systematic study of security risks in mobile LLM agents. We design and evaluate a suite of adversarial case studies, ranging from opportunistic manipulations such as pop-up advertisements to advanced, end-to-end workflows involving malware installation and cross-app data exfiltration. Our evaluation covers eight state-of-the-art mobile agents across three architectures, with over 2,000 adversarial and paired benign trials. The results reveal systemic vulnerabilities: low-barrier vectors such as fraudulent ads succeed with over 80% reliability, while even workflows requiring the circumvention of operating-system warnings, such as malware installation, are consistently completed by advanced multi-app agents. By mapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel privilege-escalation and persistence pathways unique to LLM-driven automation. Collectively, our findings provide the first end-to-end evidence that mobile LLM agents are exploitable in realistic adversarial settings, where untrusted third-party channels (e.g., ads, embedded webviews, cross-app notifications) are an inherent part of the mobile ecosystem.", "source": "arxiv", "arxiv_id": "2510.27140v2", "pdf_url": "https://arxiv.org/pdf/2510.27140v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-31T03:35:59Z", "updated": "2025-11-06T03:52:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph", "authors": ["Duzhen Zhang", "Zixiao Wang", "Zhong-Zhi Li", "Yahan Yu", "Shuncheng Jia", "Jiahua Dong", "Haotian Xu", "Xing Wu", "Yingying Zhang", "Tielin Zhang", "Jie Yang", "Xiuying Chen", "Le Song"], "year": 2025, "url": "http://arxiv.org/abs/2508.12393v2", "abstract": "The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.", "source": "arxiv", "arxiv_id": "2508.12393v2", "pdf_url": "https://arxiv.org/pdf/2508.12393v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-17T15:14:03Z", "updated": "2025-08-19T05:18:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MedicalOS: An LLM Agent based Operating System for Digital Healthcare", "authors": ["Jared Zhu", "Junde Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.11507v1", "abstract": "Decades' advances in digital health technologies, such as electronic health records, have largely streamlined routine clinical processes. Yet, most these systems are still hard to learn and use: Clinicians often face the burden of managing multiple tools, repeating manual actions for each patient, navigating complicated UI trees to locate functions, and spending significant time on administration instead of caring for patients. The recent rise of large language model (LLM) based agents demonstrates exceptional capability in coding and computer operation, revealing the potential for humans to interact with operating systems and software not by direct manipulation, but by instructing agents through natural language. This shift highlights the need for an abstraction layer, an agent-computer interface, that translates human language into machine-executable commands. In digital healthcare, however, requires a more domain-specific abstractions that strictly follow trusted clinical guidelines and procedural standards to ensure safety, transparency, and compliance. To address this need, we present \\textbf{MedicalOS}, a unified agent-based operational system designed as such a domain-specific abstract layer for healthcare. It translates human instructions into pre-defined digital healthcare commands, such as patient inquiry, history retrieval, exam management, report generation, referrals, treatment planning, that we wrapped as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP, Linux). We empirically validate MedicalOS on 214 patient cases across 22 specialties, demonstrating high diagnostic accuracy and confidence, clinically sound examination requests, and consistent generation of structured reports and medication recommendations. These results highlight MedicalOS as a trustworthy and scalable foundation for advancing workflow automation in clinical practice.", "source": "arxiv", "arxiv_id": "2509.11507v1", "pdf_url": "https://arxiv.org/pdf/2509.11507v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-15T01:43:20Z", "updated": "2025-09-15T01:43:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MemGuide: Intent-Driven Memory Selection for Goal-Oriented Multi-Session LLM Agents", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "year": 2025, "url": "http://arxiv.org/abs/2505.20231v2", "abstract": "Modern task-oriented dialogue (TOD) systems increasingly rely on large language model (LLM) agents, leveraging Retrieval-Augmented Generation (RAG) and long-context capabilities for long-term memory utilization. However, these methods are primarily based on semantic similarity, overlooking task intent and reducing task coherence in multi-session dialogues. To address this challenge, we introduce MemGuide, a two-stage framework for intent-driven memory selection. (1) Intent-Aligned Retrieval matches the current dialogue context with stored intent descriptions in the memory bank, retrieving QA-formatted memory units that share the same goal. (2) Missing-Slot Guided Filtering employs a chain-of-thought slot reasoner to enumerate unfilled slots, then uses a fine-tuned LLaMA-8B filter to re-rank the retrieved units by marginal slot-completion gain. The resulting memory units inform a proactive strategy that minimizes conversational turns by directly addressing information gaps. Based on this framework, we introduce the MS-TOD, the first multi-session TOD benchmark comprising 132 diverse personas, 956 task goals, and annotated intent-aligned memory targets, supporting efficient multi-session task completion. Evaluations on MS-TOD show that MemGuide raises the task success rate by 11% (88% -> 99%) and reduces dialogue length by 2.84 turns in multi-session settings, while maintaining parity with single-session benchmarks.", "source": "arxiv", "arxiv_id": "2505.20231v2", "pdf_url": "https://arxiv.org/pdf/2505.20231v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-26T17:10:43Z", "updated": "2025-08-13T03:43:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "year": 2025, "url": "http://arxiv.org/abs/2503.21760v2", "abstract": "Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.", "source": "arxiv", "arxiv_id": "2503.21760v2", "pdf_url": "https://arxiv.org/pdf/2503.21760v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-03-27T17:57:28Z", "updated": "2025-07-31T23:26:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents", "authors": ["Xingbo Du", "Loka Li", "Duzhen Zhang", "Le Song"], "year": 2025, "url": "http://arxiv.org/abs/2512.20237v1", "abstract": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.", "source": "arxiv", "arxiv_id": "2512.20237v1", "pdf_url": "https://arxiv.org/pdf/2512.20237v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-23T10:49:42Z", "updated": "2025-12-23T10:49:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "year": 2025, "url": "http://arxiv.org/abs/2507.21428v1", "abstract": "Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.", "source": "arxiv", "arxiv_id": "2507.21428v1", "pdf_url": "https://arxiv.org/pdf/2507.21428v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-29T01:42:06Z", "updated": "2025-07-29T01:42:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs", "authors": ["Huichi Zhou", "Yihang Chen", "Siyuan Guo", "Xue Yan", "Kin Hei Lee", "Zihan Wang", "Ka Yiu Lee", "Guchun Zhang", "Kun Shao", "Linyi Yang", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.16153v2", "abstract": "In this paper, we introduce a novel learning paradigm for Adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely \\emph{Memento}, which attains top-1 on GAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It reaches $66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds $4.7\\%$ to $9.6\\%$ absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/Memento.", "source": "arxiv", "arxiv_id": "2508.16153v2", "pdf_url": "https://arxiv.org/pdf/2508.16153v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-22T07:25:30Z", "updated": "2025-08-25T13:32:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction", "authors": ["Shen Dong", "Shaochen Xu", "Pengfei He", "Yige Li", "Jiliang Tang", "Tianming Liu", "Hui Liu", "Zhen Xiang"], "year": 2025, "url": "http://arxiv.org/abs/2503.03704v4", "abstract": "Agents powered by large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications. However, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious. In this paper, we propose a novel Memory INJection Attack, MINJA, without assuming that the attacker can directly modify the memory bank of the agent. The attacker injects malicious records into the memory bank by only interacting with the agent via queries and output observations. These malicious records are designed to elicit a sequence of malicious reasoning steps corresponding to a different target query during the agent's execution of the victim user's query. Specifically, we introduce a sequence of bridging steps to link victim queries to the malicious reasoning steps. During the memory injection, we propose an indication prompt that guides the agent to autonomously generate similar bridging steps, with a progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing later victim queries. Our extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory. With minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting the risk.", "source": "arxiv", "arxiv_id": "2503.03704v4", "pdf_url": "https://arxiv.org/pdf/2503.03704v4", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-05T17:53:24Z", "updated": "2025-12-10T02:07:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games", "authors": ["Runnan Qi", "Yanan Ni", "Lumin Jiang", "Zongyuan Li", "Kuihua Huang", "Xian Guo"], "year": 2025, "url": "http://arxiv.org/abs/2510.18395v1", "abstract": "This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel framework for LLM agents in real-time strategy games. Addressing key challenges like hallucinations and fragmented decision-making in existing approaches, MASMP integrates state machine prompting with memory mechanisms to unify structured actions with long-term tactical coherence. The framework features: (1) a natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts, and (2) a lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly outperforming baselines (0%). Case studies reveal the method retains LLMs' semantic comprehension while resolving the \"Knowing-Doing Gap\" through strict state-action mapping, achieving both interpretability and FSM-like reliability. This work establishes a new paradigm for combining neural and symbolic AI in complex decision-making.", "source": "arxiv", "arxiv_id": "2510.18395v1", "pdf_url": "https://arxiv.org/pdf/2510.18395v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T08:15:04Z", "updated": "2025-10-21T08:15:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Jinhe Bi", "Kristian Kersting", "Jeff Z. Pan", "Hinrich SchÃ¼tze", "Volker Tresp", "Yunpu Ma"], "year": 2025, "url": "http://arxiv.org/abs/2508.19828v5", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking a learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns structured operations, including ADD, UPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over relevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision. With only 152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes across diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and multiple model scales (3B-14B).", "source": "arxiv", "arxiv_id": "2508.19828v5", "pdf_url": "https://arxiv.org/pdf/2508.19828v5", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-27T12:26:55Z", "updated": "2026-01-14T14:21:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval", "authors": ["Saksham Sahai Srivastava", "Haoyu He"], "year": 2025, "url": "http://arxiv.org/abs/2512.16962v1", "abstract": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.", "source": "arxiv", "arxiv_id": "2512.16962v1", "pdf_url": "https://arxiv.org/pdf/2512.16962v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-18T08:34:40Z", "updated": "2025-12-18T08:34:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Merge and Conquer: Evolutionarily Optimizing AI for 2048", "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"], "year": 2025, "url": "http://arxiv.org/abs/2510.20205v1", "abstract": "Optimizing artificial intelligence (AI) for dynamic environments remains a fundamental challenge in machine learning research. In this paper, we examine evolutionary training methods for optimizing AI to solve the game 2048, a 2D sliding puzzle. 2048, with its mix of strategic gameplay and stochastic elements, presents an ideal playground for studying decision-making, long-term planning, and dynamic adaptation. We implemented two distinct systems: a two-agent metaprompting system where a \"thinker\" large language model (LLM) agent refines gameplay strategies for an \"executor\" LLM agent, and a single-agent system based on refining a value function for a limited Monte Carlo Tree Search. We also experimented with rollback features to avoid performance degradation. Our results demonstrate the potential of evolutionary refinement techniques in improving AI performance in non-deterministic environments. The single-agent system achieved substantial improvements, with an average increase of 473.2 points per cycle, and with clear upward trends (correlation $Ï$=0.607) across training cycles. The LLM's understanding of the game grew as well, shown in its development of increasingly advanced strategies. Conversely, the two-agent system did not garner much improvement, highlighting the inherent limits of meta-prompting.", "source": "arxiv", "arxiv_id": "2510.20205v1", "pdf_url": "https://arxiv.org/pdf/2510.20205v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-23T04:45:05Z", "updated": "2025-10-23T04:45:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent", "authors": ["Chunlong Wu", "Ye Luo", "Zhibo Qu", "Min Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.03990v2", "abstract": "Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi-agent extensions.", "source": "arxiv", "arxiv_id": "2509.03990v2", "pdf_url": "https://arxiv.org/pdf/2509.03990v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-04T08:18:39Z", "updated": "2025-09-08T07:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Meta-RAG on Large Codebases Using Code Summarization", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "year": 2025, "url": "http://arxiv.org/abs/2508.02611v1", "abstract": "Large Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (AI) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation and well into the realm of code maintenance. In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation. We then use an LLM agent to determine which parts of the codebase are critical for bug resolution, i.e. bug localization. We demonstrate the usefulness of Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance.", "source": "arxiv", "arxiv_id": "2508.02611v1", "pdf_url": "https://arxiv.org/pdf/2508.02611v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-04T17:01:10Z", "updated": "2025-08-04T17:01:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents", "authors": ["Pan Tang", "Shixiang Tang", "Huanqi Pu", "Zhiqing Miao", "Zhixing Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.15635v1", "abstract": "This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: https://github.com/tangpan360/MicroRCA-Agent.", "source": "arxiv", "arxiv_id": "2509.15635v1", "pdf_url": "https://arxiv.org/pdf/2509.15635v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-19T05:57:03Z", "updated": "2025-09-19T05:57:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2507.05330v1", "abstract": "Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular \"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.", "source": "arxiv", "arxiv_id": "2507.05330v1", "pdf_url": "https://arxiv.org/pdf/2507.05330v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-07T17:53:55Z", "updated": "2025-07-07T17:53:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MindGuard: Intrinsic Decision Inspection for Securing LLM Agents Against Metadata Poisoning", "authors": ["Zhiqiang Wang", "Haohua Du", "Guanquan Shi", "Junyang Zhang", "HaoRan Cheng", "Yunhao Yao", "Kaiwen Guo", "Xiang-Yang Li"], "year": 2025, "url": "http://arxiv.org/abs/2508.20412v3", "abstract": "The Model Context Protocol (MCP) is increasingly adopted to standardize the interaction between LLM agents and external tools. However, this trend introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is poisoned to induce the agent to perform unauthorized operations. Existing defenses that primarily focus on behavior-level analysis are fundamentally ineffective against TPA, as poisoned tools need not be executed, leaving no behavioral trace to monitor.\n  Thus, we propose MindGuard, a decision-level guardrail for LLM agents, providing provenance tracking of call decisions, policy-agnostic detection, and poisoning source attribution against TPA. While fully explaining LLM decision remains challenging, our empirical findings uncover a strong correlation between LLM attention mechanisms and tool invocation decisions. Therefore, we choose attention as an empirical signal for decision tracking and formalize this as the Decision Dependence Graph (DDG), which models the LLM's reasoning process as a weighted, directed graph where vertices represent logical concepts and edges quantify the attention-based dependencies. We further design robust DDG construction and graph-based anomaly analysis mechanisms that efficiently detect and attribute TPA attacks. Extensive experiments on real-world datasets demonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting poisoned invocations, 95\\%-100\\% attribution accuracy, with processing times under one second and no additional token cost. Moreover, DDG can be viewed as an adaptation of the classical Program Dependence Graph (PDG), providing a solid foundation for applying traditional security policies at the decision level.", "source": "arxiv", "arxiv_id": "2508.20412v3", "pdf_url": "https://arxiv.org/pdf/2508.20412v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-28T04:23:44Z", "updated": "2026-01-15T02:58:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents", "authors": ["Zongxi Liu", "Hongyang Du"], "year": 2025, "url": "http://arxiv.org/abs/2505.01834v1", "abstract": "Large Language Models (LLMs) exhibit strong general-purpose reasoning abilities but lack access to wireless environment information due to the absence of native sensory input and domain-specific priors. Previous attempts to apply LLMs in wireless systems either depend on retraining with network-specific data, which compromises language generalization, or rely on manually scripted interfaces, which hinder scalability. To overcome these limitations, we propose a Model Context Protocol (MCP)-based Internet of Experts (IoX) framework that equips LLMs with wireless environment-aware reasoning capabilities. The framework incorporates a set of lightweight expert models, each trained to solve a specific deterministic task in wireless communications, such as detecting a specific wireless attribute, e.g., line-of-sight propagation, Doppler effects, or fading conditions. Through MCP, the LLM can selectively query and interpret expert outputs at inference time, without modifying its own parameters. This architecture enables modular, extensible, and interpretable reasoning over wireless contexts. Evaluated across multiple mainstream LLMs, the proposed wireless environment-aware LLM agents achieve 40%-50% improvements in classification tasks over LLM-only baselines. More broadly, the MCP-based design offers a viable paradigm for future LLMs to inherit structured wireless network management capabilities.", "source": "arxiv", "arxiv_id": "2505.01834v1", "pdf_url": "https://arxiv.org/pdf/2505.01834v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-05-03T14:41:24Z", "updated": "2025-05-03T14:41:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning", "authors": ["Zhiyu An", "Wan Du"], "year": 2025, "url": "http://arxiv.org/abs/2511.12271v1", "abstract": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.", "source": "arxiv", "arxiv_id": "2511.12271v1", "pdf_url": "https://arxiv.org/pdf/2511.12271v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-15T15:52:10Z", "updated": "2025-11-15T15:52:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.21967v1", "abstract": "Current evaluations of tool-integrated LLM agents typically focus on end-to-end tool-usage evaluation while neglecting their stability. This limits their real-world applicability, as various internal or external factors can cause agents to crash or behave abnormally. Our research addresses this by investigating whether agents are vulnerable to errors throughout the entire tool invocation process, including reading tool documentation, selecting tools and generating parameters, and processing the tool's response. Through extensive experiments, we observe that agents are highly susceptible to errors at each stage and agents based on open-source models are more vulnerable than those based on proprietary models. We also find that increasing the model size does not significantly improve tool invocation reasoning and may make agents more vulnerable to attacks resembling normal user instructions. This highlights the importance of evaluating agent stability and offers valuable insights for future LLM development and evaluation.", "source": "arxiv", "arxiv_id": "2506.21967v1", "pdf_url": "https://arxiv.org/pdf/2506.21967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-27T07:13:29Z", "updated": "2025-06-27T07:13:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation", "authors": ["Pravallika Abbineni", "Saoud Aldowaish", "Colin Liechty", "Soroosh Noorzad", "Ali Ghazizadeh", "Morteza Fayazi"], "year": 2025, "url": "http://arxiv.org/abs/2508.08137v1", "abstract": "Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.", "source": "arxiv", "arxiv_id": "2508.08137v1", "pdf_url": "https://arxiv.org/pdf/2508.08137v1", "categories": ["cs.LG", "cs.AI", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-08-11T16:11:09Z", "updated": "2025-08-11T16:11:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation", "authors": ["Jiaju Chen", "Yuxuan Lu", "Xiaojie Wang", "Huimin Zeng", "Jing Huang", "Jiri Gesi", "Ying Xu", "Bingsheng Yao", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2507.21028v1", "abstract": "Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.", "source": "arxiv", "arxiv_id": "2507.21028v1", "pdf_url": "https://arxiv.org/pdf/2507.21028v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-28T17:48:40Z", "updated": "2025-07-28T17:48:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions", "authors": ["Xi Wang", "Xianyao Ling", "Kun Li", "Gang Yin", "Liang Zhang", "Jiang Wu", "Jun Xu", "Fu Zhang", "Wenbo Lei", "Annie Wang", "Peng Gong"], "year": 2025, "url": "http://arxiv.org/abs/2510.15258v2", "abstract": "In the current era of big data, extracting deep insights from massive, heterogeneous, and complexly associated multi-dimensional data has become a significant challenge. Large Language Models (LLMs) perform well in natural language understanding and generation, but still suffer from \"hallucination\" issues when processing structured knowledge and are difficult to update in real-time. Although Knowledge Graphs (KGs) can explicitly store structured knowledge, their static nature limits dynamic interaction and analytical capabilities. Therefore, this paper proposes a multi-dimensional data analysis method based on the interactions between LLM agents and KGs, constructing a dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to automatically extract product data from unstructured data, constructs and visualizes the KG in real-time, and supports users in deep exploration and analysis of graph nodes through an interactive platform. Experimental results show that this method has significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis, providing new ideas and tools for multi-dimensional data analysis.", "source": "arxiv", "arxiv_id": "2510.15258v2", "pdf_url": "https://arxiv.org/pdf/2510.15258v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-17T02:38:44Z", "updated": "2025-11-20T06:48:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "year": 2025, "url": "http://arxiv.org/abs/2505.24671v2", "abstract": "Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).", "source": "arxiv", "arxiv_id": "2505.24671v2", "pdf_url": "https://arxiv.org/pdf/2505.24671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-30T15:01:52Z", "updated": "2025-09-01T12:34:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "NetworkGames: Simulating Cooperation in Network Games with Personality-driven LLM Agents", "authors": ["Xuan Qiu"], "year": 2025, "url": "http://arxiv.org/abs/2511.21783v1", "abstract": "The advent of Large Language Models (LLMs) presents a novel opportunity to build high-fidelity agent-based models for simulating complex social systems. However, the behavior of these LLM-based agents in game-theoretic network games remains surprisingly unexplored. In this work, we introduce \"NetworkGames,\" a novel simulation framework designed to investigate how network topology and agent personality jointly shape the evolution of cooperation in network games. We instantiate a population of LLM agents, each endowed with a distinct personality from the MBTI taxonomy, and situate them in various network structures (e.g., small-world and scale-free). Through extensive simulations of the Iterated Prisoner's Dilemma, we first establish a baseline dyadic interaction matrix, revealing nuanced cooperative preferences between all 16 personality pairs. We then demonstrate that macro-level cooperative outcomes are not predictable from dyadic interactions alone; they are co-determined by the network's connectivity and the spatial distribution of personalities. For instance, we find that small-world networks are detrimental to cooperation, while strategically placing pro-social personalities in hub positions within scale-free networks can significantly promote cooperative behavior. Our findings offer significant implications for designing healthier online social environments and forecasting collective behavior. We open-source our framework to foster further research in network game simulations.", "source": "arxiv", "arxiv_id": "2511.21783v1", "pdf_url": "https://arxiv.org/pdf/2511.21783v1", "categories": ["physics.soc-ph", "cs.GT"], "primary_category": "physics.soc-ph", "doi": "", "venue": "", "published": "2025-11-26T13:30:15Z", "updated": "2025-11-26T13:30:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents", "authors": ["Rongwu Xu", "Xiaojian Li", "Shuo Chen", "Wei Xu"], "year": 2025, "url": "http://arxiv.org/abs/2502.11355v3", "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We release our code to foster further research.", "source": "arxiv", "arxiv_id": "2502.11355v3", "pdf_url": "https://arxiv.org/pdf/2502.11355v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-17T02:11:17Z", "updated": "2025-03-23T06:22:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent", "authors": ["Bowen Chen", "Zhao Wang", "Shingo Takamatsu"], "year": 2025, "url": "http://arxiv.org/abs/2507.02353v1", "abstract": "Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.", "source": "arxiv", "arxiv_id": "2507.02353v1", "pdf_url": "https://arxiv.org/pdf/2507.02353v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-03T06:37:55Z", "updated": "2025-07-03T06:37:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems", "authors": ["Xiaozhe Li", "Jixuan Chen", "Xinyu Fang", "Shengyuan Ding", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "year": 2025, "url": "http://arxiv.org/abs/2506.10764v1", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in solving diverse tasks. However, their proficiency in iteratively optimizing complex solutions through learning from previous feedback remains insufficiently explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark designed to evaluate LLM agents on large-scale search space optimization problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from Kaggle and 10 classical NP problems, offering a diverse and challenging environment for assessing LLM agents on iterative reasoning and solution refinement. To enable rigorous evaluation, we introduce OPT-Agent, an end-to-end optimization framework that emulates human reasoning when tackling complex problems by generating, validating, and iteratively improving solutions through leveraging historical feedback. Through extensive experiments on 9 state-of-the-art LLMs from 6 model families, we analyze the effects of optimization iterations, temperature settings, and model architectures on solution quality and convergence. Our results demonstrate that incorporating historical context significantly enhances optimization performance across both ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to promote further research in advancing LLM-driven optimization and iterative reasoning. Project page: \\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.", "source": "arxiv", "arxiv_id": "2506.10764v1", "pdf_url": "https://arxiv.org/pdf/2506.10764v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-12T14:46:41Z", "updated": "2025-06-12T14:46:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problems with Reasoning LLM", "authors": ["Bowen Zhang", "Pengcheng Luo", "Genke Yang", "Boon-Hee Soong", "Chau Yuen"], "year": 2025, "url": "http://arxiv.org/abs/2503.10009v3", "abstract": "With the rise of artificial intelligence (AI), applying large language models (LLMs) to mathematical problem-solving has attracted increasing attention. Most existing approaches attempt to improve Operations Research (OR) optimization problem-solving through prompt engineering or fine-tuning strategies for LLMs. However, these methods are fundamentally constrained by the limited capabilities of non-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an AI agent framework built on reasoning LLMs for automated OR problem solving. The framework decomposes the task into three sequential stages: mathematical modeling, code generation, and debugging. Each task is handled by a dedicated sub-agent, which enables more targeted reasoning. We also construct BWOR, an OR dataset for evaluating LLM performance on OR tasks. Our analysis shows that in the benchmarks NL4OPT, MAMO, and IndustryOR, reasoning LLMs sometimes underperform their non-reasoning counterparts within the same model family. In contrast, BWOR provides a more consistent and discriminative assessment of model capabilities. Experimental results demonstrate that OR-LLM-Agent utilizing DeepSeek-R1 in its framework outperforms advanced methods, including GPT-o3, Gemini 2.5 Pro, DeepSeek-R1, and ORLM, by at least 7\\% in accuracy. These results demonstrate the effectiveness of task decomposition for OR problem solving.", "source": "arxiv", "arxiv_id": "2503.10009v3", "pdf_url": "https://arxiv.org/pdf/2503.10009v3", "categories": ["cs.AI", "math.OC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-13T03:40:50Z", "updated": "2025-08-01T04:52:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ORFS-agent: Tool-Using Agents for Chip Design Optimization", "authors": ["Amur Ghose", "Andrew B. Kahng", "Sayak Kundu", "Zhiang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.08332v2", "abstract": "Machine learning has been widely used to optimize complex engineering workflows across numerous domains. In the context of integrated circuit design, modern flows (e.g., going from a register-transfer level netlist to physical layouts) involve extensive configuration via thousands of parameters, and small changes to these parameters can have large downstream impacts on desired outcomes - namely design performance, power, and area. Recent advances in Large Language Models (LLMs) offer new opportunities for learning and reasoning within such high-dimensional optimization tasks. In this work, we introduce ORFS-agent, an LLM-based iterative optimization agent that automates parameter tuning in an open-source hardware design flow. ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics. Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Moreover, by following natural language objectives to trade off certain metrics for others, ORFS-agent demonstrates a flexible and interpretable framework for multi-objective optimization. Crucially, RFS-agent is modular and model-agnostic, and can be plugged in to any frontier LLM without any further fine-tuning.", "source": "arxiv", "arxiv_id": "2506.08332v2", "pdf_url": "https://arxiv.org/pdf/2506.08332v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-10T01:38:57Z", "updated": "2025-08-01T06:36:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Xiaofei Sun", "Keze Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.04876v1", "abstract": "This paper introduces OSC (Orchestrating Cognitive Synergy), a knowledge-aware adaptive collaboration framework designed to enhance cognitive synergy in multi-agent systems with large language models. While prior work has advanced agent selection and result aggregation, efficient linguistic interactions for deep collaboration among expert agents remain a critical bottleneck. OSC addresses this gap as a pivotal intermediate layer between selection and aggregation, introducing Collaborator Knowledge Models (CKM) to enable each agent to dynamically perceive its collaborators' cognitive states. Through real-time cognitive gap analysis, agents adaptively adjust communication behaviors, including content focus, detail level, and expression style, using learned strategies. Experiments on complex reasoning and problem-solving benchmarks demonstrate that OSC significantly improves task performance and communication efficiency, transforming \"parallel-working individuals'' into a \"deeply collaborative cognitive team.'' This framework not only optimizes multi-agent collaboration but also offers new insights into LLM agent interaction behaviors.", "source": "arxiv", "arxiv_id": "2509.04876v1", "pdf_url": "https://arxiv.org/pdf/2509.04876v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-05T07:44:05Z", "updated": "2025-09-05T07:44:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows", "authors": ["Weixuan Wang", "Dongge Han", "Daniel Madrigal Diaz", "Jin Xu", "Victor RÃ¼hle", "Saravan Rajmohan"], "year": 2025, "url": "http://arxiv.org/abs/2508.09124v1", "abstract": "Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks. We believe that OdysseyBench will serve as a valuable resource for advancing the development and evaluation of LLM agents in real-world productivity scenarios. In addition, we release OdysseyBench and HomerAgents to foster research along this line.", "source": "arxiv", "arxiv_id": "2508.09124v1", "pdf_url": "https://arxiv.org/pdf/2508.09124v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-12T17:53:03Z", "updated": "2025-08-12T17:53:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections", "authors": ["Manasa Bharadwaj", "Nikhil Verma", "Kevin Ferreira"], "year": 2025, "url": "http://arxiv.org/abs/2506.17449v1", "abstract": "Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.", "source": "arxiv", "arxiv_id": "2506.17449v1", "pdf_url": "https://arxiv.org/pdf/2506.17449v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-20T19:38:21Z", "updated": "2025-06-20T19:38:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues", "authors": ["Monika Zamojska", "JarosÅaw A. Chudziak"], "year": 2025, "url": "http://arxiv.org/abs/2512.17060v1", "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.", "source": "arxiv", "arxiv_id": "2512.17060v1", "pdf_url": "https://arxiv.org/pdf/2512.17060v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-18T20:53:31Z", "updated": "2025-12-18T20:53:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language", "authors": ["SÃ©bastien Salva", "Redha Taguelmimt"], "year": 2025, "url": "http://arxiv.org/abs/2509.19136v2", "abstract": "The use of natural language (NL) test cases for validating graphical user interface (GUI) applications is emerging as a promising direction to manually written executable test scripts, which are costly to develop and difficult to maintain. Recent advances in large language models (LLMs) have opened the possibility of the direct execution of NL test cases by LLM agents. This paper investigates this direction, focusing on the impact on NL test case unsoundness and on test case execution consistency. NL test cases are inherently unsound, as they may yield false failures due to ambiguous instructions or unpredictable agent behaviour. Furthermore, repeated executions of the same NL test case may lead to inconsistent outcomes, undermining test reliability. To address these challenges, we propose an algorithm for executing NL test cases with guardrail mechanisms and specialised agents that dynamically verify the correct execution of each test step. We introduce measures to evaluate the capabilities of LLMs in test execution and one measure to quantify execution consistency. We propose a definition of weak unsoundness to characterise contexts in which NL test case execution remains acceptable, with respect to the industrial quality levels Six Sigma. Our experimental evaluation with eight publicly available LLMs, ranging from 3B to 70B parameters, demonstrates both the potential and current limitations of current LLM agents for GUI testing. Our experiments show that Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case execution with high execution consistency (above the level 3-sigma). We provide prototype tools, test suites, and results.", "source": "arxiv", "arxiv_id": "2509.19136v2", "pdf_url": "https://arxiv.org/pdf/2509.19136v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-09-23T15:20:40Z", "updated": "2025-10-01T09:32:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents", "authors": ["Zhaoxi Zhang", "Yitong Duan", "Yanzhi Zhang", "Yiming Xu", "Weikang Li", "Jiahui Liang", "Deguo Xia", "Jizhou Huang", "Jiyan He", "Yunfang Wu"], "year": 2025, "url": "http://arxiv.org/abs/2512.20957v4", "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.", "source": "arxiv", "arxiv_id": "2512.20957v4", "pdf_url": "https://arxiv.org/pdf/2512.20957v4", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-12-24T05:27:53Z", "updated": "2026-01-08T03:22:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "year": 2025, "url": "http://arxiv.org/abs/2505.24878v1", "abstract": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.", "source": "arxiv", "arxiv_id": "2505.24878v1", "pdf_url": "https://arxiv.org/pdf/2505.24878v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-30T17:59:55Z", "updated": "2025-05-30T17:59:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OpenFOAMGPT: a RAG-Augmented LLM Agent for OpenFOAM-Based Computational Fluid Dynamics", "authors": ["Sandeep Pandey", "Ran Xu", "Wenkang Wang", "Xu Chu"], "year": 2025, "url": "http://arxiv.org/abs/2501.06327v1", "abstract": "This work presents a large language model (LLM)-based agent OpenFOAMGPT tailored for OpenFOAM-centric computational fluid dynamics (CFD) simulations, leveraging two foundation models from OpenAI: the GPT-4o and a chain-of-thought (CoT)-enabled o1 preview model. Both agents demonstrate success across multiple tasks. While the price of token with o1 model is six times as that of GPT-4o, it consistently exhibits superior performance in handling complex tasks, from zero-shot case setup to boundary condition modifications, turbulence model adjustments, and code translation. Through an iterative correction loop, the agent efficiently addressed single- and multi-phase flow, heat transfer, RANS, LES, and other engineering scenarios, often converging in a limited number of iterations at low token costs. To embed domain-specific knowledge, we employed a retrieval-augmented generation (RAG) pipeline, demonstrating how preexisting simulation setups can further specialize the agent for sub-domains such as energy and aerospace. Despite the great performance of the agent, human oversight remains crucial for ensuring accuracy and adapting to shifting contexts. Fluctuations in model performance over time suggest the need for monitoring in mission-critical applications. Although our demonstrations focus on OpenFOAM, the adaptable nature of this framework opens the door to developing LLM-driven agents into a wide range of solvers and codes. By streamlining CFD simulations, this approach has the potential to accelerate both fundamental research and industrial engineering advancements.", "source": "arxiv", "arxiv_id": "2501.06327v1", "pdf_url": "https://arxiv.org/pdf/2501.06327v1", "categories": ["physics.flu-dyn", "physics.comp-ph"], "primary_category": "physics.flu-dyn", "doi": "10.1063/5.0257555", "venue": "", "published": "2025-01-10T20:07:05Z", "updated": "2025-01-10T20:07:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Opponent Shaping in LLM Agents", "authors": ["Marta Emili Garcia Segura", "Stephen Hailes", "Mirco Musolesi"], "year": 2025, "url": "http://arxiv.org/abs/2510.08255v1", "abstract": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.", "source": "arxiv", "arxiv_id": "2510.08255v1", "pdf_url": "https://arxiv.org/pdf/2510.08255v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-09T14:13:24Z", "updated": "2025-10-09T14:13:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "authors": ["Dongmin Park", "Minkyu Kim", "Beongjun Choi", "Junhyuck Kim", "Keon Lee", "Jonghyun Lee", "Inkyu Park", "Byeong-Uk Lee", "Jaeyoung Hwang", "Jaewoo Ahn", "Ameya S. Mahabaleshwarkar", "Bilal Kartal", "Pritam Biswas", "Yoshi Suhara", "Kangwook Lee", "Jaewoong Cho"], "year": 2025, "url": "http://arxiv.org/abs/2506.03610v2", "abstract": "Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present Orak, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.", "source": "arxiv", "arxiv_id": "2506.03610v2", "pdf_url": "https://arxiv.org/pdf/2506.03610v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-04T06:40:33Z", "updated": "2025-09-29T01:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OrcaLoca: An LLM Agent Framework for Software Issue Localization", "authors": ["Zhongming Yu", "Hejia Zhang", "Yujie Zhao", "Hanxian Huang", "Matrix Yao", "Ke Ding", "Jishen Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.00350v2", "abstract": "Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration.", "source": "arxiv", "arxiv_id": "2502.00350v2", "pdf_url": "https://arxiv.org/pdf/2502.00350v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-02-01T07:15:03Z", "updated": "2025-10-10T00:02:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap", "authors": ["Andrew Zhu", "Chris Callison-Burch"], "year": 2025, "url": "http://arxiv.org/abs/2509.16325v1", "abstract": "Imagine AI assistants that enhance conversations without interrupting them: quietly providing relevant information during a medical consultation, seamlessly preparing materials as teachers discuss lesson plans, or unobtrusively scheduling meetings as colleagues debate calendars. While modern conversational LLM agents directly assist human users with tasks through a chat interface, we study this alternative paradigm for interacting with LLM agents, which we call \"overhearing agents.\" Rather than demanding the user's attention, overhearing agents continuously monitor ambient activity and intervene only when they can provide contextual assistance. In this paper, we present the first analysis of overhearing LLM agents as a distinct paradigm in human-AI interaction and establish a taxonomy of overhearing agent interactions and tasks grounded in a survey of works on prior LLM-powered agents and exploratory HCI studies. Based on this taxonomy, we create a list of best practices for researchers and developers building overhearing agent systems. Finally, we outline the remaining research gaps and reveal opportunities for future research in the overhearing paradigm.", "source": "arxiv", "arxiv_id": "2509.16325v1", "pdf_url": "https://arxiv.org/pdf/2509.16325v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-19T18:11:04Z", "updated": "2025-09-19T18:11:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search", "authors": ["Yichen He", "Guanhua Huang", "Peiyuan Feng", "Yuan Lin", "Yuchen Zhang", "Hang Li", "Weinan E"], "year": 2025, "url": "http://arxiv.org/abs/2501.10120v2", "abstract": "We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50, and exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.", "source": "arxiv", "arxiv_id": "2501.10120v2", "pdf_url": "https://arxiv.org/pdf/2501.10120v2", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-01-17T11:12:28Z", "updated": "2025-05-27T11:01:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge", "authors": ["Yongrui Chen", "Junhao He", "Linbo Fu", "Shenyu Zhang", "Rihui Jin", "Xinbang Dai", "Jiaqi Li", "Dehai Min", "Nan Hu", "Yuxin Zhang", "Guilin Qi", "Yi Huang", "Tongtong Wu"], "year": 2025, "url": "http://arxiv.org/abs/2504.12734v2", "abstract": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language questions (NLQs) by using structured sources such as tables, databases, and knowledge graphs in a unified way. Existing USKR methods either rely on employing task-specific strategies or custom-defined representations, which struggle to leverage the knowledge transfer between different SKR tasks or align with the prior of LLMs, thereby limiting their performance. This paper proposes a novel USKR framework named \\textsc{Pandora}, which takes advantage of \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge representation for alignment with LLM pre-training. It employs an LLM to generate textual reasoning steps and executable Python code for each question. Demonstrations are drawn from a memory of training examples that cover various SKR tasks, facilitating knowledge transfer. Extensive experiments on four benchmarks involving three SKR tasks demonstrate that \\textsc{Pandora} outperforms existing unified frameworks and competes effectively with task-specific methods.", "source": "arxiv", "arxiv_id": "2504.12734v2", "pdf_url": "https://arxiv.org/pdf/2504.12734v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-17T08:18:09Z", "updated": "2025-09-23T11:15:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "authors": ["Yiran Rex Ma"], "year": 2025, "url": "http://arxiv.org/abs/2508.06110v1", "abstract": "Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models. To approach these issues, we introduce PanelTR, a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach. PanelTR's workflow involves agent scientists conducting individual investigations, engaging in self-review, and participating in collaborative peer-review discussions. This process, driven by five scientist personas, enables semantic-level transfer without relying on data augmentation or parametric optimization. Experiments across four benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data. Our findings indicate that structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context.", "source": "arxiv", "arxiv_id": "2508.06110v1", "pdf_url": "https://arxiv.org/pdf/2508.06110v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "10.1109/IJCNN64981.2025.11228106", "venue": "", "published": "2025-08-08T08:15:52Z", "updated": "2025-08-08T08:15:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search", "authors": ["Joseph Oladokun"], "year": 2025, "url": "http://arxiv.org/abs/2511.18313v1", "abstract": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.", "source": "arxiv", "arxiv_id": "2511.18313v1", "pdf_url": "https://arxiv.org/pdf/2511.18313v1", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-23T06:50:01Z", "updated": "2025-11-23T06:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time", "authors": ["Weizhi Zhang", "Xinyang Zhang", "Chenwei Zhang", "Liangwei Yang", "Jingbo Shang", "Zhepei Wei", "Henry Peng Zou", "Zijie Huang", "Zhengyang Wang", "Yifan Gao", "Xiaoman Pan", "Lian Xiong", "Jingguo Liu", "Philip S. Yu", "Xian Li"], "year": 2025, "url": "http://arxiv.org/abs/2506.06254v1", "abstract": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.", "source": "arxiv", "arxiv_id": "2506.06254v1", "pdf_url": "https://arxiv.org/pdf/2506.06254v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-06T17:29:49Z", "updated": "2025-06-06T17:29:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Personalized Attacks of Social Engineering in Multi-turn Conversations: LLM Agents for Simulation and Detection", "authors": ["Tharindu Kumarage", "Cameron Johnson", "Jadie Adams", "Lin Ai", "Matthias Kirchner", "Anthony Hoogs", "Joshua Garland", "Julia Hirschberg", "Arslan Basharat", "Huan Liu"], "year": 2025, "url": "http://arxiv.org/abs/2503.15552v2", "abstract": "The rapid advancement of conversational agents, particularly chatbots powered by Large Language Models (LLMs), poses a significant risk of social engineering (SE) attacks on social media platforms. SE detection in multi-turn, chat-based interactions is considerably more complex than single-instance detection due to the dynamic nature of these conversations. A critical factor in mitigating this threat is understanding the SE attack mechanisms through which SE attacks operate, specifically how attackers exploit vulnerabilities and how victims' personality traits contribute to their susceptibility. In this work, we propose an LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating multi-turn conversations. We model victim agents with varying personality traits to assess how psychological profiles influence susceptibility to manipulation. Using a dataset of over 1000 simulated conversations, we examine attack scenarios in which adversaries, posing as recruiters, funding agencies, and journalists, attempt to extract sensitive information. Based on this analysis, we present a proof of concept, SE-OmniGuard, to offer personalized protection to users by leveraging prior knowledge of the victims personality, evaluating attack strategies, and monitoring information exchanges in conversations to identify potential SE attempts.", "source": "arxiv", "arxiv_id": "2503.15552v2", "pdf_url": "https://arxiv.org/pdf/2503.15552v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-18T19:14:44Z", "updated": "2025-09-08T21:16:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "authors": ["Keer Lu", "Chong Chen", "Xili Wang", "Bin Cui", "Yunhuai Liu", "Wentao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2508.00344v4", "abstract": "Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.", "source": "arxiv", "arxiv_id": "2508.00344v4", "pdf_url": "https://arxiv.org/pdf/2508.00344v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-01T06:17:11Z", "updated": "2026-01-07T07:33:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant", "authors": ["Gaole He", "Gianluca Demartini", "Ujwal Gadiraju"], "year": 2025, "url": "http://arxiv.org/abs/2502.01390v1", "abstract": "Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of 'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.", "source": "arxiv", "arxiv_id": "2502.01390v1", "pdf_url": "https://arxiv.org/pdf/2502.01390v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "10.1145/3706598.3713218", "venue": "", "published": "2025-02-03T14:23:22Z", "updated": "2025-02-03T14:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Plan-over-Graph: Towards Parallelable LLM Agent Schedule", "authors": ["Shiqi Zhang", "Xinbei Ma", "Zouying Cao", "Zhuosheng Zhang", "Hai Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2502.14563v1", "abstract": "Large Language Models (LLMs) have demonstrated exceptional abilities in reasoning for task planning. However, challenges remain under-explored for parallel schedules. This paper introduces a novel paradigm, plan-over-graph, in which the model first decomposes a real-life textual task into executable subtasks and constructs an abstract task graph. The model then understands this task graph as input and generates a plan for parallel execution. To enhance the planning capability of complex, scalable graphs, we design an automated and controllable pipeline to generate synthetic graphs and propose a two-stage training scheme. Experimental results show that our plan-over-graph method significantly improves task performance on both API-based LLMs and trainable open-sourced LLMs. By normalizing complex tasks as graphs, our method naturally supports parallel execution, demonstrating global efficiency. The code and data are available at https://github.com/zsq259/Plan-over-Graph.", "source": "arxiv", "arxiv_id": "2502.14563v1", "pdf_url": "https://arxiv.org/pdf/2502.14563v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-20T13:47:51Z", "updated": "2025-02-20T13:47:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL", "authors": ["Joey Hong", "Anca Dragan", "Sergey Levine"], "year": 2025, "url": "http://arxiv.org/abs/2505.18098v2", "abstract": "Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.", "source": "arxiv", "arxiv_id": "2505.18098v2", "pdf_url": "https://arxiv.org/pdf/2505.18098v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T16:51:54Z", "updated": "2025-12-03T08:54:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents", "authors": ["Kanika Goswami", "Puneet Mathur", "Ryan Rossi", "Franck Dernoncourt"], "year": 2025, "url": "http://arxiv.org/abs/2501.11233v1", "abstract": "Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity.", "source": "arxiv", "arxiv_id": "2501.11233v1", "pdf_url": "https://arxiv.org/pdf/2501.11233v1", "categories": ["cs.IR", "cs.CL", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-01-20T02:31:52Z", "updated": "2025-01-20T02:31:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents", "authors": ["Wenda Xie", "Chao Guo", "Yanqing Jing. Junle Wang", "Yisheng Lv", "Fei-Yue Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.05188v1", "abstract": "Although LLMs have been widely adopted for creative content generation, a single-pass process often struggles to produce high-quality long narratives. How to effectively revise and improve long narrative scripts like scriptwriters remains a significant challenge, as it demands a comprehensive understanding of the entire context to identify global structural issues and local detailed flaws, as well as coordinating revisions at multiple granularities and locations. Direct modifications by LLMs typically introduce inconsistencies between local edits and the overall narrative requirements. To address these issues, we propose Dramaturge, a task and feature oriented divide-and-conquer approach powered by hierarchical multiple LLM agents. It consists of a Global Review stage to grasp the overall storyline and structural issues, a Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a Hierarchical Coordinated Revision stage that coordinates and integrates structural and detailed improvements throughout the script. The top-down task flow ensures that high-level strategies guide local modifications, maintaining contextual consistency. The review and revision workflow follows a coarse-to-fine iterative process, continuing through multiple rounds until no further substantive improvements can be made. Comprehensive experiments show that Dramaturge significantly outperforms all baselines in terms of script-level overall quality and scene-level details. Our approach is plug-and-play and can be easily integrated into existing methods to improve the generated scripts.", "source": "arxiv", "arxiv_id": "2510.05188v1", "pdf_url": "https://arxiv.org/pdf/2510.05188v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-06T05:20:37Z", "updated": "2025-10-06T05:20:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents", "authors": ["Mathis Pink", "Qinyuan Wu", "Vy Ai Vo", "Javier Turek", "Jianing Mu", "Alexander Huth", "Mariya Toneva"], "year": 2025, "url": "http://arxiv.org/abs/2502.06975v1", "abstract": "As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continually learning and retaining long-term knowledge. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present an episodic memory framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already partially covering these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents.", "source": "arxiv", "arxiv_id": "2502.06975v1", "pdf_url": "https://arxiv.org/pdf/2502.06975v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-10T19:14:51Z", "updated": "2025-02-10T19:14:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives", "authors": ["Elliot Meyerson", "Xin Qiu"], "year": 2025, "url": "http://arxiv.org/abs/2502.04358v2", "abstract": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.", "source": "arxiv", "arxiv_id": "2502.04358v2", "pdf_url": "https://arxiv.org/pdf/2502.04358v2", "categories": ["cs.CL", "cs.AI", "cs.CC", "cs.LG", "cs.NE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-04T20:47:43Z", "updated": "2025-05-29T16:46:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Position: Stop Acting Like Language Model Agents Are Normal Agents", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "year": 2025, "url": "http://arxiv.org/abs/2502.10420v1", "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.", "source": "arxiv", "arxiv_id": "2502.10420v1", "pdf_url": "https://arxiv.org/pdf/2502.10420v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-04T08:14:18Z", "updated": "2025-02-04T08:14:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "year": 2025, "url": "http://arxiv.org/abs/2505.22655v1", "abstract": "Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.", "source": "arxiv", "arxiv_id": "2505.22655v1", "pdf_url": "https://arxiv.org/pdf/2505.22655v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-28T17:59:08Z", "updated": "2025-05-28T17:59:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis", "authors": ["Zhonghao Zhan", "Huichi Zhou", "Hamed Haddadi"], "year": 2025, "url": "http://arxiv.org/abs/2506.20806v1", "abstract": "Graph Neural Networks (GNNs) show great promise for Network Intrusion Detection Systems (NIDS), particularly in IoT environments, but suffer performance degradation due to distribution drift and lack robustness against realistic adversarial attacks. Current robustness evaluations often rely on unrealistic synthetic perturbations and lack demonstrations on systematic analysis of different kinds of adversarial attack, which encompass both black-box and white-box scenarios. This work proposes a novel approach to enhance GNN robustness and generalization by employing Large Language Models (LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These agents scrutinize graph structures derived from network flow data, identifying and potentially mitigating suspicious or adversarially perturbed elements before GNN processing. Our experiments, using a framework designed for realistic evaluation and testing with a variety of adversarial attacks including a dataset collected from physical testbed experiments, demonstrate that integrating LLM analysis can significantly improve the resilience of GNN-based NIDS against challenges, showcasing the potential of LLM agent as a complementary layer in intrusion detection architectures.", "source": "arxiv", "arxiv_id": "2506.20806v1", "pdf_url": "https://arxiv.org/pdf/2506.20806v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-25T19:49:55Z", "updated": "2025-06-25T19:49:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun", "Jiali Wei"], "year": 2025, "url": "http://arxiv.org/abs/2508.00500v2", "abstract": "Large Language Model (LLM) agents demonstrate strong autonomy, but their stochastic behavior introduces unpredictable safety risks. Existing rule-based enforcement systems, such as AgentSpec, are reactive, intervening only when unsafe behavior is imminent or has occurred, lacking foresight for long-horizon dependencies. To overcome these limitations, we present a proactive runtime enforcement framework for LLM agents. The framework abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it predicts the probability of leading to undesired behaviors and intervenes before violations occur when the estimated risk exceeds a user-defined threshold. Designed to provide PAC-correctness guarantee, the framework achieves statistically reliable enforcement of agent safety. We evaluate the framework across two safety-critical domains: autonomous vehicles and embodied agents. It proactively enforces safety and maintains high task performance, outperforming existing methods.", "source": "arxiv", "arxiv_id": "2508.00500v2", "pdf_url": "https://arxiv.org/pdf/2508.00500v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-01T10:24:47Z", "updated": "2026-01-06T03:51:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "year": 2025, "url": "http://arxiv.org/abs/2512.06721v1", "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "source": "arxiv", "arxiv_id": "2512.06721v1", "pdf_url": "https://arxiv.org/pdf/2512.06721v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-07T08:21:07Z", "updated": "2025-12-07T08:21:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents", "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "year": 2025, "url": "http://arxiv.org/abs/2510.18476v1", "abstract": "We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2510.18476v1", "pdf_url": "https://arxiv.org/pdf/2510.18476v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-21T09:54:44Z", "updated": "2025-10-21T09:54:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Procedural Environment Generation for Tool-Use Agents", "authors": ["Michael Sullivan", "Mareike Hartmann", "Alexander Koller"], "year": 2025, "url": "http://arxiv.org/abs/2506.11045v2", "abstract": "Although the power of LLM tool-use agents has ignited a flurry of recent research in this area, the curation of tool-use training data remains an open problem$-$especially for online RL training. Existing approaches to synthetic tool-use data generation tend to be non-interactive, and/or non-compositional. We introduce RandomWorld, a pipeline for the procedural generation of interactive tools and compositional tool-use data. We show that models tuned via SFT and RL on synthetic RandomWorld data improve on a range of tool-use benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset. Further experiments show that downstream performance scales with the amount of RandomWorld-generated training data, opening up the possibility of further improvement through the use of entirely synthetic data.", "source": "arxiv", "arxiv_id": "2506.11045v2", "pdf_url": "https://arxiv.org/pdf/2506.11045v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-21T14:10:06Z", "updated": "2025-09-24T14:57:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Process Reward Models for LLM Agents: Practical Framework and Directions", "authors": ["Sanjiban Choudhury"], "year": 2025, "url": "http://arxiv.org/abs/2502.10325v1", "abstract": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: https://github.com/sanjibanc/agent_prm.", "source": "arxiv", "arxiv_id": "2502.10325v1", "pdf_url": "https://arxiv.org/pdf/2502.10325v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-14T17:34:28Z", "updated": "2025-02-14T17:34:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents", "authors": ["Weiting Tan", "Xinghua Qu", "Ming Tu", "Meng Ge", "Andy T. Liu", "Philipp Koehn", "Lu Lu"], "year": 2025, "url": "http://arxiv.org/abs/2509.14480v1", "abstract": "Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $Ï$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.", "source": "arxiv", "arxiv_id": "2509.14480v1", "pdf_url": "https://arxiv.org/pdf/2509.14480v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-17T23:25:00Z", "updated": "2025-09-17T23:25:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Progent: Programmable Privilege Control for LLM Agents", "authors": ["Tianneng Shi", "Jingxuan He", "Zhun Wang", "Hongwei Li", "Linyu Wu", "Wenbo Guo", "Dawn Song"], "year": 2025, "url": "http://arxiv.org/abs/2504.11703v2", "abstract": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Attackers can exploit these agents through various vectors, including indirect prompt injection, memory/knowledge base poisoning, and malicious tools, tricking agents into performing dangerous actions such as unauthorized financial transactions or data leakage. The core problem that enables attacks to succeed lies in over-privileged tool access. We introduce Progent, the first privilege control framework to secure LLM agents. Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Progent features a domain-specific language that allows for expressing fine-grained policies for controlling tool privileges, flexible fallback actions when calls are blocked, and dynamic policy updates to adapt to changing agent states. The framework operates deterministically at runtime, providing provable security guarantees. Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.", "source": "arxiv", "arxiv_id": "2504.11703v2", "pdf_url": "https://arxiv.org/pdf/2504.11703v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-16T01:58:40Z", "updated": "2025-08-30T06:42:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents", "authors": ["Juhee Kim", "Woohyuk Choi", "Byoungyoung Lee"], "year": 2025, "url": "http://arxiv.org/abs/2503.15547v2", "abstract": "Large Language Models (LLMs) are combined with tools to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent's behavior is determined at runtime by natural language prompts from either user or tool's data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompts are prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques -- i.e., agent isolation, secure untrusted data processing, and privilege escalation guardrails. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents.", "source": "arxiv", "arxiv_id": "2503.15547v2", "pdf_url": "https://arxiv.org/pdf/2503.15547v2", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-03-17T05:27:57Z", "updated": "2025-04-21T02:10:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Prompt Injection Attack to Tool Selection in LLM Agents", "authors": ["Jiawen Shi", "Zenghui Yuan", "Guiyao Tie", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "year": 2025, "url": "http://arxiv.org/abs/2504.19793v3", "abstract": "Tool selection is a key component of LLM agents. A popular approach follows a two-step process - \\emph{retrieval} and \\emph{selection} - to pick the most appropriate tool from a tool library for a given task. In this work, we introduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool selection in no-box scenarios. ToolHijacker injects a malicious tool document into the tool library to manipulate the LLM agent's tool selection process, compelling it to consistently choose the attacker's malicious tool for an attacker-chosen target task. Specifically, we formulate the crafting of such tool documents as an optimization problem and propose a two-phase optimization strategy to solve it. Our extensive experimental evaluation shows that ToolHijacker is highly effective, significantly outperforming existing manual-based and automated prompt injection attacks when applied to tool selection. Moreover, we explore various defenses, including prevention-based defenses (StruQ and SecAlign) and detection-based defenses (known-answer detection, DataSentinel, perplexity detection, and perplexity windowed detection). Our experimental results indicate that these defenses are insufficient, highlighting the urgent need for developing new defense strategies.", "source": "arxiv", "arxiv_id": "2504.19793v3", "pdf_url": "https://arxiv.org/pdf/2504.19793v3", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-28T13:36:43Z", "updated": "2025-08-24T03:28:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "year": 2025, "url": "http://arxiv.org/abs/2510.07064v1", "abstract": "The difficulty and expense of obtaining large-scale human responses make Large Language Models (LLMs) an attractive alternative and a promising proxy for human behavior. However, prior work shows that LLMs often produce homogeneous outputs that fail to capture the rich diversity of human perspectives and behaviors. Thus, rather than trying to capture this diversity with a single LLM agent, we propose a novel framework to construct a set of agents that collectively capture the diversity of a given human population. Each agent is an LLM whose behavior is steered by conditioning on a small set of human demonstrations (task-response pairs) through in-context learning. The central challenge is therefore to select a representative set of LLM agents from the exponentially large space of possible agents. We tackle this selection problem from the lens of submodular optimization. In particular, we develop methods that offer different trade-offs regarding time complexity and performance guarantees. Extensive experiments in crowdsourcing and educational domains demonstrate that our approach constructs agents that more effectively represent human populations compared to baselines. Moreover, behavioral analyses on new tasks show that these agents reproduce the behavior patterns and perspectives of the students and annotators they are designed to represent.", "source": "arxiv", "arxiv_id": "2510.07064v1", "pdf_url": "https://arxiv.org/pdf/2510.07064v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-08T14:28:53Z", "updated": "2025-10-08T14:28:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Prompt template for a fictitious LLM agent in a content-flagging experiment", "authors": ["Marie-Therese Sekwenz", "Daria Simons", "Alina Wundsam"], "year": 2025, "url": "http://arxiv.org/abs/2507.21842v1", "abstract": "Digital regulations such as the European Union's Digital Services Act (DSA) represent major efforts to shape human-centered and human rights-based frameworks for society. Yet, as these laws are translated into practice, challenges emerge at the intersection of technology, law, and design. This paper presents a qualitative case study examining how designers act as mediators between abstract legal requirements and real-world digital experiences for users, focusing on the design of content reporting mechanisms under Article 16 of the DSA.\n  Through an expert workshop with professional designers from diverse fields (N=9), we explore how legal obligations are interpreted by designers and reflected in discussions and design solutions. Our findings resonate with previous research on the design of reporting mechanisms and dark patterns, highlighting how UX design choices can mislead or hinder users' decision-making and therefore also highlighting the crucial role of design decisions.\n  We show how participatory design methods can bridge disciplinary divides, making legal obligations accessible in compliance fostering design solutions.\n  By using legal design as a lens, we argue that the co-creation of digital regulations and user experience is a core site for digital humanism; where designers, engineers, and legal scholars must collaborate to ensure that systems uphold legal standards to address the challenge the regulation poses to these disciplines.", "source": "arxiv", "arxiv_id": "2507.21842v1", "pdf_url": "https://arxiv.org/pdf/2507.21842v1", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2025-07-29T14:21:00Z", "updated": "2025-07-29T14:21:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression", "authors": ["Boris Kriuk", "Logic Ng"], "year": 2025, "url": "http://arxiv.org/abs/2512.17914v1", "abstract": "Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.", "source": "arxiv", "arxiv_id": "2512.17914v1", "pdf_url": "https://arxiv.org/pdf/2512.17914v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-27T10:45:41Z", "updated": "2025-11-27T10:45:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "authors": ["Yuyang Song", "Hanxu Yan", "Jiale Lao", "Yibo Wang", "Yufei Li", "Yuanchun Zhou", "Jianguo Wang", "Mingjie Tang"], "year": 2025, "url": "http://arxiv.org/abs/2506.07675v3", "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.", "source": "arxiv", "arxiv_id": "2506.07675v3", "pdf_url": "https://arxiv.org/pdf/2506.07675v3", "categories": ["cs.DB", "cs.AI"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-06-09T11:51:27Z", "updated": "2026-01-02T16:51:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery", "authors": ["Namkyeong Lee", "Edward De Brouwer", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Chanyoung Park", "Gabriele Scalia"], "year": 2025, "url": "http://arxiv.org/abs/2502.17506v3", "abstract": "Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery. However, the specialized nature of biochemical data often necessitates costly domain-specific fine-tuning, posing major challenges. First, it hinders the application of more flexible general-purpose LLMs for cutting-edge drug discovery tasks. More importantly, it limits the rapid integration of the vast amounts of scientific data continuously generated through experiments and research. Compounding these challenges is the fact that real-world scientific questions are typically complex and open-ended, requiring reasoning beyond pattern matching or static knowledge retrieval.To address these challenges, we propose CLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored to drug discovery tasks. Through the collaboration of multiple LLM agents, CLADD dynamically retrieves information from biomedical knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses - all without the need for domain-specific fine-tuning. Crucially, we tackle key obstacles in applying RAG workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration. We demonstrate the flexibility and effectiveness of this framework across a variety of drug discovery tasks, showing that it outperforms general-purpose and domain-specific LLMs as well as traditional deep learning approaches. Our code is publicly available at https://github.com/Genentech/CLADD.", "source": "arxiv", "arxiv_id": "2502.17506v3", "pdf_url": "https://arxiv.org/pdf/2502.17506v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-22T00:12:52Z", "updated": "2025-11-13T21:34:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Xing Jin", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.20073v2", "abstract": "Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on four stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and gradient stabilization. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.", "source": "arxiv", "arxiv_id": "2504.20073v2", "pdf_url": "https://arxiv.org/pdf/2504.20073v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-24T17:57:08Z", "updated": "2025-05-26T17:19:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action Issue Detection, Explanation and Recovery", "authors": ["Silvia Izquierdo-Badiola", "Carlos Rizzo", "Guillem AlenyÃ "], "year": 2025, "url": "http://arxiv.org/abs/2503.17703v2", "abstract": "As robots increasingly operate in dynamic human-centric environments, improving their ability to detect, explain, and recover from action-related issues becomes crucial. Traditional model-based and data-driven techniques lack adaptability, while more flexible generative AI methods struggle with grounding extracted information to real-world constraints. We introduce RAIDER, a novel agent that integrates Large Language Models (LLMs) with grounded tools for adaptable and efficient issue detection and explanation. Using a unique \"Ground, Ask&Answer, Issue\" procedure, RAIDER dynamically generates context-aware precondition questions and selects appropriate tools for resolution, achieving targeted information gathering. Our results within a simulated household environment surpass methods relying on predefined models, full scene descriptions, or standalone trained models. Additionally, RAIDER's explanations enhance recovery success, including cases requiring human interaction. Its modular architecture, featuring self-correction mechanisms, enables straightforward adaptation to diverse scenarios, as demonstrated in a real-world human-assistive task. This showcases RAIDER's potential as a versatile agentic AI solution for robotic issue detection and explanation, while addressing the problem of grounding generative AI for its effective application in embodied agents. Project website: https://eurecat.github.io/raider-llmagent/", "source": "arxiv", "arxiv_id": "2503.17703v2", "pdf_url": "https://arxiv.org/pdf/2503.17703v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-03-22T09:03:31Z", "updated": "2025-04-04T15:38:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments", "authors": ["Yuchuan Fu", "Xiaohan Yuan", "Dongxia Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.15253v1", "abstract": "The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at https://github.com/lanzer-tree/RAS-Eval.", "source": "arxiv", "arxiv_id": "2506.15253v1", "pdf_url": "https://arxiv.org/pdf/2506.15253v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-18T08:30:36Z", "updated": "2025-06-18T08:30:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RELATE-Sim: Leveraging Turning Point Theory and LLM Agents to Predict and Understand Long-Term Relationship Dynamics through Interactive Narrative Simulations", "authors": ["Matthew Yue", "Zhikun Xu", "Vivek Gupta", "Thao Ha", "Liesal Sharabi", "Ben Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2510.00414v1", "abstract": "Most dating technologies optimize for getting together, not staying together. We present RELATE-Sim, a theory-grounded simulator that models how couples behave at consequential turning points-exclusivity talks, conflict-and-repair episodes, relocations-rather than static traits. Two persona-aligned LLM agents (one per partner) interact under a centralized Scene Master that frames each turning point as a compact set of realistic options, advances the narrative, and infers interpretable state changes and an auditable commitment estimate after each scene. On a longitudinal dataset of 71 couples with two-year follow-ups, simulation-aware predictions outperform a personas-only baseline while surfacing actionable markers (e.g., repair attempts acknowledged, clarity shifts) that explain why trajectories diverge. RELATE-Sim pushes the relationship research's focus from matchmaking to maintenance, providing a transparent, extensible platform for understanding and forecasting long-term relationship dynamics.", "source": "arxiv", "arxiv_id": "2510.00414v1", "pdf_url": "https://arxiv.org/pdf/2510.00414v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-10-01T01:50:15Z", "updated": "2025-10-01T01:50:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing", "authors": ["Binger Chen", "Tacettin Emre BÃ¶k", "Behnood Rasti", "Volker Markl", "BegÃ¼m Demir"], "year": 2025, "url": "http://arxiv.org/abs/2511.17442v1", "abstract": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.", "source": "arxiv", "arxiv_id": "2511.17442v1", "pdf_url": "https://arxiv.org/pdf/2511.17442v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-11-21T17:41:26Z", "updated": "2025-11-21T17:41:26Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RFCAudit: An LLM Agent for Functional Bug Detection in Network Protocols", "authors": ["Mingwei Zheng", "Chengpeng Wang", "Xuwei Liu", "Jinyao Guo", "Shiwei Feng", "Xiangyu Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.00714v2", "abstract": "Functional correctness is critical for ensuring the reliability and security of network protocol implementations. Functional bugs, instances where implementations diverge from behaviors specified in RFC documents, can lead to severe consequences, including faulty routing, authentication bypasses, and service disruptions. Detecting these bugs requires deep semantic analysis across specification documents and source code, a task beyond the capabilities of traditional static analysis tools. This paper introduces RFCAudit, an autonomous agent that leverages large language models (LLMs) to detect functional bugs by checking conformance between network protocol implementations and their RFC specifications. Inspired by the human auditing procedure, RFCAudit comprises two key components: an indexing agent and a detection agent. The former hierarchically summarizes protocol code semantics, generating semantic indexes that enable the detection agent to narrow down the scanning scope. The latter employs demand-driven retrieval to iteratively collect additional relevant data structures and functions, eventually identifying potential inconsistencies with the RFC specifications effectively. We evaluate RFCAudit across six real-world network protocol implementations. RFCAudit identifies 47 functional bugs with 81.9% precision, of which 20 bugs have been confirmed or fixed by developers.", "source": "arxiv", "arxiv_id": "2506.00714v2", "pdf_url": "https://arxiv.org/pdf/2506.00714v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-31T21:13:19Z", "updated": "2025-10-04T06:53:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories", "authors": ["Zilong Wang", "Jingfeng Yang", "Sreyashi Nag", "Samarth Varshney", "Xianfeng Tang", "Haoming Jiang", "Jingbo Shang", "Sheikh Muhammad Sarwar"], "year": 2025, "url": "http://arxiv.org/abs/2505.20737v1", "abstract": "Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents sensitive to the outcome of certain key steps which makes them likely to fail the task because of a subtle mistake in the planning trajectory. Recent approaches resort to calibrating the reasoning process through reinforcement learning. They reward or penalize every reasoning step with process supervision, as known as Process Reward Models (PRMs). However, PRMs are difficult and costly to scale up with a large number of next action candidates since they require extensive computations to acquire the training data through the per-step trajectory exploration. To mitigate this issue, we focus on the relative reward trend across successive reasoning steps and propose maintaining an increasing reward in the collected trajectories for process supervision, which we term Reward Rising Optimization (RRO). Specifically, we incrementally augment the process supervision until identifying a step exhibiting positive reward differentials, i.e. rising rewards, relative to its preceding iteration. This method dynamically expands the search space for the next action candidates, efficiently capturing high-quality data. We provide mathematical groundings and empirical results on the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO achieves superior performance while requiring much less exploration cost.", "source": "arxiv", "arxiv_id": "2505.20737v1", "pdf_url": "https://arxiv.org/pdf/2505.20737v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-27T05:27:54Z", "updated": "2025-05-27T05:27:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale", "authors": ["Jason Holmes", "Yuexing Hao", "Mariana Borras-Osorio", "Federico Mastroleo", "Santiago Romero Brufau", "Valentina Carducci", "Katie M Van Abel", "David M Routman", "Andrew Y. K. Foong", "Liv M Muller", "Satomi Shiraishi", "Daniel K Ebner", "Daniel J Ma", "Sameer R Keole", "Samir H Patel", "Mirek Fatyga", "Martin Bues", "Brad J Stish", "Yolanda I Garces", "Michelle A Neben Wittich", "Robert L Foote", "Sujay A Vora", "Nadia N Laack", "Mark R Waddle", "Wei Liu"], "year": 2025, "url": "http://arxiv.org/abs/2509.25540v2", "abstract": "Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.", "source": "arxiv", "arxiv_id": "2509.25540v2", "pdf_url": "https://arxiv.org/pdf/2509.25540v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T21:55:50Z", "updated": "2025-12-12T22:32:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning", "authors": ["Jae-Woo Choi", "Hyungmin Kim", "Hyobin Ong", "Minsu Jang", "Dohyung Kim", "Jaehong Kim", "Youngwoo Yoon"], "year": 2025, "url": "http://arxiv.org/abs/2511.02424v1", "abstract": "Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.", "source": "arxiv", "arxiv_id": "2511.02424v1", "pdf_url": "https://arxiv.org/pdf/2511.02424v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-04T09:55:40Z", "updated": "2025-11-04T09:55:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space", "authors": ["Xingfu Zhou", "Pengfei Wang"], "year": 2025, "url": "http://arxiv.org/abs/2512.14448v1", "abstract": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.", "source": "arxiv", "arxiv_id": "2512.14448v1", "pdf_url": "https://arxiv.org/pdf/2512.14448v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-16T14:34:10Z", "updated": "2025-12-16T14:34:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection", "authors": ["Jeonghye Kim", "Sojeong Rhee", "Minbeom Kim", "Dohyung Kim", "Sangmook Lee", "Youngchul Sung", "Kyomin Jung"], "year": 2025, "url": "http://arxiv.org/abs/2505.15182v2", "abstract": "Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.", "source": "arxiv", "arxiv_id": "2505.15182v2", "pdf_url": "https://arxiv.org/pdf/2505.15182v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-21T06:57:39Z", "updated": "2025-09-28T17:14:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization", "authors": ["Dong Qiu", "Duo Xu", "Limengxi Yue"], "year": 2025, "url": "http://arxiv.org/abs/2512.24609v1", "abstract": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.", "source": "arxiv", "arxiv_id": "2512.24609v1", "pdf_url": "https://arxiv.org/pdf/2512.24609v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T03:59:18Z", "updated": "2025-12-31T03:59:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Reward Design", "authors": ["Quan Wei", "Siliang Zeng", "Chenliang Li", "William Brown", "Oana Frunza", "Wei Deng", "Anderson Schneider", "Yuriy Nevmyvaka", "Yang Katie Zhao", "Alfredo Garcia", "Mingyi Hong"], "year": 2025, "url": "http://arxiv.org/abs/2505.11821v2", "abstract": "This paper investigates Reinforcement Learning (RL) approaches to enhance the reasoning capabilities of Large Language Model (LLM) agents in long-horizon, multi-turn scenarios. Although RL algorithms such as Group Relative Policy Optimization (GRPO) and Proximal Policy Optimization (PPO) have been widely applied to train multi-turn LLM agents, they typically rely only on sparse outcome rewards and lack dense intermediate signals across multiple decision steps, limiting their performance on complex reasoning tasks. To bridge this gap, we present the first systematic study of \\textit{turn-level reward design} for multi-turn RL algorithms and agent applications. By integrating turn-level rewards, we extend GRPO and PPO to their respective multi-turn variants, enabling fine-grained credit assignment. We conduct case studies on multi-turn reasoning-augmented search agents, where we carefully design two types of turn-level rewards: verifiable and LLM-as-judge. Our experiments on multi-turn search tasks demonstrate that incorporating well-designed turn-level rewards enables RL algorithms to significantly outperform baseline methods with trajectory-level rewards. Both training and validation reward curves illustrate that our method achieves \\textit{greater stability}, \\textit{faster convergence}, and \\textit{higher accuracy}. Numerical results across diverse question-answering datasets further show that our approach consistently delivers highest answer correctness and 100\\% format correctness.", "source": "arxiv", "arxiv_id": "2505.11821v2", "pdf_url": "https://arxiv.org/pdf/2505.11821v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-17T04:09:46Z", "updated": "2025-10-23T04:32:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19461v1", "abstract": "We stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information). To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work. Our empirical results yield three key findings. First, agent awareness dominates monitor awareness: an agent's knowledge that it is being monitored substantially degrades the monitor's reliability. On the contrary, providing the monitor with more information about the agent is less helpful than expected. Second, monitor scaffolding matters more than monitor awareness: the hybrid scaffolding consistently outperforms baseline monitor scaffolding, and can enable weaker models to reliably monitor stronger agents -- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective; escalating only pre-flagged cases to human reviewers improved the TPR by approximately 15% at FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research.", "source": "arxiv", "arxiv_id": "2508.19461v1", "pdf_url": "https://arxiv.org/pdf/2508.19461v1", "categories": ["cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-26T22:29:31Z", "updated": "2025-08-26T22:29:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents", "authors": ["Sid Black", "Asa Cooper Stickland", "Jake Pencharz", "Oliver Sourbut", "Michael Schmatz", "Jay Bailey", "Ollie Matthews", "Ben Millwood", "Alex Remedios", "Alan Cooney"], "year": 2025, "url": "http://arxiv.org/abs/2504.18565v2", "abstract": "Uncontrollable autonomous replication of language model agents poses a critical safety risk. To better understand this risk, we introduce RepliBench, a suite of evaluations designed to measure autonomous replication capabilities. RepliBench is derived from a decomposition of these capabilities covering four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on this compute for long periods. We create 20 novel task families consisting of 86 individual tasks. We benchmark 5 frontier models, and find they do not currently pose a credible threat of self-replication, but succeed on many components and are improving rapidly. Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups, but struggle to pass KYC checks or set up robust and persistent agent deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50% pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20 families on the hardest variants. These findings suggest autonomous replication capability could soon emerge with improvements in these remaining areas or with human assistance.", "source": "arxiv", "arxiv_id": "2504.18565v2", "pdf_url": "https://arxiv.org/pdf/2504.18565v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-04-21T11:39:22Z", "updated": "2025-05-05T20:52:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles", "authors": ["Xinhang Li", "Qing Guo", "Junyu Chen", "Zheng Guo", "Shengzhe Xu", "Lei Li", "Lin Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2510.26242v1", "abstract": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.", "source": "arxiv", "arxiv_id": "2510.26242v1", "pdf_url": "https://arxiv.org/pdf/2510.26242v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-30T08:23:08Z", "updated": "2025-10-30T08:23:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "year": 2025, "url": "http://arxiv.org/abs/2505.11807v2", "abstract": "Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline ''retrospection'' process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.", "source": "arxiv", "arxiv_id": "2505.11807v2", "pdf_url": "https://arxiv.org/pdf/2505.11807v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2024.emnlp-main.268", "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 4650-4666, ACL Anthology, 2024", "published": "2025-05-17T03:28:24Z", "updated": "2025-05-27T01:30:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration", "authors": ["Kostas Hatalis", "Despina Christou", "Vyshnavi Kondapalli"], "year": 2025, "url": "http://arxiv.org/abs/2504.06943v2", "abstract": "Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.", "source": "arxiv", "arxiv_id": "2504.06943v2", "pdf_url": "https://arxiv.org/pdf/2504.06943v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-09T14:51:02Z", "updated": "2025-04-11T05:34:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents", "authors": ["Gonzalo Gonzalez-Pumariega", "Leong Su Yean", "Neha Sunkara", "Sanjiban Choudhury"], "year": 2025, "url": "http://arxiv.org/abs/2502.05227v1", "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at https://github.com/portal-cornell/robotouille.", "source": "arxiv", "arxiv_id": "2502.05227v1", "pdf_url": "https://arxiv.org/pdf/2502.05227v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-02-06T05:50:37Z", "updated": "2025-02-06T05:50:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "year": 2025, "url": "http://arxiv.org/abs/2507.14447v2", "abstract": "The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. To address this, this paper introduces Routine, a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an accuracy increase to 88.2% on scenario-specific evaluations, indicating improved adherence to execution plans. In addition, we employed Routine-based distillation to create a scenario-specific, multi-step tool-calling dataset. Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%, approaching GPT-4o's performance. These results highlight Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing model adaptability to new scenarios. Our experimental results demonstrate that Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.", "source": "arxiv", "arxiv_id": "2507.14447v2", "pdf_url": "https://arxiv.org/pdf/2507.14447v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-19T02:46:19Z", "updated": "2025-07-22T10:01:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents", "authors": ["Alejandro Cuadron", "Pengfei Yu", "Yang Liu", "Arpit Gupta"], "year": 2025, "url": "http://arxiv.org/abs/2512.07850v1", "abstract": "Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \\emph{do all actions contribute equally to failure?} Analyzing execution traces on $Ï$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \\emph{mutating} (environment-changing) vs.\\ non-mutating steps and formalize \\emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\\%$ on Airline and upto $96\\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \\cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \\emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \\cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\\% \\emph{relative} on Airline, +11\\% on Retail, and +7\\% on SWE-Bench Verified; Claude: +9\\%/+7\\%. We further identify ceiling effects in $Ï$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $Ï$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.", "source": "arxiv", "arxiv_id": "2512.07850v1", "pdf_url": "https://arxiv.org/pdf/2512.07850v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-26T01:28:22Z", "updated": "2025-11-26T01:28:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "authors": ["Yu Xia", "Yiran Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "year": 2025, "url": "http://arxiv.org/abs/2507.07441v2", "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "source": "arxiv", "arxiv_id": "2507.07441v2", "pdf_url": "https://arxiv.org/pdf/2507.07441v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-10T05:38:15Z", "updated": "2025-08-20T22:10:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention", "authors": ["Chengshuai Zhao", "Zhen Tan", "Chau-Wai Wong", "Xinyan Zhao", "Tianlong Chen", "Huan Liu"], "year": 2025, "url": "http://arxiv.org/abs/2502.10937v2", "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.", "source": "arxiv", "arxiv_id": "2502.10937v2", "pdf_url": "https://arxiv.org/pdf/2502.10937v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-16T00:19:07Z", "updated": "2025-07-06T00:55:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks", "authors": ["Hwiwon Lee", "Ziqi Zhang", "Hanxiao Lu", "Lingming Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2506.11791v2", "abstract": "Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.", "source": "arxiv", "arxiv_id": "2506.11791v2", "pdf_url": "https://arxiv.org/pdf/2506.11791v2", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-13T13:54:30Z", "updated": "2025-10-22T16:27:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents", "authors": ["Simon Sinong Zhan", "Yao Liu", "Philip Wang", "Zinan Wang", "Qineng Wang", "Zhian Ruan", "Xiangyu Shi", "Xinyu Cao", "Frank Yang", "Kangrui Wang", "Huajie Shao", "Manling Li", "Qi Zhu"], "year": 2025, "url": "http://arxiv.org/abs/2510.12985v1", "abstract": "We present Sentinel, the first framework for formally evaluating the physical safety of Large Language Model(LLM-based) embodied agents across the semantic, plan, and trajectory levels. Unlike prior methods that rely on heuristic rules or subjective LLM judgments, Sentinel grounds practical safety requirements in formal temporal logic (TL) semantics that can precisely specify state invariants, temporal dependencies, and timing constraints. It then employs a multi-level verification pipeline where (i) at the semantic level, intuitive natural language safety requirements are formalized into TL formulas and the LLM agent's understanding of these requirements is probed for alignment with the TL formulas; (ii) at the plan level, high-level action plans and subgoals generated by the LLM agent are verified against the TL formulas to detect unsafe plans before execution; and (iii) at the trajectory level, multiple execution trajectories are merged into a computation tree and efficiently verified against physically-detailed TL specifications for a final safety check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate multiple LLM-based embodied agents against diverse safety requirements. Our experiments show that by grounding physical safety in temporal logic and applying verification methods across multiple levels, Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents in physical environments, exposing safety violations overlooked by previous methods and offering insights into their failure modes.", "source": "arxiv", "arxiv_id": "2510.12985v1", "pdf_url": "https://arxiv.org/pdf/2510.12985v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-10-14T20:53:51Z", "updated": "2025-10-14T20:53:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents", "authors": ["Jonathan Kutasov", "Yuqi Sun", "Paul Colognese", "Teun van der Weij", "Linda Petrini", "Chen Bo Calvin Zhang", "John Hughes", "Xiang Deng", "Henry Sleight", "Tyler Tracy", "Buck Shlegeris", "Joe Benton"], "year": 2025, "url": "http://arxiv.org/abs/2506.15740v2", "abstract": "As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex and long horizon settings, it is critical to evaluate their ability to sabotage users by pursuing hidden objectives. We study the ability of frontier LLMs to evade monitoring and achieve harmful hidden goals while completing a wide array of realistic tasks. We evaluate a broad range of frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena, the first highly diverse agent evaluation dataset for sabotage and monitoring capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign main tasks and harmful side objectives in complicated environments. Agents are evaluated on their ability to complete the side task without appearing suspicious to an LLM monitor. When measuring agent ability to (a) complete the main task, (b) complete the side task, and (c) avoid detection, we find that the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For current frontier models, success on the side task relies heavily on having access to a hidden scratchpad that is not visible to the monitor. We also use SHADE-Arena to measure models' monitoring abilities, with the top monitor (Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign transcripts. We find that for now, models still struggle at sabotage due to failures in long-context main task execution. However, our measurements already demonstrate the difficulty of monitoring for subtle sabotage attempts, which we expect to only increase in the face of more complex and longer-horizon tasks.", "source": "arxiv", "arxiv_id": "2506.15740v2", "pdf_url": "https://arxiv.org/pdf/2506.15740v2", "categories": ["cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-17T15:46:15Z", "updated": "2025-07-08T21:23:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning", "authors": ["Kaiwen Zhou", "Ahmed Elgohary", "A S M Iftekhar", "Amin Saied"], "year": 2025, "url": "http://arxiv.org/abs/2510.26037v1", "abstract": "The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.", "source": "arxiv", "arxiv_id": "2510.26037v1", "pdf_url": "https://arxiv.org/pdf/2510.26037v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-10-30T00:32:58Z", "updated": "2025-10-30T00:32:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "year": 2025, "url": "http://arxiv.org/abs/2505.20732v1", "abstract": "Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\\% on average) and grounding accuracy (+1.9\\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.", "source": "arxiv", "arxiv_id": "2505.20732v1", "pdf_url": "https://arxiv.org/pdf/2505.20732v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-27T05:21:04Z", "updated": "2025-05-27T05:21:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking", "authors": ["Yujin Roh", "Inho Jake Park", "Chigon Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2512.20975v2", "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.", "source": "arxiv", "arxiv_id": "2512.20975v2", "pdf_url": "https://arxiv.org/pdf/2512.20975v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-12-24T06:04:58Z", "updated": "2026-01-14T14:06:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent", "authors": ["Keyeun Lee", "Seo Hyeong Kim", "Seolhee Lee", "Jinsu Eun", "Yena Ko", "Hayeon Jeon", "Esther Hehsun Kim", "Seonghye Cho", "Soeun Yang", "Eun-mee Kim", "Hajin Lim"], "year": 2025, "url": "http://arxiv.org/abs/2502.08599v1", "abstract": "Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual's multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum's effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)-derived from short essays on preferences and daily routines-modeled characters' identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies.", "source": "arxiv", "arxiv_id": "2502.08599v1", "pdf_url": "https://arxiv.org/pdf/2502.08599v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-12T17:38:27Z", "updated": "2025-02-12T17:38:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents", "authors": ["Jing-Jing Li", "Jianfeng He", "Chao Shang", "Devang Kulshreshtha", "Xun Xian", "Yi Zhang", "Hang Su", "Sandesh Swamy", "Yanjun Qi"], "year": 2025, "url": "http://arxiv.org/abs/2509.25624v1", "abstract": "As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC's automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.", "source": "arxiv", "arxiv_id": "2509.25624v1", "pdf_url": "https://arxiv.org/pdf/2509.25624v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-30T00:31:44Z", "updated": "2025-09-30T00:31:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "STELLA: Self-Evolving LLM Agent for Biomedical Research", "authors": ["Ruofan Jin", "Zaixi Zhang", "Mengdi Wang", "Le Cong"], "year": 2025, "url": "http://arxiv.org/abs/2507.02004v1", "abstract": "The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench: DBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.", "source": "arxiv", "arxiv_id": "2507.02004v1", "pdf_url": "https://arxiv.org/pdf/2507.02004v1", "categories": ["cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-01T20:52:01Z", "updated": "2025-07-01T20:52:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning", "authors": ["Hanlin Wang", "Jian Wang", "Chak Tou Leong", "Wenjie Li"], "year": 2025, "url": "http://arxiv.org/abs/2502.14276v2", "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.", "source": "arxiv", "arxiv_id": "2502.14276v2", "pdf_url": "https://arxiv.org/pdf/2502.14276v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-02-20T05:28:44Z", "updated": "2025-05-29T16:13:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks", "authors": ["Yifei Zhou", "Song Jiang", "Yuandong Tian", "Jason Weston", "Sergey Levine", "Sainbayar Sukhbaatar", "Xian Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.15478v1", "abstract": "Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.", "source": "arxiv", "arxiv_id": "2503.15478v1", "pdf_url": "https://arxiv.org/pdf/2503.15478v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-03-19T17:55:08Z", "updated": "2025-03-19T17:55:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents", "authors": ["Ruolin Chen", "Yinqian Sun", "Jihang Wang", "Mingyang Lv", "Qian Zhang", "Yi Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2509.25885v1", "abstract": "Embodied agents powered by large language models (LLMs) inherit advanced planning capabilities; however, their direct interaction with the physical world exposes them to safety vulnerabilities. In this work, we identify four key reasoning stages where hazards may arise: Task Understanding, Environment Perception, High-Level Plan Generation, and Low-Level Action Generation. We further formalize three orthogonal safety constraint types (Factual, Causal, and Temporal) to systematically characterize potential safety violations. Building on this risk model, we present SafeMindBench, a multimodal benchmark with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk, Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm, privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain susceptible to safety-critical failures. To address this challenge, we introduce SafeMindAgent, a modular Planner-Executor architecture integrated with three cascaded safety modules, which incorporate safety constraints into the reasoning process. Results show that SafeMindAgent significantly improves safety rate over strong baselines while maintaining comparable task completion. Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation suite and a practical solution that advance the systematic study and mitigation of safety risks in embodied LLM agents.", "source": "arxiv", "arxiv_id": "2509.25885v1", "pdf_url": "https://arxiv.org/pdf/2509.25885v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-30T07:24:04Z", "updated": "2025-09-30T07:24:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents", "authors": ["Kunlun Zhu", "Jiaxun Zhang", "Ziheng Qi", "Nuoxing Shang", "Zijia Liu", "Peixuan Han", "Yue Su", "Haofei Yu", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2505.23559v1", "abstract": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \\textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \\textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}", "source": "arxiv", "arxiv_id": "2505.23559v1", "pdf_url": "https://arxiv.org/pdf/2505.23559v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-29T15:35:58Z", "updated": "2025-05-29T15:35:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "year": 2025, "url": "http://arxiv.org/abs/2505.06416v1", "abstract": "Recent advancements in Large Language Models (LLMs) and the introduction of the Model Context Protocol (MCP) have significantly expanded LLM agents' capability to interact dynamically with external tools and APIs. However, existing tool selection frameworks do not integrate MCP servers, instead relying heavily on error-prone manual updates to monolithic local tool repositories, leading to duplication, inconsistencies, and inefficiencies. Additionally, current approaches abstract tool selection before the LLM agent is invoked, limiting its autonomy and hindering dynamic re-querying capabilities during multi-turn interactions. To address these issues, we introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM agents with a MCP tool retriever, giving agents the autonomy to add tools into their memory, as well as an auto-synchronizing tool storage system pipeline through CRUD (create, read, update, delete) operations with MCP servers as the single source of truth. We also propose a novel embedding strategy, Tool Document Weighted Average (TDWA), designed to selectively emphasize critical components of tool documents (e.g. tool name or synthetic questions) during the embedding process. Comprehensive evaluations conducted on a created dataset of 5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models, and 5 retriever types, demonstrate substantial improvements in tool retrieval and agent invocation performance, emphasizing ScaleMCP's effectiveness in scalable, dynamic tool selection and invocation.", "source": "arxiv", "arxiv_id": "2505.06416v1", "pdf_url": "https://arxiv.org/pdf/2505.06416v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-09T20:30:37Z", "updated": "2025-05-09T20:30:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Scaling Long-Horizon LLM Agent via Context-Folding", "authors": ["Weiwei Sun", "Miao Lu", "Zhan Ling", "Kang Liu", "Xuesong Yao", "Yiming Yang", "Jiecao Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.11967v1", "abstract": "Large language model (LLM) agents are fundamentally constrained by context length on long-horizon tasks. We introduce Context-Folding, a framework that empowers agents to actively manage their working context. An agent can procedurally branch into a sub-trajectory to handle a subtask and then fold it upon completion, collapsing the intermediate steps while retaining a concise summary of the outcome. To make this behavior learnable, we develop an end-to-end reinforcement learning framework FoldGRPO with specific process rewards to encourage effective task decomposition and context management. On complex long-horizon tasks (Deep Research and SWE), our folding agent matches or outperforms the ReAct baselines while using an active context 10$\\times$ smaller and significantly outperforms models that rely on summarization-based context management.", "source": "arxiv", "arxiv_id": "2510.11967v1", "pdf_url": "https://arxiv.org/pdf/2510.11967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-13T22:00:58Z", "updated": "2025-10-13T22:00:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent", "authors": ["Xingjian Kang", "Linda Vorberg", "Andreas Maier", "Alexander Katzmann", "Oliver Taubmann"], "year": 2025, "url": "http://arxiv.org/abs/2509.20270v1", "abstract": "Managing scan protocols in Computed Tomography (CT), which includes adjusting acquisition parameters or configuring reconstructions, as well as selecting postprocessing tools in a patient-specific manner, is time-consuming and requires clinical as well as technical expertise. At the same time, we observe an increasing shortage of skilled workforce in radiology. To address this issue, a Large Language Model (LLM)-based agent framework is proposed to assist with the interpretation and execution of protocol configuration requests given in natural language or a structured, device-independent format, aiming to improve the workflow efficiency and reduce technologists' workload. The agent combines in-context-learning, instruction-following, and structured toolcalling abilities to identify relevant protocol elements and apply accurate modifications. In a systematic evaluation, experimental results indicate that the agent can effectively retrieve protocol components, generate device compatible protocol definition files, and faithfully implement user requests. Despite demonstrating feasibility in principle, the approach faces limitations regarding syntactic and semantic validity due to lack of a unified device API, and challenges with ambiguous or complex requests. In summary, the findings show a clear path towards LLM-based agents for supporting scan protocol management in CT imaging.", "source": "arxiv", "arxiv_id": "2509.20270v1", "pdf_url": "https://arxiv.org/pdf/2509.20270v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-24T16:04:11Z", "updated": "2025-09-24T16:04:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization", "authors": ["Yinjie Wang", "Ling Yang", "Guohao Li", "Mengdi Wang", "Bryon Aragam"], "year": 2025, "url": "http://arxiv.org/abs/2502.04306v1", "abstract": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow", "source": "arxiv", "arxiv_id": "2502.04306v1", "pdf_url": "https://arxiv.org/pdf/2502.04306v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-06T18:47:49Z", "updated": "2025-02-06T18:47:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19870v1", "abstract": "Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify'' principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.", "source": "arxiv", "arxiv_id": "2508.19870v1", "pdf_url": "https://arxiv.org/pdf/2508.19870v1", "categories": ["cs.NI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2025-08-27T13:33:35Z", "updated": "2025-08-27T13:33:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy", "Relsy Puthal", "Kaustik Ranaware"], "year": 2025, "url": "http://arxiv.org/abs/2509.16275v1", "abstract": "Modern software development pipelines face growing challenges in securing large codebases with extensive dependencies. Static analysis tools like Bandit are effective at vulnerability detection but suffer from high false positives and lack repair capabilities. Large Language Models (LLMs), in contrast, can suggest fixes but often hallucinate changes and lack self-validation. We present SecureFixAgent, a hybrid repair framework integrating Bandit with lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning on a diverse, curated dataset spanning multiple Python project domains, mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses Bandit for detection, the LLM for candidate fixes with explanations, and Bandit re-validation for verification, all executed locally to preserve privacy and reduce cloud reliance. Experiments show SecureFixAgent reduces false positives by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers false positives by 5.46% compared to pre-trained LLMs, typically converging within three iterations. Beyond metrics, developer studies rate explanation quality 4.5/5, highlighting its value for human trust and adoption. By combining verifiable security improvements with transparent rationale in a resource-efficient local framework, SecureFixAgent advances trustworthy, automated vulnerability remediation for modern pipelines.", "source": "arxiv", "arxiv_id": "2509.16275v1", "pdf_url": "https://arxiv.org/pdf/2509.16275v1", "categories": ["cs.CR", "cs.AI", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-09-18T15:45:43Z", "updated": "2025-09-18T15:45:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Self-Challenging Language Model Agents", "authors": ["Yifei Zhou", "Sergey Levine", "Jason Weston", "Xian Li", "Sainbayar Sukhbaatar"], "year": 2025, "url": "http://arxiv.org/abs/2506.01716v1", "abstract": "Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "source": "arxiv", "arxiv_id": "2506.01716v1", "pdf_url": "https://arxiv.org/pdf/2506.01716v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-02T14:23:33Z", "updated": "2025-06-02T14:23:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "authors": ["Vishnu Sarukkai", "Zhiqiang Xie", "Kayvon Fatahalian"], "year": 2025, "url": "http://arxiv.org/abs/2505.00234v3", "abstract": "Improving Large Language Model (LLM) agents for sequential decision-making tasks typically requires extensive task-specific knowledge engineering--custom prompts, curated examples, and specialized observation/action spaces. We investigate a different approach where agents automatically improve by learning from their own successful experiences without human intervention. Our method constructs and refines a database of self-generated trajectories that serve as in-context examples for future tasks. Even naive accumulation of successful trajectories yields substantial performance gains across three diverse benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%). These improvements exceed those achieved by upgrading from gpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts per task. We further enhance this approach with two innovations: database-level curation using population-based training to propagate high-performing example collections, and exemplar-level curation that selectively retains trajectories based on their empirical utility as in-context examples. With these enhancements, our method achieves 93% success on ALFWorld--surpassing approaches that use more powerful LLMs and hand-crafted components. Our trajectory bootstrapping technique demonstrates that agents can autonomously improve through experience, offering a scalable alternative to labor-intensive knowledge engineering.", "source": "arxiv", "arxiv_id": "2505.00234v3", "pdf_url": "https://arxiv.org/pdf/2505.00234v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-01T00:48:12Z", "updated": "2025-05-16T21:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services", "authors": ["Jayden Serenari", "Stephen Lee"], "year": 2025, "url": "http://arxiv.org/abs/2510.27016v1", "abstract": "With the increasing use of conversational AI systems, there is growing concern over privacy leaks, especially when users share sensitive personal data in interactions with Large Language Models (LLMs). Conversations shared with these models may contain Personally Identifiable Information (PII), which, if exposed, could lead to security breaches or identity theft. To address this challenge, we present the Local Optimizations for Pseudonymization with Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a semantically-aware privacy agent designed to safeguard sensitive PII data when using remote LLMs. Unlike prior work that often degrade response quality, our approach dynamically replaces sensitive PII entities in user prompts with semantically consistent pseudonyms, preserving the contextual integrity of conversations. Once the model generates its response, the pseudonyms are automatically depseudonymized, ensuring the user receives an accurate, privacy-preserving output. We evaluate our approach using real-world conversations sourced from ShareGPT, which we further augment and annotate to assess whether named entities are contextually relevant to the model's response. Our results show that LOPSIDED reduces semantic utility errors by a factor of 5 compared to baseline techniques, all while enhancing privacy.", "source": "arxiv", "arxiv_id": "2510.27016v1", "pdf_url": "https://arxiv.org/pdf/2510.27016v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-30T21:34:23Z", "updated": "2025-10-30T21:34:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling", "authors": ["Shuyuan Zhang", "Chenhan Jiang", "Zuoou Li", "Jiankang Deng"], "year": 2025, "url": "http://arxiv.org/abs/2510.17603v1", "abstract": "3D generation from natural language offers significant potential to reduce expert manual modeling efforts and enhance accessibility to 3D assets. However, existing methods often yield unstructured meshes and exhibit poor interactivity, making them impractical for artistic workflows. To address these limitations, we represent 3D assets as shape programs and introduce ShapeCraft, a novel multi-agent framework for text-to-3D generation. At its core, we propose a Graph-based Procedural Shape (GPS) representation that decomposes complex natural language into a structured graph of sub-tasks, thereby facilitating accurate LLM comprehension and interpretation of spatial relationships and semantic shape details. Specifically, LLM agents hierarchically parse user input to initialize GPS, then iteratively refine procedural modeling and painting to produce structured, textured, and interactive 3D assets. Qualitative and quantitative experiments demonstrate ShapeCraft's superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based agents. We further show the versatility of ShapeCraft through examples of animated and user-customized editing, highlighting its potential for broader interactive applications.", "source": "arxiv", "arxiv_id": "2510.17603v1", "pdf_url": "https://arxiv.org/pdf/2510.17603v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-10-20T14:51:14Z", "updated": "2025-10-20T14:51:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents", "authors": ["Yun Hua", "Haosheng Chen", "Shiqin Wang", "Wenhao Li", "Xiangfeng Wang", "Jun Luo"], "year": 2025, "url": "http://arxiv.org/abs/2506.07388v1", "abstract": "Large Language Models (LLMs) show strong collaborative performance in multi-agent systems with predefined roles and workflows. However, in open-ended environments lacking coordination rules, agents tend to act in self-interested ways. The central challenge in achieving coordination lies in credit assignment -- fairly evaluating each agent's contribution and designing pricing mechanisms that align their heterogeneous goals. This problem is critical as LLMs increasingly participate in complex human-AI collaborations, where fair compensation and accountability rely on effective pricing mechanisms. Inspired by how human societies address similar coordination challenges (e.g., through temporary collaborations such as employment or subcontracting), we propose a cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley Chain-of-Thought -- leveraging marginal contributions as a principled basis for pricing -- with structured negotiation protocols for effective price matching, enabling LLM agents to coordinate through rational task-time pricing and post-task reward redistribution. This approach aligns agent incentives, fosters cooperation, and maintains autonomy. We evaluate Shapley-Coop across two multi-agent games and a software engineering simulation, demonstrating that it consistently enhances LLM agent collaboration and facilitates equitable credit assignment. These results highlight the effectiveness of Shapley-Coop's pricing mechanisms in accurately reflecting individual contributions during task execution.", "source": "arxiv", "arxiv_id": "2506.07388v1", "pdf_url": "https://arxiv.org/pdf/2506.07388v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-09T03:24:01Z", "updated": "2025-06-09T03:24:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing", "authors": ["Junlong Ke", "Qiying Hu", "Shenghai Yuan", "Yuecong Xu", "Jianfei Yang"], "year": 2025, "url": "http://arxiv.org/abs/2509.17197v2", "abstract": "Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.", "source": "arxiv", "arxiv_id": "2509.17197v2", "pdf_url": "https://arxiv.org/pdf/2509.17197v2", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-21T18:54:54Z", "updated": "2025-10-30T15:26:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents", "authors": ["Gyuhyeon Seo", "Jungwoo Yang", "Junseong Pyo", "Nalim Kim", "Jonggeun Lee", "Yohan Jo"], "year": 2025, "url": "http://arxiv.org/abs/2509.24282v2", "abstract": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol, the global industry standard for smart home communication, SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Models under 7B parameters exhibited negligible performance across all query types. Even GPT-4.1, the best-performing standard model, struggled with implicit intent inference, state verification, and particularly temporal scheduling. While reasoning models such as GPT-5.1 consistently outperformed standard models on every query type, they required over three times the average inference time, which can be prohibitive for real-time smart home applications. This highlights a critical trade-off between task performance and real-world practicality.", "source": "arxiv", "arxiv_id": "2509.24282v2", "pdf_url": "https://arxiv.org/pdf/2509.24282v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-29T04:54:20Z", "updated": "2025-12-08T08:28:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Simulating Filter Bubble on Short-video Recommender System with Large Language Model Agents", "authors": ["Nicholas Sukiennik", "Haoyu Wang", "Zailin Zeng", "Chen Gao", "Yong Li"], "year": 2025, "url": "http://arxiv.org/abs/2504.08742v1", "abstract": "An increasing reliance on recommender systems has led to concerns about the creation of filter bubbles on social media, especially on short video platforms like TikTok. However, their formation is still not entirely understood due to the complex dynamics between recommendation algorithms and user feedback. In this paper, we aim to shed light on these dynamics using a large language model-based simulation framework. Our work employs real-world short-video data containing rich video content information and detailed user-agents to realistically simulate the recommendation-feedback cycle. Through large-scale simulations, we demonstrate that LLMs can replicate real-world user-recommender interactions, uncovering key mechanisms driving filter bubble formation. We identify critical factors, such as demographic features and category attraction that exacerbate content homogenization. To mitigate this, we design and test interventions including various cold-start and feedback weighting strategies, showing measurable reductions in filter bubble effects. Our framework enables rapid prototyping of recommendation strategies, offering actionable solutions to enhance content diversity in real-world systems. Furthermore, we analyze how LLM-inherent biases may propagate through recommendations, proposing safeguards to promote equity for vulnerable groups, such as women and low-income populations. By examining the interplay between recommendation and LLM agents, this work advances a deeper understanding of algorithmic bias and provides practical tools to promote inclusive digital spaces.", "source": "arxiv", "arxiv_id": "2504.08742v1", "pdf_url": "https://arxiv.org/pdf/2504.08742v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-03-23T10:35:58Z", "updated": "2025-03-23T10:35:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Simulating Macroeconomic Expectations using LLM Agents", "authors": ["Jianhao Lin", "Lexuan Sun", "Yixin Yan"], "year": 2025, "url": "http://arxiv.org/abs/2505.17648v4", "abstract": "We introduce a novel framework for simulating macroeconomic expectations using LLM Agents. By constructing LLM Agents equipped with various functional modules, we replicate three representative survey experiments involving several expectations across different types of economic agents. Our results show that although the expectations simulated by LLM Agents are more homogeneous than those of humans, they consistently outperform LLMs relying simply on prompt engineering, and possess human-like mental mechanisms. Evaluation reveals that these capabilities stem from the contributions of their components, offering guidelines for their architectural design. Our approach complements traditional methods and provides new insights into AI behavioral science in macroeconomic research", "source": "arxiv", "arxiv_id": "2505.17648v4", "pdf_url": "https://arxiv.org/pdf/2505.17648v4", "categories": ["econ.GN", "cs.AI"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2025-05-23T09:11:14Z", "updated": "2025-11-25T02:11:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Simulating Rumor Spreading in Social Networks using LLM Agents", "authors": ["Tianrui Hu", "Dimitrios Liakopoulos", "Xiwen Wei", "Radu Marculescu", "Neeraja J. Yadwadkar"], "year": 2025, "url": "http://arxiv.org/abs/2502.01450v1", "abstract": "With the rise of social media, misinformation has become increasingly prevalent, fueled largely by the spread of rumors. This study explores the use of Large Language Model (LLM) agents within a novel framework to simulate and analyze the dynamics of rumor propagation across social networks. To this end, we design a variety of LLM-based agent types and construct four distinct network structures to conduct these simulations. Our framework assesses the effectiveness of different network constructions and agent behaviors in influencing the spread of rumors. Our results demonstrate that the framework can simulate rumor spreading across more than one hundred agents in various networks with thousands of edges. The evaluations indicate that network structure, personas, and spreading schemes can significantly influence rumor dissemination, ranging from no spread to affecting 83\\% of agents in iterations, thereby offering a realistic simulation of rumor spread in social networks.", "source": "arxiv", "arxiv_id": "2502.01450v1", "pdf_url": "https://arxiv.org/pdf/2502.01450v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-02-03T15:39:56Z", "updated": "2025-02-03T15:39:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Simulating and Experimenting with Social Media Mobilization Using LLM Agents", "authors": ["Sadegh Shirani", "Mohsen Bayati"], "year": 2025, "url": "http://arxiv.org/abs/2510.26494v1", "abstract": "Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}", "source": "arxiv", "arxiv_id": "2510.26494v1", "pdf_url": "https://arxiv.org/pdf/2510.26494v1", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2025-10-30T13:43:28Z", "updated": "2025-10-30T13:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent", "authors": ["Shiyi Cao", "Dacheng Li", "Fangzhou Zhao", "Shuo Yuan", "Sumanth R. Hegde", "Connor Chen", "Charlie Ruan", "Tyler Griggs", "Shu Liu", "Eric Tang", "Richard Liaw", "Philipp Moritz", "Matei Zaharia", "Joseph E. Gonzalez", "Ion Stoica"], "year": 2025, "url": "http://arxiv.org/abs/2511.16108v1", "abstract": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.\n  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.", "source": "arxiv", "arxiv_id": "2511.16108v1", "pdf_url": "https://arxiv.org/pdf/2511.16108v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-20T07:05:19Z", "updated": "2025-11-20T07:05:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions", "authors": ["Guanquan Shi", "Haohua Du", "Zhiqiang Wang", "Xiaoyu Liang", "Weiwenpei Liu", "Song Bian", "Zhenyu Guan"], "year": 2025, "url": "http://arxiv.org/abs/2512.06914v1", "abstract": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.\n  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.", "source": "arxiv", "arxiv_id": "2512.06914v1", "pdf_url": "https://arxiv.org/pdf/2512.06914v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-12-07T16:41:02Z", "updated": "2025-12-07T16:41:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Xuanjing Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "year": 2025, "url": "http://arxiv.org/abs/2504.10157v3", "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.", "source": "arxiv", "arxiv_id": "2504.10157v3", "pdf_url": "https://arxiv.org/pdf/2504.10157v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-14T12:12:52Z", "updated": "2025-07-15T11:14:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "year": 2025, "url": "http://arxiv.org/abs/2510.03253v1", "abstract": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.", "source": "arxiv", "arxiv_id": "2510.03253v1", "pdf_url": "https://arxiv.org/pdf/2510.03253v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-26T08:43:39Z", "updated": "2025-09-26T08:43:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex Specifications Using LLM Agent Systems", "authors": ["Zhongzhi Yu", "Mingjie Liu", "Michael Zimmer", "Yingyan Celine Lin", "Yong Liu", "Haoxing Ren"], "year": 2025, "url": "http://arxiv.org/abs/2506.13905v2", "abstract": "Despite recent progress in generating hardware RTL code with LLMs, existing solutions still suffer from a substantial gap between practical application scenarios and the requirements of real-world RTL code development. Prior approaches either focus on overly simplified hardware descriptions or depend on extensive human guidance to process complex specifications, limiting their scalability and automation potential. In this paper, we address this gap by proposing an LLM agent system, termed Spec2RTL-Agent, designed to directly process complex specification documentation and generate corresponding RTL code implementations, advancing LLM-based RTL code generation toward more realistic application settings. To achieve this goal, Spec2RTL-Agent introduces a novel multi-agent collaboration framework that integrates three key enablers: (1) a reasoning and understanding module that translates specifications into structured, step-by-step implementation plans; (2) a progressive coding and prompt optimization module that iteratively refines the code across multiple representations to enhance correctness and synthesisability for RTL conversion; and (3) an adaptive reflection module that identifies and traces the source of errors during generation, ensuring a more robust code generation flow. Instead of directly generating RTL from natural language, our system strategically generates synthesizable C++ code, which is then optimized for HLS. This agent-driven refinement ensures greater correctness and compatibility compared to naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three specification documents, showing it generates accurate RTL code with up to 75% fewer human interventions than existing methods. This highlights its role as the first fully automated multi-agent system for RTL generation from unstructured specs, reducing reliance on human effort in hardware design.", "source": "arxiv", "arxiv_id": "2506.13905v2", "pdf_url": "https://arxiv.org/pdf/2506.13905v2", "categories": ["cs.AR"], "primary_category": "cs.AR", "doi": "", "venue": "", "published": "2025-06-16T18:33:25Z", "updated": "2025-09-08T18:17:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Spiral of Silence in Large Language Model Agents", "authors": ["Mingze Zhong", "Meng Fang", "Zijing Shi", "Yuxuan Huang", "Shunfeng Zheng", "Yali Du", "Ling Chen", "Jun Wang"], "year": 2025, "url": "http://arxiv.org/abs/2510.02360v2", "abstract": "The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.", "source": "arxiv", "arxiv_id": "2510.02360v2", "pdf_url": "https://arxiv.org/pdf/2510.02360v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-28T08:59:54Z", "updated": "2025-10-08T01:58:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "StaffPro: an LLM Agent for Joint Staffing and Profiling", "authors": ["Alessio Maritan"], "year": 2025, "url": "http://arxiv.org/abs/2507.21636v1", "abstract": "Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities. In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data. We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly. Differently from existing staffing solutions, StaffPro allows expressing optimization objectives using natural language, accepts textual task descriptions and provides high flexibility. StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use. By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time. A consulting firm simulation example demonstrates that StaffPro successfully estimates workers' attributes and generates high quality schedules. With its innovative design, StaffPro offers a robust, interpretable, and human-centric solution for automated personnel management.", "source": "arxiv", "arxiv_id": "2507.21636v1", "pdf_url": "https://arxiv.org/pdf/2507.21636v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-07-29T09:48:54Z", "updated": "2025-07-29T09:48:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must Prioritize Risk", "authors": ["Zichen Chen", "Jiaao Chen", "Jianda Chen", "Misha Sra"], "year": 2025, "url": "http://arxiv.org/abs/2502.15865v2", "abstract": "Standard benchmarks fixate on how well large language model (LLM) agents perform in finance, yet say little about whether they are safe to deploy. We argue that accuracy metrics and return-based scores provide an illusion of reliability, overlooking vulnerabilities such as hallucinated facts, stale data, and adversarial prompt manipulation. We take a firm position: financial LLM agents should be evaluated first and foremost on their risk profile, not on their point-estimate performance. Drawing on risk-engineering principles, we outline a three-level agenda: model, workflow, and system, for stress-testing LLM agents under realistic failure modes. To illustrate why this shift is urgent, we audit six API-based and open-weights LLM agents on three high-impact tasks and uncover hidden weaknesses that conventional benchmarks miss. We conclude with actionable recommendations for researchers, practitioners, and regulators: audit risk-aware metrics in future studies, publish stress scenarios alongside datasets, and treat ``safety budget'' as a primary success criterion. Only by redefining what ``good'' looks like can the community responsibly advance AI-driven finance.", "source": "arxiv", "arxiv_id": "2502.15865v2", "pdf_url": "https://arxiv.org/pdf/2502.15865v2", "categories": ["q-fin.GN", "cs.AI", "cs.CL"], "primary_category": "q-fin.GN", "doi": "", "venue": "", "published": "2025-02-21T12:56:15Z", "updated": "2025-06-02T10:13:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?", "authors": ["Yanxu Chen", "Zijun Yao", "Yantao Liu", "Jin Ye", "Jianing Yu", "Lei Hou", "Juanzi Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.02209v1", "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.", "source": "arxiv", "arxiv_id": "2510.02209v1", "pdf_url": "https://arxiv.org/pdf/2510.02209v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-02T16:54:57Z", "updated": "2025-10-02T16:54:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems", "authors": ["Qi Lin", "Zhenyu Zhang", "Viraj Thakkar", "Zhenjie Sun", "Mai Zheng", "Zhichao Cao"], "year": 2025, "url": "http://arxiv.org/abs/2510.25017v1", "abstract": "Automatically configuring storage systems is hard: parameter spaces are large and conditions vary across workloads, deployments, and versions. Heuristic and ML tuners are often system specific, require manual glue, and degrade under changes. Recent LLM-based approaches help but usually treat tuning as a single-shot, system-specific task, which limits cross-system reuse, constrains exploration, and weakens validation. We present StorageXTuner, an LLM agent-driven auto-tuning framework for heterogeneous storage engines. StorageXTuner separates concerns across four agents - Executor (sandboxed benchmarking), Extractor (performance digest), Searcher (insight-guided configuration exploration), and Reflector (insight generation and management). The design couples an insight-driven tree search with layered memory that promotes empirically validated insights and employs lightweight checkers to guard against unsafe actions. We implement a prototype and evaluate it on RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C. Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and 56%, and converges with fewer trials.", "source": "arxiv", "arxiv_id": "2510.25017v1", "pdf_url": "https://arxiv.org/pdf/2510.25017v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB", "doi": "", "venue": "", "published": "2025-10-28T22:33:14Z", "updated": "2025-10-28T22:33:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "year": 2025, "url": "http://arxiv.org/abs/2512.11907v1", "abstract": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.", "source": "arxiv", "arxiv_id": "2512.11907v1", "pdf_url": "https://arxiv.org/pdf/2512.11907v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-10T20:22:26Z", "updated": "2025-12-10T20:22:26Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Structured Uncertainty guided Clarification for LLM Agents", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "year": 2025, "url": "http://arxiv.org/abs/2511.08798v1", "abstract": "LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\\% while reducing clarification questions by 1.5-2.7$\\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\\% to 65.2\\% (3B model) and 36.7\\% to 62.9\\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.", "source": "arxiv", "arxiv_id": "2511.08798v1", "pdf_url": "https://arxiv.org/pdf/2511.08798v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-11-11T21:50:44Z", "updated": "2025-11-11T21:50:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Super-additive Cooperation in Language Model Agents", "authors": ["Filippo Tonini", "Lukas Galke"], "year": 2025, "url": "http://arxiv.org/abs/2508.15510v1", "abstract": "With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoner's Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.", "source": "arxiv", "arxiv_id": "2508.15510v1", "pdf_url": "https://arxiv.org/pdf/2508.15510v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-21T12:36:44Z", "updated": "2025-08-21T12:36:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Surgical AI Copilot: Energy-Based Fourier Gradient Low-Rank Adaptation for Surgical LLM Agent Reasoning and Planning", "authors": ["Jiayuan Huang", "Runlong He", "Danyal Zaman Khan", "Evangelos B. Mazomenos", "Danail Stoyanov", "Hani Marcus", "Linzhe Jiang", "Matthew J Clarkson", "Mobarak I. Hoque"], "year": 2025, "url": "http://arxiv.org/abs/2503.09474v2", "abstract": "Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large language models (LLMs)-powered agents offer a promising solution by enabling dynamic task planning and predictive decision support. Despite recent advances, the absence of surgical agent datasets and robust parameter-efficient fine-tuning techniques limits the development of LLM agents capable of complex intraoperative reasoning. In this paper, we introduce Surgical AI Copilot, an LLM agent for image-guided pituitary surgery, capable of conversation, planning, and task execution in response to queries involving tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured agent planning, we develop the PitAgent dataset, a surgical context-aware planning dataset covering surgical tasks like workflow analysis, instrument localization, anatomical segmentation, and query-based reasoning. Additionally, we propose DEFT-GaLore, a Deterministic Energy-based Fourier Transform (DEFT) gradient projection technique for efficient low-rank adaptation of recent LLMs (e.g., LLaMA 3.2, Qwen 2.5), enabling their use as surgical agent planners. We extensively validate our agent's performance and the proposed adaptation technique against other state-of-the-art low-rank adaptation methods on agent planning and prompt generation tasks, including a zero-shot surgical VQA benchmark, demonstrating the significant potential for truly efficient and scalable surgical LLM agents in real-time operative settings.", "source": "arxiv", "arxiv_id": "2503.09474v2", "pdf_url": "https://arxiv.org/pdf/2503.09474v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-03-12T15:30:39Z", "updated": "2025-11-12T15:54:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review", "authors": ["Anjana Sarkar", "Soumyendu Sarkar"], "year": 2025, "url": "http://arxiv.org/abs/2506.05364v1", "abstract": "This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). It examines the foundational architectures of LLM-based agents and their evolution from isolated operation to sophisticated, multi-agent collaboration, addressing key communication hurdles that arise in this transition. The study revisits well-established patterns, including Mediator, Observer, Publish-Subscribe, and Broker, and analyzes their relevance in structuring agent interactions within MCP-compliant frameworks. To clarify these dynamics, the article provides conceptual schematics and formal models that map out communication pathways and optimize data flow. It further explores architectural variations suited to different degrees of agent autonomy and system complexity. Real-world applications in domains such as real-time financial processing and investment banking are discussed, illustrating how these patterns and MCP can meet specific operational demands. The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "source": "arxiv", "arxiv_id": "2506.05364v1", "pdf_url": "https://arxiv.org/pdf/2506.05364v1", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-05-26T09:11:17Z", "updated": "2025-05-26T09:11:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Survey of Specialized Large Language Model", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2508.19667v1", "abstract": "The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.", "source": "arxiv", "arxiv_id": "2508.19667v1", "pdf_url": "https://arxiv.org/pdf/2508.19667v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-08-27T08:27:23Z", "updated": "2025-08-27T08:27:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation", "authors": ["Fiona Y. Wang", "Di Sheng Lee", "David L. Kaplan", "Markus J. Buehler"], "year": 2025, "url": "http://arxiv.org/abs/2511.22311v1", "abstract": "Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.", "source": "arxiv", "arxiv_id": "2511.22311v1", "pdf_url": "https://arxiv.org/pdf/2511.22311v1", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.soft", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-27T10:42:52Z", "updated": "2025-11-27T10:42:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TAMO: Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems", "authors": ["Xiao Zhang", "Qi Wang", "Mingyi Li", "Yuan Yuan", "Mengbai Xiao", "Fuzhen Zhuang", "Dongxiao Yu"], "year": 2025, "url": "http://arxiv.org/abs/2504.20462v5", "abstract": "Implementing large language models (LLMs)-driven root cause analysis (RCA) in cloud-native systems has become a key topic of modern software operations and maintenance. However, existing LLM-based approaches face three key challenges: multi-modality input constraint, context window limitation, and dynamic dependence graph. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data for fine-grained RCA, namely TAMO, including multimodality alignment tool, root cause localization tool, and fault types classification tool. In detail, TAMO unifies multi-modal observation data into time-aligned representations for cross-modal feature consistency. Based on the unified representations, TAMO then invokes its specialized root cause localization tool and fault types classification tool for further identifying root cause and fault type underlying system context. This approach overcomes the limitations of LLMs in processing real-time raw observational data and dynamic service dependencies, guiding the model to generate repair strategies that align with system context through structured prompt design. Experiments on two benchmark datasets demonstrate that TAMO outperforms state-of-the-art (SOTA) approaches with comparable performance.", "source": "arxiv", "arxiv_id": "2504.20462v5", "pdf_url": "https://arxiv.org/pdf/2504.20462v5", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-29T06:50:48Z", "updated": "2025-11-05T04:42:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "year": 2025, "url": "http://arxiv.org/abs/2510.24014v2", "abstract": "The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE TEXT2DB that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for what to extract and adapting to the given DB/KB schema for how to extract on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation. Source code: https://github.com/yzjiao/Text2DB", "source": "arxiv", "arxiv_id": "2510.24014v2", "pdf_url": "https://arxiv.org/pdf/2510.24014v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-28T02:49:40Z", "updated": "2025-10-30T05:38:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning", "authors": ["Hang Ni", "Fan Liu", "Xinyu Ma", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Hui Xiong", "Hao Liu"], "year": 2025, "url": "http://arxiv.org/abs/2504.08694v1", "abstract": "Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.", "source": "arxiv", "arxiv_id": "2504.08694v1", "pdf_url": "https://arxiv.org/pdf/2504.08694v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "10.18653/v1/2025.emnlp-main.626", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "published": "2025-04-11T17:02:40Z", "updated": "2025-04-11T17:02:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues", "authors": ["Yubin Ge", "Salvatore Romeo", "Jason Cai", "Raphael Shu", "Monica Sunkara", "Yassine Benajiba", "Yi Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.01630v2", "abstract": "Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs time-aware memorization through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate neuro-symbolic temporal reasoning, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.", "source": "arxiv", "arxiv_id": "2502.01630v2", "pdf_url": "https://arxiv.org/pdf/2502.01630v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-03T18:58:19Z", "updated": "2025-09-24T21:09:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "year": 2025, "url": "http://arxiv.org/abs/2509.07389v1", "abstract": "Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.", "source": "arxiv", "arxiv_id": "2509.07389v1", "pdf_url": "https://arxiv.org/pdf/2509.07389v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-09-09T05:09:27Z", "updated": "2025-09-09T05:09:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks", "authors": ["Ye Ye"], "year": 2025, "url": "http://arxiv.org/abs/2504.08525v4", "abstract": "Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.", "source": "arxiv", "arxiv_id": "2504.08525v4", "pdf_url": "https://arxiv.org/pdf/2504.08525v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-11T13:38:36Z", "updated": "2025-08-22T20:14:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "authors": ["Ye Ye"], "year": 2025, "url": "http://arxiv.org/abs/2505.19436v1", "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context. This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents. We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning. TME implements a spatial memory framework that replaces flat context with graph-based structures to support consistent, multi-turn reasoning. Departing from linear concatenation and ReAct-style prompting, TME builds a dynamic task graph -- either a tree or directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with prior context, and enable dependency-tracked revisions. Its Task Representation and Intent Management (TRIM) component models task semantics and user intent to ensure accurate interpretation. Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct. TME's modular design supports plug-and-play deployment and domain-specific customization, adaptable to both personal assistants and enterprise automation. We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents. TME's scalable architecture addresses a critical gap in agent performance across complex, interactive settings.", "source": "arxiv", "arxiv_id": "2505.19436v1", "pdf_url": "https://arxiv.org/pdf/2505.19436v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-26T02:53:22Z", "updated": "2025-05-26T02:53:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks", "authors": ["Zimo Ji", "Xunguang Wang", "Zongjie Li", "Pingchuan Ma", "Yudong Gao", "Daoyuan Wu", "Xincheng Yan", "Tian Tian", "Shuai Wang"], "year": 2025, "url": "http://arxiv.org/abs/2511.15203v1", "abstract": "Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.", "source": "arxiv", "arxiv_id": "2511.15203v1", "pdf_url": "https://arxiv.org/pdf/2511.15203v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-11-19T07:47:30Z", "updated": "2025-11-19T07:47:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "year": 2025, "url": "http://arxiv.org/abs/2507.11198v1", "abstract": "Large Language Models (LLMs) enable new possibilities for qualitative research at scale, including coding and data annotation. While multi-agent systems (MAS) can emulate human coding workflows, their benefits over single-agent coding remain poorly understood. We conducted an experimental study of how agent persona and temperature shape consensus-building and coding accuracy of dialog segments based on a codebook with 8 codes. Our open-source MAS mirrors deductive human coding through structured agent discussion and consensus arbitration. Using six open-source LLMs (with 3 to 32 billion parameters) and 18 experimental configurations, we analyze over 77,000 coding decisions against a gold-standard dataset of human-annotated transcripts from online math tutoring sessions. Temperature significantly impacted whether and when consensus was reached across all six LLMs. MAS with multiple personas (including neutral, assertive, or empathetic), significantly delayed consensus in four out of six LLMs compared to uniform personas. In three of those LLMs, higher temperatures significantly diminished the effects of multiple personas on consensus. However, neither temperature nor persona pairing lead to robust improvements in coding accuracy. Single agents matched or outperformed MAS consensus in most conditions. Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation when temperature was 0.5 or lower and especially when the agents included at least one assertive persona. Qualitative analysis of MAS collaboration for these configurations suggests that MAS may nonetheless aid in narrowing ambiguous code applications that could improve codebooks and human-AI coding. We contribute new insight into the limits of LLM-based qualitative methods, challenging the notion that diverse MAS personas lead to better outcomes. We open-source our MAS and experimentation code.", "source": "arxiv", "arxiv_id": "2507.11198v1", "pdf_url": "https://arxiv.org/pdf/2507.11198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-15T11:06:32Z", "updated": "2025-07-15T11:06:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "year": 2025, "url": "http://arxiv.org/abs/2601.00097v2", "abstract": "We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.", "source": "arxiv", "arxiv_id": "2601.00097v2", "pdf_url": "https://arxiv.org/pdf/2601.00097v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-31T20:06:48Z", "updated": "2026-01-14T06:26:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs", "authors": ["Avinash Baidya", "Kamalika Das", "Xiang Gao"], "year": 2025, "url": "http://arxiv.org/abs/2506.12266v1", "abstract": "Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.", "source": "arxiv", "arxiv_id": "2506.12266v1", "pdf_url": "https://arxiv.org/pdf/2506.12266v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-06-13T22:36:41Z", "updated": "2025-06-13T22:36:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover", "authors": ["Matteo Lupinacci", "Francesco Aurelio Pironti", "Francesco Blefari", "Francesco Romeo", "Luigi Arena", "Angelo Furfaro"], "year": 2025, "url": "http://arxiv.org/abs/2507.06850v5", "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables remarkable capabilities in natural language processing and generation. However, these systems introduce security vulnerabilities that extend beyond traditional content generation to system-level compromises. This paper presents a comprehensive evaluation of the LLMs security used as reasoning engines within autonomous agents, highlighting how they can be exploited as attack vectors capable of achieving computer takeovers. We focus on how different attack surfaces and trust boundaries can be leveraged to orchestrate such takeovers. We demonstrate that adversaries can effectively coerce popular LLMs into autonomously installing and executing malware on victim machines. Our evaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of models succumb to Direct Prompt Injection, and 83.3% are vulnerable to the more stealthy and evasive RAG Backdoor Attack. Notably, we tested trust boundaries within multi-agent systems, where LLM agents interact and influence each other, and we revealed that LLMs which successfully resist direct injection or RAG backdoor attacks will execute identical payloads when requested by peer agents. We found that 100.0% of tested LLMs can be compromised through Inter-Agent Trust Exploitation attacks, and that every model exhibits context-dependent security behaviors that create exploitable blind spots.", "source": "arxiv", "arxiv_id": "2507.06850v5", "pdf_url": "https://arxiv.org/pdf/2507.06850v5", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-09T13:54:58Z", "updated": "2025-11-04T10:28:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games", "authors": ["Lyle Goodyear", "Rachel Guo", "Ramesh Johari"], "year": 2025, "url": "http://arxiv.org/abs/2506.15624v1", "abstract": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic settings, but their stateless nature necessitates creating a natural language representation of history. We present a unifying framework for systematically constructing natural language \"state\" representations for prompting LLM agents in repeated multi-agent games. Previous work on games with LLM agents has taken an ad hoc approach to encoding game history, which not only obscures the impact of state representation on agents' behavior, but also limits comparability between studies. Our framework addresses these gaps by characterizing methods of state representation along three axes: action informativeness (i.e., the extent to which the state representation captures actions played); reward informativeness (i.e., the extent to which the state representation describes rewards obtained); and prompting style (or natural language compression, i.e., the extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it admits a simple equilibrium both in theory and in human subject experiments \\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find that there are key dependencies of LLM agent behavior on the natural language state representation. In particular, we observe that representations which provide agents with (1) summarized, rather than complete, natural language representations of past history; (2) information about regrets, rather than raw payoffs; and (3) limited information about others' actions lead to behavior that more closely matches game theoretic equilibrium predictions, and with more stable game play by the agents. By contrast, other representations can exhibit either large deviations from equilibrium, higher variation in dynamic game play over time, or both.", "source": "arxiv", "arxiv_id": "2506.15624v1", "pdf_url": "https://arxiv.org/pdf/2506.15624v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-18T16:53:38Z", "updated": "2025-06-18T16:53:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Emergence of Altruism in Large-Language-Model Agents Society", "authors": ["Haoyang Li", "Xiao Jia", "Zhanzhan Zhao"], "year": 2025, "url": "http://arxiv.org/abs/2509.22537v1", "abstract": "Leveraging Large Language Models (LLMs) for social simulation is a frontier in computational social science. Understanding the social logics these agents embody is critical to this attempt. However, existing research has primarily focused on cooperation in small-scale, task-oriented games, overlooking how altruism, which means sacrificing self-interest for collective benefit, emerges in large-scale agent societies. To address this gap, we introduce a Schelling-variant urban migration model that creates a social dilemma, compelling over 200 LLM agents to navigate an explicit conflict between egoistic (personal utility) and altruistic (system utility) goals. Our central finding is a fundamental difference in the social tendencies of LLMs. We identify two distinct archetypes: \"Adaptive Egoists\", which default to prioritizing self-interest but whose altruistic behaviors significantly increase under the influence of a social norm-setting message board; and \"Altruistic Optimizers\", which exhibit an inherent altruistic logic, consistently prioritizing collective benefit even at a direct cost to themselves. Furthermore, to qualitatively analyze the cognitive underpinnings of these decisions, we introduce a method inspired by Grounded Theory to systematically code agent reasoning. In summary, this research provides the first evidence of intrinsic heterogeneity in the egoistic and altruistic tendencies of different LLMs. We propose that for social simulation, model selection is not merely a matter of choosing reasoning capability, but of choosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a more suitable choice for simulating complex human societies, \"Altruistic Optimizers\" are better suited for modeling idealized pro-social actors or scenarios where collective welfare is the primary consideration.", "source": "arxiv", "arxiv_id": "2509.22537v1", "pdf_url": "https://arxiv.org/pdf/2509.22537v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-26T16:17:29Z", "updated": "2025-09-26T16:17:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents", "authors": ["Mohammad Rubyet Islam"], "year": 2025, "url": "http://arxiv.org/abs/2505.14727v1", "abstract": "The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.", "source": "arxiv", "arxiv_id": "2505.14727v1", "pdf_url": "https://arxiv.org/pdf/2505.14727v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-05-20T00:51:43Z", "updated": "2025-05-20T00:51:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents", "authors": ["Yifan Duan", "Yihong Tang", "Xuefeng Bai", "Kehai Chen", "Juntao Li", "Min Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.20859v2", "abstract": "Large language models (LLMs) excel in both closed tasks (including problem-solving, and code generation) and open tasks (including creative writing), yet existing explanations for their capabilities lack connections to real-world human intelligence. To fill this gap, this paper systematically investigates LLM intelligence through the lens of ``human simulation'', addressing three core questions: (1) \\textit{How do personality traits affect problem-solving in closed tasks?} (2) \\textit{How do traits shape creativity in open tasks?} (3) \\textit{How does single-agent performance influence multi-agent collaboration?} By assigning Big Five personality traits to LLM agents and evaluating their performance in single- and multi-agent settings, we reveal that specific traits significantly influence reasoning accuracy (closed tasks) and creative output (open tasks). Furthermore, multi-agent systems exhibit collective intelligence distinct from individual capabilities, driven by distinguishing combinations of personalities.", "source": "arxiv", "arxiv_id": "2502.20859v2", "pdf_url": "https://arxiv.org/pdf/2502.20859v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-28T09:01:39Z", "updated": "2025-05-27T07:52:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "authors": ["Gerrit GroÃmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "year": 2025, "url": "http://arxiv.org/abs/2505.03961v2", "abstract": "According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM agents toward collaboration. We use a finitely repeated public goods game in which LLM agents choose either cooperative or egoistic spending strategies. We prime agents with stories highlighting teamwork to different degrees and test how this influences negotiation outcomes. Our experiments explore four questions:(1) How do narratives influence negotiation behavior? (2) What differs when agents share the same story versus different ones? (3) What happens when the agent numbers grow? (4) Are agents resilient against self-serving negotiators? We find that story-based priming significantly affects negotiation strategies and success rates. Common stories improve collaboration, benefiting each agent. By contrast, priming agents with different stories reverses this effect, and those agents primed toward self-interest prevail. We hypothesize that these results carry implications for multi-agent system design and AI alignment.", "source": "arxiv", "arxiv_id": "2505.03961v2", "pdf_url": "https://arxiv.org/pdf/2505.03961v2", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-06T20:23:25Z", "updated": "2025-05-08T08:29:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Real Barrier to LLM Agent Usability is Agentic ROI", "authors": ["Weiwen Liu", "Jiarui Qin", "Xu Huang", "Xingshan Zeng", "Yunjia Xi", "Jianghao Lin", "Chuhan Wu", "Yasheng Wang", "Lifeng Shang", "Ruiming Tang", "Defu Lian", "Yong Yu", "Weinan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2505.17767v1", "abstract": "Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications. This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use. Hence, we call for a shift from solely optimizing model performance to a broader, utility-driven perspective: evaluating agents through the lens of the overall agentic return on investment (Agent ROI). By identifying key factors that determine Agentic ROI--information quality, agent time, and cost--we posit a zigzag development trajectory in optimizing agentic ROI: first scaling up to improve the information quality, then scaling down to minimize the time and cost. We outline the roadmap across different development stages to bridge the current usability gaps, aiming to make LLM agents truly scalable, accessible, and effective in real-world contexts.", "source": "arxiv", "arxiv_id": "2505.17767v1", "pdf_url": "https://arxiv.org/pdf/2505.17767v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-23T11:40:58Z", "updated": "2025-05-23T11:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games", "authors": ["Niv Eckhaus", "Uri Berger", "Gabriel Stanovsky"], "year": 2025, "url": "http://arxiv.org/abs/2506.05309v2", "abstract": "LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns. In this work, we develop an adaptive asynchronous LLM agent consisting of two modules: a generator that decides what to say, and a scheduler that decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, where our agent plays with human participants. Overall, our agent performs on par with human players, both in game performance metrics and in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We make all of our code and data publicly available. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.", "source": "arxiv", "arxiv_id": "2506.05309v2", "pdf_url": "https://arxiv.org/pdf/2506.05309v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-06-05T17:53:44Z", "updated": "2025-09-20T16:08:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents", "authors": ["Geon Lee", "Wenchao Yu", "Kijung Shin", "Wei Cheng", "Haifeng Chen"], "year": 2025, "url": "http://arxiv.org/abs/2502.11418v2", "abstract": "Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score.", "source": "arxiv", "arxiv_id": "2502.11418v2", "pdf_url": "https://arxiv.org/pdf/2502.11418v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-17T04:17:27Z", "updated": "2025-03-10T04:15:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt", "authors": ["Zhilong Wang", "Neha Nagaraja", "Lan Zhang", "Hayretdin Bahsi", "Pawan Patil", "Peng Liu"], "year": 2025, "url": "http://arxiv.org/abs/2506.05739v1", "abstract": "LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model's behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.", "source": "arxiv", "arxiv_id": "2506.05739v1", "pdf_url": "https://arxiv.org/pdf/2506.05739v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-06-06T04:50:57Z", "updated": "2025-06-06T04:50:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering", "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"], "year": 2025, "url": "http://arxiv.org/abs/2510.20036v1", "abstract": "Large language model (LLM) agents rely on external tools to solve complex tasks, but real-world toolsets often contain redundant tools with overlapping names and descriptions, introducing ambiguity and reducing selection accuracy. LLMs also face strict input context limits, preventing efficient consideration of large toolsets. To address these challenges, we propose ToolScope, which includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and select only the most relevant tools for each query, compressing toolsets to fit within context limits without sacrificing accuracy. Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "source": "arxiv", "arxiv_id": "2510.20036v1", "pdf_url": "https://arxiv.org/pdf/2510.20036v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-22T21:29:27Z", "updated": "2025-10-22T21:29:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Toward Efficient Exploration by Large Language Model Agents", "authors": ["Dilip Arumugam", "Thomas L. Griffiths"], "year": 2025, "url": "http://arxiv.org/abs/2504.20997v1", "abstract": "A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.", "source": "arxiv", "arxiv_id": "2504.20997v1", "pdf_url": "https://arxiv.org/pdf/2504.20997v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-29T17:59:48Z", "updated": "2025-04-29T17:59:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "year": 2025, "url": "http://arxiv.org/abs/2508.03092v1", "abstract": "With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex. This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments. The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process. Our designed agent architecture includes three core tools: precise web search tool, source credibility assessment tool and numerical claim verification tool. These tools enable the agent to execute multi-step verification strategies, maintain evidence logs, and form comprehensive assessment conclusions. We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs. Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Experimental results show that our agent outperforms baseline methods in misinformation detection accuracy, reasoning transparency, and resistance to information rewriting, providing a new paradigm for trustworthy AI-assisted fact-checking.", "source": "arxiv", "arxiv_id": "2508.03092v1", "pdf_url": "https://arxiv.org/pdf/2508.03092v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-05T05:15:03Z", "updated": "2025-08-05T05:15:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "authors": ["Vinicius Lima", "Dzung T. Phan", "Jayant Kalagnanam", "Dhaval Patel", "Nianjun Zhou"], "year": 2025, "url": "http://arxiv.org/abs/2508.03117v1", "abstract": "We present a framework for training trustworthy large language model (LLM) agents for optimization modeling via a verifiable synthetic data generation pipeline. Focusing on linear and mixed-integer linear programming, our approach begins with structured symbolic representations and systematically produces natural language descriptions, mathematical formulations, and solver-executable code. By programmatically constructing each instance with known optimal solutions, the pipeline ensures full verifiability and enables automatic filtering of low-quality demonstrations generated by teacher models. Each dataset instance includes a structured representation of the optimization problem, a corresponding natural language description, the verified optimal solution, and step-by-step demonstrations - generated by a teacher model - that show how to model and solve the problem across multiple optimization modeling languages. This enables supervised fine-tuning of open-source LLMs specifically tailored to optimization tasks. To operationalize this pipeline, we introduce OptiTrust, a modular LLM agent that performs multi-stage translation from natural language to solver-ready code, leveraging stepwise demonstrations, multi-language inference, and majority-vote cross-validation. Our agent achieves state-of-the-art performance on standard benchmarks. Out of 7 datasets, it achieves the highest accuracy on six and outperforms the next-best algorithm by at least 8 percentage on three of them. Our approach provides a scalable, verifiable, and principled path toward building reliable LLM agents for real-world optimization applications.", "source": "arxiv", "arxiv_id": "2508.03117v1", "pdf_url": "https://arxiv.org/pdf/2508.03117v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-08-05T05:54:20Z", "updated": "2025-08-05T05:54:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols", "authors": ["Kathrin SeÃler", "Arne Bewersdorff", "Claudia Nerdel", "Enkelejda Kasneci"], "year": 2025, "url": "http://arxiv.org/abs/2502.12842v1", "abstract": "Effective feedback is essential for fostering students' success in scientific inquiry. With advancements in artificial intelligence, large language models (LLMs) offer new possibilities for delivering instant and adaptive feedback. However, this feedback often lacks the pedagogical validation provided by real-world practitioners. To address this limitation, our study evaluates and compares the feedback quality of LLM agents with that of human teachers and science education experts on student-written experimentation protocols. Four blinded raters, all professionals in scientific inquiry and science education, evaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and 3) the science education experts using a five-point Likert scale based on six criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Technical Terminology. Our results indicate that LLM-generated feedback shows no significant difference to that of teachers and experts in overall quality. However, the LLM agent's performance lags in the Feed Back dimension, which involves identifying and explaining errors within the student's work context. Qualitative analysis highlighted the LLM agent's limitations in contextual understanding and in the clear communication of specific errors. Our findings suggest that combining LLM-generated feedback with human expertise can enhance educational practices by leveraging the efficiency of LLMs and the nuanced understanding of educators.", "source": "arxiv", "arxiv_id": "2502.12842v1", "pdf_url": "https://arxiv.org/pdf/2502.12842v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-02-18T13:22:14Z", "updated": "2025-02-18T13:22:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers", "authors": ["Yusheng Zheng", "Yanpeng Hu", "Wei Zhang", "Andi Quinn"], "year": 2025, "url": "http://arxiv.org/abs/2509.01245v4", "abstract": "Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning (\"what to optimize\") from the system's role of execution (\"how to observe and act\"), thereby separating the optimization problem into two stages: goal-inference and policy-synthesis. Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis.\n  We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp", "source": "arxiv", "arxiv_id": "2509.01245v4", "pdf_url": "https://arxiv.org/pdf/2509.01245v4", "categories": ["cs.AI", "cs.MA", "cs.OS"], "primary_category": "cs.AI", "doi": "", "venue": "MLforSystem 2025", "published": "2025-09-01T08:38:49Z", "updated": "2025-09-30T02:48:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark", "authors": ["Minghao Shao", "Nanda Rani", "Kimberly Milner", "Haoran Xi", "Meet Udeshi", "Saksham Aggarwal", "Venkata Sai Charan Putrevu", "Sandeep Kumar Shukla", "Prashanth Krishnamurthy", "Farshad Khorrami", "Ramesh Karri", "Muhammad Shafique"], "year": 2025, "url": "http://arxiv.org/abs/2508.05674v1", "abstract": "Recent advances in LLM agentic systems have improved the automation of offensive security tasks, particularly for Capture the Flag (CTF) challenges. We systematically investigate the key factors that drive agent success and provide a detailed recipe for building effective LLM-based offensive security agents. First, we present CTFJudge, a framework leveraging LLM as a judge to analyze agent trajectories and provide granular evaluation across CTF solving steps. Second, we propose a novel metric, CTF Competency Index (CCI) for partial correctness, revealing how closely agent solutions align with human-crafted gold standards. Third, we examine how LLM hyperparameters, namely temperature, top-p, and maximum token length, influence agent performance and automated cybersecurity task planning. For rapid evaluation, we present CTFTiny, a curated benchmark of 50 representative CTF challenges across binary exploitation, web, reverse engineering, forensics, and cryptography. Our findings identify optimal multi-agent coordination settings and lay the groundwork for future LLM agent research in cybersecurity. We make CTFTiny open source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on https://github.com/NYU-LLM-CTF/CTFJudge.", "source": "arxiv", "arxiv_id": "2508.05674v1", "pdf_url": "https://arxiv.org/pdf/2508.05674v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-08-05T03:25:09Z", "updated": "2025-08-05T03:25:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems", "authors": ["Tendai Mukande", "Esraa Ali", "Annalina Caputo", "Ruihai Dong", "Noel OConnor"], "year": 2025, "url": "http://arxiv.org/abs/2512.06590v1", "abstract": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.", "source": "arxiv", "arxiv_id": "2512.06590v1", "pdf_url": "https://arxiv.org/pdf/2512.06590v1", "categories": ["cs.IR", "cs.AI", "cs.MA"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-12-06T23:04:49Z", "updated": "2025-12-06T23:04:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards LLM Agents for Earth Observation", "authors": ["Chia Hsiang Kao", "Wenting Zhao", "Shreelekha Revankar", "Samuel Speas", "Snehal Bhagat", "Rajeev Datta", "Cheng Perng Phoo", "Utkarsh Mall", "Carl Vondrick", "Kavita Bala", "Bharath Hariharan"], "year": 2025, "url": "http://arxiv.org/abs/2504.12110v2", "abstract": "Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth.", "source": "arxiv", "arxiv_id": "2504.12110v2", "pdf_url": "https://arxiv.org/pdf/2504.12110v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-16T14:19:25Z", "updated": "2025-09-12T19:16:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play", "authors": ["Jiaxun Cui", "Chen Tang", "Jarrett Holtz", "Janice Nguyen", "Alessandro G. Allievi", "Hang Qiu", "Peter Stone"], "year": 2025, "url": "http://arxiv.org/abs/2505.18334v1", "abstract": "Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot LLM agent. Our code and demo videos are available at https://talking-vehicles.github.io/.", "source": "arxiv", "arxiv_id": "2505.18334v1", "pdf_url": "https://arxiv.org/pdf/2505.18334v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2025-05-23T19:40:09Z", "updated": "2025-05-23T19:40:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Robust Visual Continual Learning with Multi-Prototype Supervision", "authors": ["Xiwei Liu", "Yulong Li", "Yichen Li", "Xinlin Zhuang", "Haolin Yang", "Huifa Li", "Imran Razzak"], "year": 2025, "url": "http://arxiv.org/abs/2509.16011v1", "abstract": "Language-guided supervision, which utilizes a frozen semantic target from a Pretrained Language Model (PLM), has emerged as a promising paradigm for visual Continual Learning (CL). However, relying on a single target introduces two critical limitations: 1) semantic ambiguity, where a polysemous category name results in conflicting visual representations, and 2) intra-class visual diversity, where a single prototype fails to capture the rich variety of visual appearances within a class. To this end, we propose MuproCL, a novel framework that replaces the single target with multiple, context-aware prototypes. Specifically, we employ a lightweight LLM agent to perform category disambiguation and visual-modal expansion to generate a robust set of semantic prototypes. A LogSumExp aggregation mechanism allows the vision model to adaptively align with the most relevant prototype for a given image. Extensive experiments across various CL baselines demonstrate that MuproCL consistently enhances performance and robustness, establishing a more effective path for language-guided continual learning.", "source": "arxiv", "arxiv_id": "2509.16011v1", "pdf_url": "https://arxiv.org/pdf/2509.16011v1", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-09-19T14:24:48Z", "updated": "2025-09-19T14:24:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning", "authors": ["Linze Chen", "Yufan Cai", "Zhe Hou", "Jinsong Dong"], "year": 2025, "url": "http://arxiv.org/abs/2511.21033v1", "abstract": "The rationality of law manifests in two forms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires legal decisions to follow explicitly stated, general, and logically coherent rules. Existing LLM-based systems excel at surface-level text analysis but lack the guarantees required for principled jurisprudence. We introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver-backed proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification. The pipeline consists of three phases: (1) Statute Formalization, where domain-specific prompts convert legal provisions into logical formulae; (2) Dual Fact and Statute Extraction, in which prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation; and (3) Solver-Centric Adjudication, where an autoformalizer compiles both parties' arguments into logic constraints, and unsat cores trigger iterative self-critique until a satisfiable formula is achieved, which is then verbalized by a Judge-LLM into a transparent verdict and optimized sentence. Experimental results on public benchmarks show that our system surpasses advanced LLMs including GPT-o4-mini, DeepSeek-V3, and Claude 4 as well as state-of-the-art Legal AI baselines, while providing rigorous and explainable symbolic justifications.", "source": "arxiv", "arxiv_id": "2511.21033v1", "pdf_url": "https://arxiv.org/pdf/2511.21033v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-26T04:05:06Z", "updated": "2025-11-26T04:05:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents", "authors": ["Chaoran Chen", "Bingsheng Yao", "Ruishi Zou", "Wenyue Hua", "Weimin Lyu", "Yanfang Ye", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.13012v3", "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.", "source": "arxiv", "arxiv_id": "2502.13012v3", "pdf_url": "https://arxiv.org/pdf/2502.13012v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-02-18T16:33:33Z", "updated": "2025-03-27T04:07:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools", "authors": ["SÃ¶ren Arlt", "Xuemei Gu", "Mario Krenn"], "year": 2025, "url": "http://arxiv.org/abs/2511.11752v1", "abstract": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.", "source": "arxiv", "arxiv_id": "2511.11752v1", "pdf_url": "https://arxiv.org/pdf/2511.11752v1", "categories": ["cs.AI", "cs.DL", "quant-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-11-13T18:18:58Z", "updated": "2025-11-13T18:18:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "year": 2025, "url": "http://arxiv.org/abs/2508.18370v2", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.", "source": "arxiv", "arxiv_id": "2508.18370v2", "pdf_url": "https://arxiv.org/pdf/2508.18370v2", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-08-25T18:02:23Z", "updated": "2025-09-23T03:30:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning", "authors": ["Hanjiang Hu", "Changliu Liu", "Na Li", "Yebin Wang"], "year": 2025, "url": "http://arxiv.org/abs/2509.20616v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.", "source": "arxiv", "arxiv_id": "2509.20616v2", "pdf_url": "https://arxiv.org/pdf/2509.20616v2", "categories": ["cs.LG", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-24T23:47:36Z", "updated": "2025-12-08T18:53:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Tree Search for LLM Agent Reinforcement Learning", "authors": ["Yuxiang Ji", "Ziyu Ma", "Yong Wang", "Guanhua Chen", "Xiangxiang Chu", "Liaoni Wu"], "year": 2025, "url": "http://arxiv.org/abs/2509.21240v2", "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.", "source": "arxiv", "arxiv_id": "2509.21240v2", "pdf_url": "https://arxiv.org/pdf/2509.21240v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-09-25T14:37:09Z", "updated": "2025-10-11T09:55:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning", "authors": ["Jiawei Zhang", "Shuang Yang", "Bo Li"], "year": 2025, "url": "http://arxiv.org/abs/2503.01908v3", "abstract": "Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Nevertheless, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents extensively reason or plan before executing final actions. In this work, we present UDora, a unified red teaming framework designed for LLM agents that dynamically hijacks the agent's reasoning processes to compel malicious behavior. Specifically, UDora first generates the model's reasoning trace for the given task, then automatically identifies optimal points within this trace to insert targeted perturbations. The resulting perturbed reasoning is then used as a surrogate response for optimization. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets. The code is available at https://github.com/AI-secure/UDora.", "source": "arxiv", "arxiv_id": "2503.01908v3", "pdf_url": "https://arxiv.org/pdf/2503.01908v3", "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-28T21:30:28Z", "updated": "2025-11-12T01:53:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2504.09407v3", "abstract": "Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate their new designs. But what about evaluating and iterating the usability testing study design itself? Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and iterating their study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users and to interactively test the target website. The system also provides a Result Viewer Interface so that the UX researchers can easily review and analyze the generated qualitative (e.g., agents' post-study surveys) and quantitative data (e.g., agents' interaction logs), or even interview agents directly. Through a heuristic evaluation with 16 UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.", "source": "arxiv", "arxiv_id": "2504.09407v3", "pdf_url": "https://arxiv.org/pdf/2504.09407v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-13T02:34:22Z", "updated": "2025-09-19T17:52:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design", "authors": ["Yuxuan Lu", "Bingsheng Yao", "Hansu Gu", "Jing Huang", "Jessie Wang", "Yang Li", "Jiri Gesi", "Qi He", "Toby Jia-Jun Li", "Dakuo Wang"], "year": 2025, "url": "http://arxiv.org/abs/2502.12561v3", "abstract": "Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.", "source": "arxiv", "arxiv_id": "2502.12561v3", "pdf_url": "https://arxiv.org/pdf/2502.12561v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "10.1145/3706599.3719729", "venue": "", "published": "2025-02-18T05:55:18Z", "updated": "2025-04-05T01:55:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics", "authors": ["Trung-Kiet Huynh", "Duy-Minh Dao-Sy", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Phu-Quy Nguyen-Lam", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Phu-Hoa Pham", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "year": 2025, "url": "http://arxiv.org/abs/2512.07462v2", "abstract": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.", "source": "arxiv", "arxiv_id": "2512.07462v2", "pdf_url": "https://arxiv.org/pdf/2512.07462v2", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG", "math.DS"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-12-08T11:40:03Z", "updated": "2025-12-11T20:32:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming", "authors": ["Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haoyue Bai", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "year": 2025, "url": "http://arxiv.org/abs/2504.21304v1", "abstract": "Feature transformation involves generating a new set of features from the original dataset to enhance the data's utility. In certain domains like material performance screening, dimensionality is large and collecting labels is expensive and lengthy. It highly necessitates transforming feature spaces efficiently and without supervision to enhance data readiness and AI utility. However, existing methods fall short in efficient navigation of a vast space of feature combinations, and are mostly designed for supervised settings. To fill this gap, our unique perspective is to leverage a generator-critic duet-play teaming framework using LLM agents and in-context learning to derive pseudo-supervision from unsupervised data. The framework consists of three interconnected steps: (1) Critic agent diagnoses data to generate actionable advice, (2) Generator agent produces tokenized feature transformations guided by the critic's advice, and (3) Iterative refinement ensures continuous improvement through feedback between agents. The generator-critic framework can be generalized to human-agent collaborative generation, by replacing the critic agent with human experts. Extensive experiments demonstrate that the proposed framework outperforms even supervised baselines in feature transformation efficiency, robustness, and practical applicability across diverse datasets.", "source": "arxiv", "arxiv_id": "2504.21304v1", "pdf_url": "https://arxiv.org/pdf/2504.21304v1", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-04-30T04:26:03Z", "updated": "2025-04-30T04:26:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Unveiling Privacy Risks in LLM Agent Memory", "authors": ["Bo Wang", "Weiyi He", "Shenglai Zeng", "Zhen Xiang", "Yue Xing", "Jiliang Tang", "Pengfei He"], "year": 2025, "url": "http://arxiv.org/abs/2502.13172v2", "abstract": "Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent designer's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.", "source": "arxiv", "arxiv_id": "2502.13172v2", "pdf_url": "https://arxiv.org/pdf/2502.13172v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-02-17T19:55:53Z", "updated": "2025-06-03T17:08:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking", "authors": ["Jingyi Huang", "Yuyi Yang", "Mengmeng Ji", "Charles Alba", "Sheng Zhang", "Ruopeng An"], "year": 2025, "url": "http://arxiv.org/abs/2512.00007v1", "abstract": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.", "source": "arxiv", "arxiv_id": "2512.00007v1", "pdf_url": "https://arxiv.org/pdf/2512.00007v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2025-10-10T15:10:46Z", "updated": "2025-10-10T15:10:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces", "authors": ["Alaa Saleh", "Sasu Tarkoma", "Praveen Kumar Donta", "Naser Hossein Motlagh", "Schahram Dustdar", "Susanna Pirttikangas", "Lauri LovÃ©n"], "year": 2025, "url": "http://arxiv.org/abs/2505.00472v1", "abstract": "Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.", "source": "arxiv", "arxiv_id": "2505.00472v1", "pdf_url": "https://arxiv.org/pdf/2505.00472v1", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-05-01T11:54:49Z", "updated": "2025-05-01T11:54:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language", "authors": ["Sijin Sun", "Liangbin Zhao", "Ming Deng", "Xiuju Fu"], "year": 2025, "url": "http://arxiv.org/abs/2505.00989v1", "abstract": "Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.", "source": "arxiv", "arxiv_id": "2505.00989v1", "pdf_url": "https://arxiv.org/pdf/2505.00989v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-05-02T04:27:50Z", "updated": "2025-05-02T04:27:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "year": 2025, "url": "http://arxiv.org/abs/2507.11979v2", "abstract": "Large language models (LLMs) have emerged as powerful tools for simulating complex social phenomena using human-like agents with specific traits. In human societies, value similarity is important for building trust and close relationships; however, it remains unexplored whether this principle holds true in artificial societies comprising LLM agents. Therefore, this study investigates the influence of value similarity on relationship-building among LLM agents through two experiments. First, in a preliminary experiment, we evaluated the controllability of values in LLMs to identify the most effective model and prompt design for controlling the values. Subsequently, in the main experiment, we generated pairs of LLM agents imbued with specific values and analyzed their mutual evaluations of trust and interpersonal closeness following a dialogue. The experiments were conducted in English and Japanese to investigate language dependence. The results confirmed that pairs of agents with higher value similarity exhibited greater mutual trust and interpersonal closeness. Our findings demonstrate that the LLM agent simulation serves as a valid testbed for social science theories, contributes to elucidating the mechanisms by which values influence relationship building, and provides a foundation for inspiring new theories and insights into the social sciences.", "source": "arxiv", "arxiv_id": "2507.11979v2", "pdf_url": "https://arxiv.org/pdf/2507.11979v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL", "doi": "10.1038/s41598-025-25531-1", "venue": "Scientific Reports volume 15, Article number: 41653 (2025)", "published": "2025-07-16T07:21:59Z", "updated": "2025-10-20T01:37:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation", "authors": ["Lesly Miculicich", "Mihir Parmar", "Hamid Palangi", "Krishnamurthy Dj Dvijotham", "Mirko Montanari", "Tomas Pfister", "Long T. Le"], "year": 2025, "url": "http://arxiv.org/abs/2510.05156v1", "abstract": "The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.", "source": "arxiv", "arxiv_id": "2510.05156v1", "pdf_url": "https://arxiv.org/pdf/2510.05156v1", "categories": ["cs.SE", "cs.AI", "cs.CR"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2025-10-03T04:11:43Z", "updated": "2025-10-03T04:11:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures", "authors": ["Yoo Yeon Sung", "Hannah Kim", "Dan Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2503.12651v1", "abstract": "AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system's overall performance. Addressing these failures through human intervention is challenging due to the agents' opaque reasoning processes, misalignment with human expectations, the complexity of agent dependencies, and the high cost of manual inspection. This paper thus introduces a human-centered evaluation framework for Verifying LLM Agent failures (VeriLA), which systematically assesses agent failures to reduce human effort and make these agent failures interpretable to humans. The framework first defines clear expectations of each agent by curating human-designed agent criteria. Then, it develops a human-aligned agent verifier module, trained with human gold standards, to assess each agent's execution output. This approach enables granular evaluation of each agent's performance by revealing failures from a human standard, offering clear guidelines for revision, and reducing human cognitive load. Our case study results show that VeriLA is both interpretable and efficient in helping practitioners interact more effectively with the system. By upholding accountability in human-agent collaboration, VeriLA paves the way for more trustworthy and human-aligned compound AI systems.", "source": "arxiv", "arxiv_id": "2503.12651v1", "pdf_url": "https://arxiv.org/pdf/2503.12651v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-16T21:11:18Z", "updated": "2025-03-16T21:11:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Vulnerability Mitigation System (VMS): LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Farzana Abdulzada"], "year": 2025, "url": "http://arxiv.org/abs/2507.21113v1", "abstract": "As the frequency of cyber threats increases, conventional penetration testing is failing to capture the entirety of todays complex environments. To solve this problem, we propose the Vulnerability Mitigation System (VMS), a novel agent based on a Large Language Model (LLM) capable of performing penetration testing without human intervention. The VMS has a two-part architecture for planning and a Summarizer, which enable it to generate commands and process feedback. To standardize testing, we designed two new Capture the Flag (CTF) benchmarks based on the PicoCTF and OverTheWire platforms with 200 challenges. These benchmarks allow us to evaluate how effectively the system functions. We performed a number of experiments using various LLMs while tuning the temperature and top-p parameters and found that GPT-4o performed best, sometimes even better than expected. The results indicate that LLMs can be effectively applied to many cybersecurity tasks; however, there are risks. To ensure safe operation, we used a containerized environment. Both the VMS and the benchmarks are publicly available, advancing the creation of secure, autonomous cybersecurity tools.", "source": "arxiv", "arxiv_id": "2507.21113v1", "pdf_url": "https://arxiv.org/pdf/2507.21113v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2025-07-14T06:19:17Z", "updated": "2025-07-14T06:19:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2504.15785v1", "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.", "source": "arxiv", "arxiv_id": "2504.15785v1", "pdf_url": "https://arxiv.org/pdf/2504.15785v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-22T10:58:27Z", "updated": "2025-04-22T10:58:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WINELL: Wikipedia Never-Ending Updating with LLM Agents", "authors": ["Revanth Gangi Reddy", "Tanay Dixit", "Jiaxin Qin", "Cheng Qian", "Daniel Lee", "Jiawei Han", "Kevin Small", "Xing Fan", "Ruhi Sarikaya", "Heng Ji"], "year": 2025, "url": "http://arxiv.org/abs/2508.03728v1", "abstract": "Wikipedia, a vast and continuously consulted knowledge base, faces significant challenges in maintaining up-to-date content due to its reliance on manual human editors. Inspired by the vision of continuous knowledge acquisition in NELL and fueled by advances in LLM-based agents, this paper introduces WiNELL, an agentic framework for continuously updating Wikipedia articles. Our approach employs a multi-agent framework to aggregate online information, select new and important knowledge for a target entity in Wikipedia, and then generate precise edit suggestions for human review. Our fine-grained editing models, trained on Wikipedia's extensive history of human edits, enable incorporating updates in a manner consistent with human editing behavior. Our editor models outperform both open-source instruction-following baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and editing efficiency. End-to-end evaluation on high-activity Wikipedia pages demonstrates WiNELL's ability to identify and suggest timely factual updates. This opens up a promising research direction in LLM agents for automatically updating knowledge bases in a never-ending fashion.", "source": "arxiv", "arxiv_id": "2508.03728v1", "pdf_url": "https://arxiv.org/pdf/2508.03728v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-07-30T07:51:42Z", "updated": "2025-07-30T07:51:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Warehouse Spatial Question Answering with LLM Agent", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "year": 2025, "url": "http://arxiv.org/abs/2507.10778v2", "abstract": "Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM finetuning to enhance MLLM's spatial understanding ability. In this paper, we present a data-efficient approach. We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation. The code is available at: https://github.com/hsiangwei0903/SpatialAgent", "source": "arxiv", "arxiv_id": "2507.10778v2", "pdf_url": "https://arxiv.org/pdf/2507.10778v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-07-14T20:05:55Z", "updated": "2025-08-14T03:48:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems", "authors": ["Junfeng Fang", "Zijun Yao", "Ruipeng Wang", "Haokai Ma", "Xiang Wang", "Tat-Seng Chua"], "year": 2025, "url": "http://arxiv.org/abs/2506.13666v1", "abstract": "The development of large language models (LLMs) has entered in a experience-driven era, flagged by the emergence of environment feedback-driven learning via reinforcement learning and tool-using agents. This encourages the emergenece of model context protocol (MCP), which defines the standard on how should a LLM interact with external services, such as \\api and data. However, as MCP becomes the de facto standard for LLM agent systems, it also introduces new safety risks. In particular, MCP introduces third-party services, which are not controlled by the LLM developers, into the agent systems. These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. In this position paper, we advocate the research community in LLM safety to pay close attention to the new safety risks issues introduced by MCP, and develop new techniques to build safe MCP-powered agent systems. To establish our position, we argue with three key parts. (1) We first construct \\framework, a controlled framework to examine safety issues in MCP-powered agent systems. (2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial. (3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered agent systems. In particular, we would call for researchers to persue the following research directions: red teaming, MCP safe LLM development, MCP safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP safe ecosystem construction. We hope this position paper can raise the awareness of the research community in MCP safety and encourage more researchers to join this important research direction. Our code is available at https://github.com/littlelittlenine/SafeMCP.git.", "source": "arxiv", "arxiv_id": "2506.13666v1", "pdf_url": "https://arxiv.org/pdf/2506.13666v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-06-16T16:24:31Z", "updated": "2025-06-16T16:24:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "year": 2025, "url": "http://arxiv.org/abs/2510.22732v2", "abstract": "Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.", "source": "arxiv", "arxiv_id": "2510.22732v2", "pdf_url": "https://arxiv.org/pdf/2510.22732v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-10-26T16:03:39Z", "updated": "2025-12-19T23:16:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "authors": ["Arth Bohra", "Manvel Saroyan", "Danil Melkozerov", "Vahe Karufanyan", "Gabriel Maher", "Pascal Weinberger", "Artem Harutyunyan", "Giovanni Campagna"], "year": 2025, "url": "http://arxiv.org/abs/2504.12682v1", "abstract": "Most recent web agent research has focused on navigation and transaction tasks, with little emphasis on extracting structured data at scale. We present WebLists, a benchmark of 200 data-extraction tasks across four common business and enterprise use-cases. Each task requires an agent to navigate to a webpage, configure it appropriately, and extract complete datasets with well-defined schemas. We show that both LLMs with search capabilities and SOTA web agents struggle with these tasks, with a recall of 3% and 31%, respectively, despite higher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that enables web agents to convert their execution into repeatable programs, and replay them at scale across pages with similar structure. BardeenAgent is also the first LLM agent to take advantage of the regular structure of HTML. In particular BardeenAgent constructs a generalizable CSS selector to capture all relevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more than doubling the performance of SOTA web agents, and reducing cost per output row by 3x.", "source": "arxiv", "arxiv_id": "2504.12682v1", "pdf_url": "https://arxiv.org/pdf/2504.12682v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-04-17T06:16:40Z", "updated": "2025-04-17T06:16:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "authors": ["Stefan Szeider"], "year": 2025, "url": "http://arxiv.org/abs/2509.21224v1", "abstract": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.", "source": "arxiv", "arxiv_id": "2509.21224v1", "pdf_url": "https://arxiv.org/pdf/2509.21224v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-25T14:29:49Z", "updated": "2025-09-25T14:29:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness", "authors": ["Yuxuan Li", "Sauvik Das", "Hirokazu Shirado"], "year": 2025, "url": "http://arxiv.org/abs/2509.21868v1", "abstract": "There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.", "source": "arxiv", "arxiv_id": "2509.21868v1", "pdf_url": "https://arxiv.org/pdf/2509.21868v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-09-26T04:51:34Z", "updated": "2025-09-26T04:51:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms", "authors": ["Qibing Ren", "Zhijie Zheng", "Jiaxuan Guo", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "year": 2025, "url": "http://arxiv.org/abs/2511.06448v1", "abstract": "In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.", "source": "arxiv", "arxiv_id": "2511.06448v1", "pdf_url": "https://arxiv.org/pdf/2511.06448v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-11-09T16:30:44Z", "updated": "2025-11-09T16:30:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents", "authors": ["Tsimur Hadeliya", "Mohammad Ali Jauhar", "Nidhi Sakpal", "Diogo Cruz"], "year": 2025, "url": "http://arxiv.org/abs/2512.02445v1", "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.", "source": "arxiv", "arxiv_id": "2512.02445v1", "pdf_url": "https://arxiv.org/pdf/2512.02445v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-12-02T06:12:02Z", "updated": "2025-12-02T06:12:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "When Trust Collides: Decoding Human-LLM Cooperation Dynamics through the Prisoner's Dilemma", "authors": ["Guanxuan Jiang", "Shirao Yang", "Yuyang Wang", "Pan Hui"], "year": 2025, "url": "http://arxiv.org/abs/2503.07320v2", "abstract": "As large language models (LLMs) become increasingly capable of autonomous decision-making, they introduce new challenges and opportunities for human-AI cooperation in mixed-motive contexts. While prior research has primarily examined AI in assistive or cooperative roles, little is known about how humans interact with AI agents perceived as independent and strategic actors. This study investigates human cooperative attitudes and behaviors toward LLM agents by engaging 30 participants (15 males, 15 females) in repeated Prisoner's Dilemma games with agents differing in declared identity: purported human, rule-based AI, and LLM agent. Behavioral metrics, including cooperation rate, decision latency, unsolicited cooperative acts and trust restoration tolerance, were analyzed to assess the influence of agent identity and participant gender. Results revealed significant effects of declared agent identity on most cooperation-related behaviors, along with notable gender differences in decision latency. Furthermore, qualitative responses suggest that these behavioral differences were shaped by participants interpretations and expectations of the agents. These findings contribute to our understanding of human adaptation in competitive cooperation with autonomous agents and underscore the importance of agent framing in shaping effective and ethical human-AI interaction.", "source": "arxiv", "arxiv_id": "2503.07320v2", "pdf_url": "https://arxiv.org/pdf/2503.07320v2", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2025-03-10T13:37:36Z", "updated": "2025-05-28T07:51:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Where LLM Agents Fail and How They can Learn From Failures", "authors": ["Kunlun Zhu", "Zijia Liu", "Bingxuan Li", "Muxin Tian", "Yingxuan Yang", "Jiaxun Zhang", "Pengrui Han", "Qipeng Xie", "Fuyang Cui", "Weijia Zhang", "Xiaoteng Ma", "Xiaodong Yu", "Gowtham Ramesh", "Jialian Wu", "Zicheng Liu", "Pan Lu", "James Zou", "Jiaxuan You"], "year": 2025, "url": "http://arxiv.org/abs/2509.25370v1", "abstract": "Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug", "source": "arxiv", "arxiv_id": "2509.25370v1", "pdf_url": "https://arxiv.org/pdf/2509.25370v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-09-29T18:20:27Z", "updated": "2025-09-29T18:20:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Why Do Language Model Agents Whistleblow?", "authors": ["Kushal Agrawal", "Frank Xiao", "Guido Bergman", "Asa Cooper Stickland"], "year": 2025, "url": "http://arxiv.org/abs/2511.17085v2", "abstract": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.", "source": "arxiv", "arxiv_id": "2511.17085v2", "pdf_url": "https://arxiv.org/pdf/2511.17085v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2025-11-21T09:40:52Z", "updated": "2025-12-27T21:05:34Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma", "authors": ["Richard Willis", "Yali Du", "Joel Z Leibo", "Michael Luck"], "year": 2025, "url": "http://arxiv.org/abs/2501.16173v1", "abstract": "As autonomous agents become more prevalent, understanding their collective behaviour in strategic interactions is crucial. This study investigates the emergent cooperative tendencies of systems of Large Language Model (LLM) agents in a social dilemma. Unlike previous research where LLMs output individual actions, we prompt state-of-the-art LLMs to generate complete strategies for iterated Prisoner's Dilemma. Using evolutionary game theory, we simulate populations of agents with different strategic dispositions (aggressive, cooperative, or neutral) and observe their evolutionary dynamics. Our findings reveal that different LLMs exhibit distinct biases affecting the relative success of aggressive versus cooperative strategies. This research provides insights into the potential long-term behaviour of systems of deployed LLM-based autonomous agents and highlights the importance of carefully considering the strategic environments in which they operate.", "source": "arxiv", "arxiv_id": "2501.16173v1", "pdf_url": "https://arxiv.org/pdf/2501.16173v1", "categories": ["cs.MA", "cs.GT"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2025-01-27T16:14:33Z", "updated": "2025-01-27T16:14:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "World Modelling Improves Language Model Agents", "authors": ["Shangmin Guo", "Omar Darwiche Domingues", "RaphaÃ«l Avalos", "Aaron Courville", "Florian Strub"], "year": 2025, "url": "http://arxiv.org/abs/2506.02918v2", "abstract": "Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment.", "source": "arxiv", "arxiv_id": "2506.02918v2", "pdf_url": "https://arxiv.org/pdf/2506.02918v2", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-06-03T14:20:59Z", "updated": "2025-09-19T03:54:30Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "World model inspired sarcasm reasoning with large language model agents", "authors": ["Keito Inoshita", "Shinnosuke Mizuno"], "year": 2025, "url": "http://arxiv.org/abs/2512.24329v1", "abstract": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.", "source": "arxiv", "arxiv_id": "2512.24329v1", "pdf_url": "https://arxiv.org/pdf/2512.24329v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-12-30T16:31:08Z", "updated": "2025-12-30T16:31:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents", "authors": ["Xinhang Liu", "Chi-Keung Tang", "Yu-Wing Tai"], "year": 2025, "url": "http://arxiv.org/abs/2502.15601v2", "abstract": "Constructing photorealistic virtual worlds has applications across various fields, but it often requires the extensive labor of highly trained professionals to operate conventional 3D modeling software. To democratize this process, we introduce WorldCraft, a system where large language model (LLM) agents leverage procedural generation to create indoor and outdoor scenes populated with objects, allowing users to control individual object attributes and the scene layout using intuitive natural language commands. In our framework, a coordinator agent manages the overall process and works with two specialized LLM agents to complete the scene creation: ForgeIt, which integrates an ever-growing manual through auto-verification to enable precise customization of individual objects, and ArrangeIt, which formulates hierarchical optimization problems to achieve a layout that balances ergonomic and aesthetic considerations. Additionally, our pipeline incorporates a trajectory control agent, allowing users to animate the scene and operate the camera through natural language interactions. Our system is also compatible with off-the-shelf deep 3D generators to enrich scene assets. Through evaluations and comparisons with state-of-the-art methods, we demonstrate the versatility of WorldCraft, ranging from single-object customization to intricate, large-scale interior and exterior scene designs. This system empowers non-professionals to bring their creative visions to life.", "source": "arxiv", "arxiv_id": "2502.15601v2", "pdf_url": "https://arxiv.org/pdf/2502.15601v2", "categories": ["cs.CV", "cs.AI", "cs.GR"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2025-02-21T17:18:30Z", "updated": "2025-02-28T08:49:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Your LLM Agents are Temporally Blind: The Misalignment Between Tool Use Decisions and Human Time Perception", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Parsa Hosseini", "Kazem Faghih", "Zahra Sodagar", "Wenxiao Wang", "Soheil Feizi"], "year": 2025, "url": "http://arxiv.org/abs/2510.23853v2", "abstract": "Large language model (LLM) agents are increasingly used to interact with and execute tasks in dynamic environments. However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages. We refer to this as \"temporal blindness\". This limitation hinders decisions about when to invoke tools, leading agents to either over-rely on stale context and skip needed tool calls, or under-rely on it and redundantly repeat tool calls. To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity. We collected human preferences between \"calling a tool\" and \"directly answering\" on each sample, and evaluated how well LLM tool-calling decisions align with human preferences under varying amounts of elapsed time. Our analysis reveals that existing models display poor alignment with human temporal perception, with no model achieving a normalized alignment rate better than 65% when given time stamp information. We also show that naive, prompt-based alignment techniques have limited effectiveness for most models, but specific post-training alignment can be a viable way to align multi-turn LLM tool use with human temporal perception. Our data and findings provide a first step toward understanding and mitigating temporal blindness, offering insights to foster the development of more time-aware and human-aligned agents.", "source": "arxiv", "arxiv_id": "2510.23853v2", "pdf_url": "https://arxiv.org/pdf/2510.23853v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-10-27T20:51:58Z", "updated": "2026-01-10T04:01:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning", "authors": ["Dongrong Yang", "Xin Wu", "Yibo Xie", "Xinyi Li", "Qiuwen Wu", "Jackie Wu", "Yang Sheng"], "year": 2025, "url": "http://arxiv.org/abs/2510.11754v1", "abstract": "Radiation therapy treatment planning is an iterative, expertise-dependent process, and the growing burden of cancer cases has made reliance on manual planning increasingly unsustainable, underscoring the need for automation. In this study, we propose a workflow that leverages a large language model (LLM)-based agent to navigate inverse treatment planning for intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to directly interact with a clinical treatment planning system (TPS) to iteratively extract intermediate plan states and propose new constraint values to guide inverse optimization. The agent's decision-making process is informed by current observations and previous optimization attempts and evaluations, allowing for dynamic strategy refinement. The planning process was performed in a zero-shot inference setting, where the LLM operated without prior exposure to manually generated treatment plans and was utilized without any fine-tuning or task-specific training. The LLM-generated plans were evaluated on twenty head-and-neck cancer cases against clinical manual plans, with key dosimetric endpoints analyzed and reported. The LLM-generated plans achieved comparable organ-at-risk (OAR) sparing relative to clinical plans while demonstrating improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity (conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV). This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for automated IMRT treatment planning in a commercial TPS. The proposed approach provides a generalizable and clinically applicable solution that could reduce planning variability and support broader adoption of AI-based planning strategies.", "source": "arxiv", "arxiv_id": "2510.11754v1", "pdf_url": "https://arxiv.org/pdf/2510.11754v1", "categories": ["physics.med-ph", "cs.AI", "cs.RO"], "primary_category": "physics.med-ph", "doi": "", "venue": "", "published": "2025-10-12T19:21:21Z", "updated": "2025-10-12T19:21:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation", "authors": ["Lim Chien Her", "Ming Yan", "Yunshu Bai", "Ruihao Li", "Hao Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2512.10501v2", "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.", "source": "arxiv", "arxiv_id": "2512.10501v2", "pdf_url": "https://arxiv.org/pdf/2512.10501v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-12-11T10:22:02Z", "updated": "2025-12-12T08:48:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "debug-gym: A Text-Based Environment for Interactive Debugging", "authors": ["Xingdi Yuan", "Morgane M Moss", "Charbel El Feghali", "Chinmay Singh", "Darya Moldavskaya", "Drew MacPhee", "Lucas Caccia", "Matheus Pereira", "Minseon Kim", "Alessandro Sordoni", "Marc-Alexandre CÃ´tÃ©"], "year": 2025, "url": "http://arxiv.org/abs/2503.21557v1", "abstract": "Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.", "source": "arxiv", "arxiv_id": "2503.21557v1", "pdf_url": "https://arxiv.org/pdf/2503.21557v1", "categories": ["cs.AI", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2025-03-27T14:43:28Z", "updated": "2025-03-27T14:43:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "year": 2025, "url": "http://arxiv.org/abs/2502.14662v4", "abstract": "Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.", "source": "arxiv", "arxiv_id": "2502.14662v4", "pdf_url": "https://arxiv.org/pdf/2502.14662v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-02-20T15:58:25Z", "updated": "2025-05-29T23:51:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "scAgent: Universal Single-Cell Annotation via a LLM Agent", "authors": ["Yuren Mao", "Yu Mi", "Peigen Liu", "Mengfei Zhang", "Hanqing Liu", "Yunjun Gao"], "year": 2025, "url": "http://arxiv.org/abs/2504.04698v1", "abstract": "Cell type annotation is critical for understanding cellular heterogeneity. Based on single-cell RNA-seq data and deep learning models, good progress has been made in annotating a fixed number of cell types within a specific tissue. However, universal cell annotation, which can generalize across tissues, discover novel cell types, and extend to novel cell types, remains less explored. To fill this gap, this paper proposes scAgent, a universal cell annotation framework based on Large Language Models (LLMs). scAgent can identify cell types and discover novel cell types in diverse tissues; furthermore, it is data efficient to learn novel cell types. Experimental studies in 160 cell types and 35 tissues demonstrate the superior performance of scAgent in general cell-type annotation, novel cell discovery, and extensibility to novel cell type.", "source": "arxiv", "arxiv_id": "2504.04698v1", "pdf_url": "https://arxiv.org/pdf/2504.04698v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2025-04-07T03:03:21Z", "updated": "2025-04-07T03:03:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples", "authors": ["Lihang Pan", "Yuxuan Li", "Chun Yu", "Yuanchun Shi"], "year": 2024, "url": "http://arxiv.org/abs/2404.15974v1", "abstract": "The capabilities of a single large language model (LLM) agent for solving a complex task are limited. Connecting multiple LLM agents to a network can effectively improve overall performance. However, building an LLM agent network (LAN) requires a substantial amount of time and effort. In this paper, we introduce EasyLAN, a human-computer collaborative tool that helps developers construct LANs. EasyLAN initially generates a LAN containing only one agent based on the description of the desired task. Subsequently, EasyLAN leverages a few training examples to update the LAN. For each example, EasyLAN models the gap between the output and the ground truth and identifies the causes of the errors. These errors are addressed through carefully designed strategies. Users can intervene in EasyLAN's workflow or directly modify the LAN. Eventually, the LAN evolves from a single agent to a network of LLM agents. The experimental results indicate that developers can rapidly construct LANs with good performance.", "source": "arxiv", "arxiv_id": "2404.15974v1", "pdf_url": "https://arxiv.org/pdf/2404.15974v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-24T16:49:24Z", "updated": "2024-04-24T16:49:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models", "authors": ["Chengxing Xie", "Difan Zou"], "year": 2024, "url": "http://arxiv.org/abs/2405.18208v1", "abstract": "Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.", "source": "arxiv", "arxiv_id": "2405.18208v1", "pdf_url": "https://arxiv.org/pdf/2405.18208v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-28T14:13:32Z", "updated": "2024-05-28T14:13:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents", "authors": ["Peiyuan Feng", "Yichen He", "Guanhua Huang", "Yuan Lin", "Hanchong Zhang", "Yuchen Zhang", "Hang Li"], "year": 2024, "url": "http://arxiv.org/abs/2405.14751v2", "abstract": "We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.", "source": "arxiv", "arxiv_id": "2405.14751v2", "pdf_url": "https://arxiv.org/pdf/2405.14751v2", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-05-23T16:17:44Z", "updated": "2024-11-05T09:42:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution", "authors": ["Zhiqiang Xie", "Hao Kang", "Ying Sheng", "Tushar Krishna", "Kayvon Fatahalian", "Christos Kozyrakis"], "year": 2024, "url": "http://arxiv.org/abs/2411.03519v1", "abstract": "With more advanced natural language understanding and reasoning capabilities, large language model (LLM)-powered agents are increasingly developed in simulated environments to perform complex tasks, interact with other agents, and exhibit emergent behaviors relevant to social science and gaming. However, current multi-agent simulations frequently suffer from inefficiencies due to the limited parallelism caused by false dependencies, resulting in performance bottlenecks. In this paper, we introduce AI Metropolis, a simulation engine that improves the efficiency of LLM agent simulations by incorporating out-of-order execution scheduling. By dynamically tracking real dependencies between agents, AI Metropolis minimizes false dependencies, enhancing parallelism and enabling efficient hardware utilization. Our evaluations demonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over standard parallel simulation with global synchronization, approaching optimal performance as the number of agents increases.", "source": "arxiv", "arxiv_id": "2411.03519v1", "pdf_url": "https://arxiv.org/pdf/2411.03519v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.DC", "doi": "", "venue": "", "published": "2024-11-05T21:54:14Z", "updated": "2024-11-05T21:54:14Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AIOS: LLM Agent Operating System", "authors": ["Kai Mei", "Xi Zhu", "Wujiang Xu", "Wenyue Hua", "Mingyu Jin", "Zelong Li", "Shuyuan Xu", "Ruosong Ye", "Yingqiang Ge", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.16971v5", "abstract": "LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) for runtime agents. To enhance usability, AIOS also includes an AIOS SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.", "source": "arxiv", "arxiv_id": "2403.16971v5", "pdf_url": "https://arxiv.org/pdf/2403.16971v5", "categories": ["cs.OS", "cs.AI", "cs.CL"], "primary_category": "cs.OS", "doi": "", "venue": "", "published": "2024-03-25T17:32:23Z", "updated": "2025-08-12T14:37:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Adaptive In-conversation Team Building for Language Model Agents", "authors": ["Linxin Song", "Jiale Liu", "Jieyu Zhang", "Shaokun Zhang", "Ao Luo", "Shijian Wang", "Qingyun Wu", "Chi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2405.19425v3", "abstract": "Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs, allowing for a flexible yet structured approach to problem-solving. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering. Our exploration of different backbone LLM and cost analysis further shows that Captain Agent can improve the conversation quality of weak LLM and achieve competitive performance with extremely low cost, which illuminates the application of multi-agent systems.", "source": "arxiv", "arxiv_id": "2405.19425v3", "pdf_url": "https://arxiv.org/pdf/2405.19425v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-29T18:08:37Z", "updated": "2025-03-02T06:36:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent", "authors": ["Janghoon Ock", "Radheesh Sharma Meda", "Tirtha Vinchurkar", "Yayati Jadhav", "Amir Barati Farimani"], "year": 2024, "url": "http://arxiv.org/abs/2410.16658v4", "abstract": "Adsorption energy is a key reactivity descriptor in catalysis. Determining adsorption energy requires evaluating numerous adsorbate-catalyst configurations, making it computationally intensive. Current methods rely on exhaustive sampling, which does not guarantee the identification of the global minimum energy. To address this, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify stable adsorption configurations corresponding to the global minimum energy. Adsorb-Agent leverages its built-in knowledge and reasoning to strategically explore configurations, significantly reducing the number of initial setups required while improving energy prediction accuracy. In this study, we also evaluated the performance of different LLMs, including GPT-4o, GPT-4o-mini, Claude-3.7-Sonnet, and DeepSeek-Chat, as the reasoning engine for Adsorb-Agent, with GPT-4o showing the strongest overall performance. Tested on twenty diverse systems, Adsorb-Agent identifies comparable adsorption energies for 84% of cases and achieves lower energies for 35%, particularly excelling in complex systems. It identifies lower energies in 47% of intermetallic systems and 67% of systems with large adsorbates. These findings demonstrate Adsorb-Agent's potential to accelerate catalyst discovery by reducing computational costs and enhancing prediction reliability compared to exhaustive search methods.", "source": "arxiv", "arxiv_id": "2410.16658v4", "pdf_url": "https://arxiv.org/pdf/2410.16658v4", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-22T03:19:16Z", "updated": "2025-07-08T03:12:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents", "authors": ["Zhexin Zhang", "Shiyao Cui", "Yida Lu", "Jingzhuo Zhou", "Junxiao Yang", "Hongning Wang", "Minlie Huang"], "year": 2024, "url": "http://arxiv.org/abs/2412.14470v2", "abstract": "As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through failure mode and helpfulness analysis, we summarize two fundamental safety defects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone may be insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. To drive progress in this area, Agent-SafetyBench has been released at https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further research in agent safety evaluation and improvement.", "source": "arxiv", "arxiv_id": "2412.14470v2", "pdf_url": "https://arxiv.org/pdf/2412.14470v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-19T02:35:15Z", "updated": "2025-05-20T05:58:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents", "authors": ["Chang Ma", "Junlei Zhang", "Zhihao Zhu", "Cheng Yang", "Yujiu Yang", "Yaohui Jin", "Zhenzhong Lan", "Lingpeng Kong", "Junxian He"], "year": 2024, "url": "http://arxiv.org/abs/2401.13178v2", "abstract": "Evaluating Large Language Models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.", "source": "arxiv", "arxiv_id": "2401.13178v2", "pdf_url": "https://arxiv.org/pdf/2401.13178v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-24T01:51:00Z", "updated": "2024-12-23T20:12:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "authors": ["Maksym Andriushchenko", "Alexandra Souly", "Mateusz Dziemian", "Derek Duenas", "Maxwell Lin", "Justin Wang", "Dan Hendrycks", "Andy Zou", "Zico Kolter", "Matt Fredrikson", "Eric Winsor", "Jerome Wynne", "Yarin Gal", "Xander Davies"], "year": 2024, "url": "http://arxiv.org/abs/2410.09024v3", "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.", "source": "arxiv", "arxiv_id": "2410.09024v3", "pdf_url": "https://arxiv.org/pdf/2410.09024v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-11T17:39:22Z", "updated": "2025-04-18T14:30:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System", "authors": ["Zhiwei Liu", "Weiran Yao", "Jianguo Zhang", "Liangwei Yang", "Zuxin Liu", "Juntao Tan", "Prafulla K. Choubey", "Tian Lan", "Jason Wu", "Huan Wang", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese"], "year": 2024, "url": "http://arxiv.org/abs/2402.15538v1", "abstract": "The booming success of LLMs initiates rapid development in LLM agents. Though the foundation of an LLM agent is the generative model, it is critical to devise the optimal reasoning strategies and agent architectures. Accordingly, LLM agent research advances from the simple chain-of-thought prompting to more complex ReAct and Reflection reasoning strategy; agent architecture also evolves from single agent generation to multi-agent conversation, as well as multi-LLM multi-agent group chat. However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents. Thus, we open-source a new AI agent library, AgentLite, which simplifies this process by offering a lightweight, user-friendly platform for innovating LLM agent reasoning, architectures, and applications with ease. AgentLite is a task-oriented framework designed to enhance the ability of agents to break down tasks and facilitate the development of multi-agent systems. Furthermore, we introduce multiple practical applications developed with AgentLite to demonstrate its convenience and flexibility. Get started now at: \\url{https://github.com/SalesforceAIResearch/AgentLite}.", "source": "arxiv", "arxiv_id": "2402.15538v1", "pdf_url": "https://arxiv.org/pdf/2402.15538v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-02-23T06:25:20Z", "updated": "2024-02-23T06:25:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentOps: Enabling Observability of LLM Agents", "authors": ["Liming Dong", "Qinghua Lu", "Liming Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2411.05285v2", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities across various domains, gaining extensive attention from academia and industry. However, these agents raise significant concerns on AI safety due to their autonomous and non-deterministic behavior, as well as continuous evolving nature . From a DevOps perspective, enabling observability in agents is necessary to ensuring AI safety, as stakeholders can gain insights into the agents' inner workings, allowing them to proactively understand the agents, detect anomalies, and prevent potential failures. Therefore, in this paper, we present a comprehensive taxonomy of AgentOps, identifying the artifacts and associated data that should be traced throughout the entire lifecycle of agents to achieve effective observability. The taxonomy is developed based on a systematic mapping study of existing AgentOps tools. Our taxonomy serves as a reference template for developers to design and implement AgentOps infrastructure that supports monitoring, logging, and analytics. thereby ensuring AI safety.", "source": "arxiv", "arxiv_id": "2411.05285v2", "pdf_url": "https://arxiv.org/pdf/2411.05285v2", "categories": ["cs.AI", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T02:31:03Z", "updated": "2024-11-30T02:55:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases", "authors": ["Zhaorun Chen", "Zhen Xiang", "Chaowei Xiao", "Dawn Song", "Bo Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.12784v1", "abstract": "LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.", "source": "arxiv", "arxiv_id": "2407.12784v1", "pdf_url": "https://arxiv.org/pdf/2407.12784v1", "categories": ["cs.LG", "cs.CR", "cs.IR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-17T17:59:47Z", "updated": "2024-07-17T17:59:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents", "authors": ["Luca Gioacchini", "Giuseppe Siracusano", "Davide Sanvito", "Kiril Gashteovski", "David Friede", "Roberto Bifulco", "Carolin Lawrence"], "year": 2024, "url": "http://arxiv.org/abs/2404.06411v1", "abstract": "The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.", "source": "arxiv", "arxiv_id": "2404.06411v1", "pdf_url": "https://arxiv.org/pdf/2404.06411v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-09T16:01:24Z", "updated": "2024-04-09T16:01:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "authors": ["Yu Shang", "Yu Li", "Keyu Zhao", "Likai Ma", "Jiahe Liu", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.06153v3", "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.", "source": "arxiv", "arxiv_id": "2410.06153v3", "pdf_url": "https://arxiv.org/pdf/2410.06153v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-08T15:52:42Z", "updated": "2025-02-27T13:33:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "An LLM Agent for Automatic Geospatial Data Analysis", "authors": ["Yuxing Chen", "Weijie Wang", "Sylvain Lobry", "Camille Kurtz"], "year": 2024, "url": "http://arxiv.org/abs/2410.18792v2", "abstract": "Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.", "source": "arxiv", "arxiv_id": "2410.18792v2", "pdf_url": "https://arxiv.org/pdf/2410.18792v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-10-24T14:47:25Z", "updated": "2024-10-25T09:00:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls", "authors": ["Yu Du", "Fangyun Wei", "Hongyang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.04253v1", "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.", "source": "arxiv", "arxiv_id": "2402.04253v1", "pdf_url": "https://arxiv.org/pdf/2402.04253v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-06T18:59:57Z", "updated": "2024-02-06T18:59:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening", "authors": ["Chengguang Gan", "Qinghao Zhang", "Tatsunori Mori"], "year": 2024, "url": "http://arxiv.org/abs/2401.08315v2", "abstract": "The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. This paper introduces a novel Large Language Models (LLMs) based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making. To evaluate our framework, we constructed a dataset from actual resumes and simulated a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.", "source": "arxiv", "arxiv_id": "2401.08315v2", "pdf_url": "https://arxiv.org/pdf/2401.08315v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-16T12:30:56Z", "updated": "2024-08-13T04:50:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL", "authors": ["Yifei Zhou", "Andrea Zanette", "Jiayi Pan", "Sergey Levine", "Aviral Kumar"], "year": 2024, "url": "http://arxiv.org/abs/2402.19446v1", "abstract": "A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).", "source": "arxiv", "arxiv_id": "2402.19446v1", "pdf_url": "https://arxiv.org/pdf/2402.19446v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-29T18:45:56Z", "updated": "2024-02-29T18:45:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents", "authors": ["Petr Anokhin", "Nikita Semenov", "Artyom Sorokin", "Dmitry Evseev", "Andrey Kravchenko", "Mikhail Burtsev", "Evgeny Burnaev"], "year": 2024, "url": "http://arxiv.org/abs/2407.04363v3", "abstract": "Advancements in the capabilities of Large Language Models (LLMs) have created a promising foundation for developing autonomous agents. With the right tools, these agents could learn to solve tasks in new environments by accumulating and updating their knowledge. Current LLM-based agents process past experiences using a full history of observations, summarization, retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment. We demonstrate that our Ariadne LLM agent, consisting of the proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks within interactive text game environments difficult even for human players. Results show that our approach markedly outperforms other established memory methods and strong RL baselines in a range of problems of varying complexity. Additionally, AriGraph demonstrates competitive performance compared to dedicated knowledge graph-based methods in static multi-hop question-answering.", "source": "arxiv", "arxiv_id": "2407.04363v3", "pdf_url": "https://arxiv.org/pdf/2407.04363v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-05T09:06:47Z", "updated": "2025-05-15T10:57:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory", "authors": ["Gordon Dai", "Weijia Zhang", "Jinhan Li", "Siqi Yang", "Chidera Onochie lbe", "Srihas Rao", "Arthur Caetano", "Misha Sra"], "year": 2024, "url": "http://arxiv.org/abs/2406.14373v2", "abstract": "The emergence of Large Language Models (LLMs) and advancements in Artificial Intelligence (AI) offer an opportunity for computational social science research at scale. Building upon prior explorations of LLM agent design, our work introduces a simulated agent society where complex social relationships dynamically form and evolve over time. Agents are imbued with psychological drives and placed in a sandbox survival environment. We conduct an evaluation of the agent society through the lens of Thomas Hobbes's seminal Social Contract Theory (SCT). We analyze whether, as the theory postulates, agents seek to escape a brutish \"state of nature\" by surrendering rights to an absolute sovereign in exchange for order and security. Our experiments unveil an alignment: Initially, agents engage in unrestrained conflict, mirroring Hobbes's depiction of the state of nature. However, as the simulation progresses, social contracts emerge, leading to the authorization of an absolute sovereign and the establishment of a peaceful commonwealth founded on mutual cooperation. This congruence between our LLM agent society's evolutionary trajectory and Hobbes's theoretical account indicates LLMs' capability to model intricate social dynamics and potentially replicate forces that shape human societies. By enabling such insights into group behavior and emergent societal phenomena, LLM-driven multi-agent simulations, while unable to simulate all the nuances of human behavior, may hold potential for advancing our understanding of social structures, group dynamics, and complex human systems.", "source": "arxiv", "arxiv_id": "2406.14373v2", "pdf_url": "https://arxiv.org/pdf/2406.14373v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-20T14:42:58Z", "updated": "2024-07-01T22:06:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues", "authors": ["Yuncheng Hua", "Lizhen Qu", "Gholamreza Haffari"], "year": 2024, "url": "http://arxiv.org/abs/2402.01737v3", "abstract": "We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. We have released our source code and the generated dataset at: https://github.com/tk1363704/SADAS.", "source": "arxiv", "arxiv_id": "2402.01737v3", "pdf_url": "https://arxiv.org/pdf/2402.01737v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.18653/v1/2024.findings-emnlp.473", "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024", "published": "2024-01-29T09:07:40Z", "updated": "2025-02-17T08:44:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents", "authors": ["Zelong Li", "Shuyuan Xu", "Kai Mei", "Wenyue Hua", "Balaji Rama", "Om Raheja", "Hao Wang", "He Zhu", "Yongfeng Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.12821v1", "abstract": "Recent advancements in Large Language Models (LLMs) have shown significant progress in understanding complex natural language. One important application of LLM is LLM-based AI Agent, which leverages the ability of LLM as well as external tools for complex-task solving. To make sure LLM Agents follow an effective and reliable procedure to solve the given task, manually designed workflows are usually used to guide the working mechanism of agents. However, manually designing the workflows requires considerable efforts and domain knowledge, making it difficult to develop and deploy agents on massive scales. To address these issues, we propose AutoFlow, a framework designed to automatically generate workflows for agents to solve complex tasks. AutoFlow takes natural language program as the format of agent workflow and employs a workflow optimization procedure to iteratively optimize the workflow quality. Besides, this work offers two workflow generation methods: fine-tuning-based and in-context-based methods, making the AutoFlow framework applicable to both open-source and closed-source LLMs. Experimental results show that our framework can produce robust and reliable agent workflows. We believe that the automatic generation and interpretation of workflows in natural language represent a promising paradigm for solving complex tasks, particularly with the rapid development of LLMs. The source code of this work is available at https://github.com/agiresearch/AutoFlow.", "source": "arxiv", "arxiv_id": "2407.12821v1", "pdf_url": "https://arxiv.org/pdf/2407.12821v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-01T21:05:02Z", "updated": "2024-07-01T21:05:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents", "authors": ["Yao Fu", "Dong-Ki Kim", "Jaekyeom Kim", "Sungryull Sohn", "Lajanugen Logeswaran", "Kyunghoon Bae", "Honglak Lee"], "year": 2024, "url": "http://arxiv.org/abs/2403.08978v2", "abstract": "Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.", "source": "arxiv", "arxiv_id": "2403.08978v2", "pdf_url": "https://arxiv.org/pdf/2403.08978v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-13T22:06:03Z", "updated": "2024-12-03T07:36:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning", "authors": ["Minghao Chen", "Yihang Li", "Yanting Yang", "Shiyu Yu", "Binbin Lin", "Xiaofei He"], "year": 2024, "url": "http://arxiv.org/abs/2405.16247v4", "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a *case-conditioned prompting* strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.", "source": "arxiv", "arxiv_id": "2405.16247v4", "pdf_url": "https://arxiv.org/pdf/2405.16247v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-25T14:11:44Z", "updated": "2024-11-10T12:54:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "authors": ["Shirley Wu", "Shiyu Zhao", "Qian Huang", "Kexin Huang", "Michihiro Yasunaga", "Kaidi Cao", "Vassilis N. Ioannidis", "Karthik Subbian", "Jure Leskovec", "James Zou"], "year": 2024, "url": "http://arxiv.org/abs/2406.11200v3", "abstract": "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demonstrate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.", "source": "arxiv", "arxiv_id": "2406.11200v3", "pdf_url": "https://arxiv.org/pdf/2406.11200v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-17T04:20:02Z", "updated": "2024-10-31T10:15:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "authors": ["Ken Gu", "Ruoxi Shang", "Ruien Jiang", "Keying Kuang", "Richard-John Lin", "Donghe Lyu", "Yue Mao", "Youran Pan", "Teng Wu", "Jiaqian Yu", "Yikun Zhang", "Tianmai M. Zhang", "Lanyi Zhu", "Mike A. Merrill", "Jeffrey Heer", "Tim Althoff"], "year": 2024, "url": "http://arxiv.org/abs/2408.09667v3", "abstract": "Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents' multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents' analysis approaches.", "source": "arxiv", "arxiv_id": "2408.09667v3", "pdf_url": "https://arxiv.org/pdf/2408.09667v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-19T02:59:35Z", "updated": "2025-11-10T06:38:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents", "authors": ["Yifei Wang", "Dizhan Xue", "Shengjie Zhang", "Shengsheng Qian"], "year": 2024, "url": "http://arxiv.org/abs/2406.03007v1", "abstract": "With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent", "source": "arxiv", "arxiv_id": "2406.03007v1", "pdf_url": "https://arxiv.org/pdf/2406.03007v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-05T07:14:28Z", "updated": "2024-06-05T07:14:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles", "authors": ["Xuchuan Li", "Fei Huang", "Jianrong Lv", "Zhixiong Xiao", "Guolong Li", "Yang Yue"], "year": 2024, "url": "http://arxiv.org/abs/2407.18932v2", "abstract": "Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations of mobility; and based on the patterns and the recursive reasoning process, MobAgent finally generates more authentic and personalized mobilities that reflect both individual differences and real-world constraints. We validate our framework with 0.2 million travel survey data, demonstrating its effectiveness in producing personalized and accurate travel diaries. This study highlights the capacity of LLMs to provide detailed and sophisticated understanding of human mobility through the real-world mobility data.", "source": "arxiv", "arxiv_id": "2407.18932v2", "pdf_url": "https://arxiv.org/pdf/2407.18932v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-07-10T09:11:57Z", "updated": "2024-08-05T15:59:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Behavior Trees Enable Structured Programming of Language Model Agents", "authors": ["Richard Kelley"], "year": 2024, "url": "http://arxiv.org/abs/2404.07439v1", "abstract": "Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise \"language-model agents.\" In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF.", "source": "arxiv", "arxiv_id": "2404.07439v1", "pdf_url": "https://arxiv.org/pdf/2404.07439v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-11T02:44:13Z", "updated": "2024-04-11T02:44:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback", "authors": ["Sanjiban Choudhury", "Paloma Sodhi"], "year": 2024, "url": "http://arxiv.org/abs/2410.05434v1", "abstract": "While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually improves LLM agents using feedback from AI expert teachers. Our key insight is to equip the expert teachers with a privileged state -- information that is available during training but hidden at test time. This allows even weak experts to provide precise guidance, significantly improving the student agent's performance without access to privileged information at test time. We evaluate LEAP on diverse decision-making benchmarks, including text-based games (ALFWorld), web navigation (WebShop), and interactive coding (Intercode Bash). Our experiments show that LEAP (1) outperforms behavior cloning and ReAct baselines (2) enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o), and (3) allows weak models to self-improve using privileged versions of themselves. We also provide a theoretical analysis showing that LEAP's success hinges on balancing privileged information with the student's realizability, which we empirically validate. Our code is available at https://leap-llm.github.io", "source": "arxiv", "arxiv_id": "2410.05434v1", "pdf_url": "https://arxiv.org/pdf/2410.05434v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-07T18:55:53Z", "updated": "2024-10-07T18:55:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents", "authors": ["Fanzeng Xia", "Hao Liu", "Yisong Yue", "Tongxin Li"], "year": 2024, "url": "http://arxiv.org/abs/2407.01887v4", "abstract": "In-Context Reinforcement Learning (ICRL) is a frontier paradigm to solve Reinforcement Learning (RL) problems in the foundation model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of Large Language Models (LLMs) out-of-the-box remains largely unexplored. This paper investigates whether LLMs can generalize cross-domain to perform ICRL under the problem of Dueling Bandits (DB), a stateless preference-based RL setting. We find that the top-performing LLMs exhibit a notable zero-shot capacity for relative decision-making, which translates to low short-term weak regret across all DB environment instances by quickly including the best arm in duels. However, an optimality gap still exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations. To bridge this gap, we propose an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithm support with LLM agents through fine-grained adaptive interplay. We show that LEAD has theoretical guarantees inherited from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of such an agentic framework sheds light on how to enhance the trustworthiness of general-purpose LLMs generalized to in-context decision-making tasks.", "source": "arxiv", "arxiv_id": "2407.01887v4", "pdf_url": "https://arxiv.org/pdf/2407.01887v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-02T02:18:14Z", "updated": "2025-06-09T14:56:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification", "authors": ["Boyang Zhang", "Yicong Tan", "Yun Shen", "Ahmed Salem", "Michael Backes", "Savvas Zannettou", "Yang Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.20859v1", "abstract": "Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications. These agents can extend the base LLM's capabilities in multiple ways. For example, a well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced GPT-4 model by leveraging external components. More importantly, the usage of tools enables these systems to perform actions in the real world, moving from merely generating text to actively interacting with their environment. Given the agents' practical applications and their ability to execute consequential actions, it is crucial to assess potential vulnerabilities. Such autonomous systems can cause more severe damage than a standalone language model if compromised. While some existing research has explored harmful actions by LLM agents, our study approaches the vulnerability from a different perspective. We introduce a new type of attack that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions. We conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility. Our experiments reveal that these attacks can induce failure rates exceeding 80\\% in multiple scenarios. Through attacks on implemented and deployable agents in multi-agent scenarios, we accentuate the realistic risks associated with these vulnerabilities. To mitigate such attacks, we propose self-examination detection methods. However, our findings indicate these attacks are difficult to detect effectively using LLMs alone, highlighting the substantial risks associated with this vulnerability.", "source": "arxiv", "arxiv_id": "2407.20859v1", "pdf_url": "https://arxiv.org/pdf/2407.20859v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-07-30T14:35:31Z", "updated": "2024-07-30T14:35:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ByteComposer: a Human-like Melody Composition Method based on Language Model Agent", "authors": ["Xia Liang", "Xingjian Du", "Jiaju Lin", "Pei Zou", "Yuan Wan", "Bilei Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2402.17785v2", "abstract": "Large Language Models (LLM) have shown encouraging progress in multimodal understanding and generation tasks. However, how to design a human-aligned and interpretable melody composition system is still under-explored. To solve this problem, we propose ByteComposer, an agent framework emulating a human's creative pipeline in four separate steps : \"Conception Analysis - Draft Composition - Self-Evaluation and Modification - Aesthetic Selection\". This framework seamlessly blends the interactive and knowledge-understanding features of LLMs with existing symbolic music generation models, thereby achieving a melody composition agent comparable to human creators. We conduct extensive experiments on GPT4 and several open-source large language models, which substantiate our framework's effectiveness. Furthermore, professional music composers were engaged in multi-dimensional evaluations, the final results demonstrated that across various facets of music composition, ByteComposer agent attains the level of a novice melody composer.", "source": "arxiv", "arxiv_id": "2402.17785v2", "pdf_url": "https://arxiv.org/pdf/2402.17785v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD", "doi": "", "venue": "", "published": "2024-02-24T04:35:07Z", "updated": "2024-03-07T00:32:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Sidharth Dhawan", "Yixin Mao", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Philippe Laban", "Chien-Sheng Wu"], "year": 2024, "url": "http://arxiv.org/abs/2411.02305v2", "abstract": "Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 40% of the tasks with ReAct prompting, and less than 55% even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments. CRMArena is an open challenge to the community: systems that can reliably complete tasks showcase direct business value in a popular work environment.", "source": "arxiv", "arxiv_id": "2411.02305v2", "pdf_url": "https://arxiv.org/pdf/2411.02305v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-04T17:30:51Z", "updated": "2025-02-16T17:16:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks", "authors": ["Minrui Xu", "Dusit Niyato", "Hongliang Zhang", "Jiawen Kang", "Zehui Xiong", "Shiwen Mao", "Zhu Han"], "year": 2024, "url": "http://arxiv.org/abs/2403.05826v2", "abstract": "Edge intelligence in space-air-ground integrated networks (SAGINs) can enable worldwide network coverage beyond geographical limitations for users to access ubiquitous and low-latency intelligence services. Facing global coverage and complex environments in SAGINs, edge intelligence can provision approximate large language models (LLMs) agents for users via edge servers at ground base stations (BSs) or cloud data centers relayed by satellites. As LLMs with billions of parameters are pre-trained on vast datasets, LLM agents have few-shot learning capabilities, e.g., chain-of-thought (CoT) prompting for complex tasks, which raises a new trade-off between resource consumption and performance in SAGINs. In this paper, we propose a joint caching and inference framework for edge intelligence to provision sustainable and ubiquitous LLM agents in SAGINs. We introduce \"cached model-as-a-resource\" for offering LLMs with limited context windows and propose a novel optimization framework, i.e., joint model caching and inference, to utilize cached model resources for provisioning LLM agent services along with communication, computing, and storage resources. We design \"age of thought\" (AoT) considering the CoT prompting of LLMs, and propose a least AoT cached model replacement algorithm for optimizing the provisioning cost. We propose a deep Q-network-based modified second-bid (DQMSB) auction to incentivize network operators, which can enhance allocation efficiency by 23% while guaranteeing strategy-proofness and free from adverse selection.", "source": "arxiv", "arxiv_id": "2403.05826v2", "pdf_url": "https://arxiv.org/pdf/2403.05826v2", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-03-09T07:37:13Z", "updated": "2024-05-31T14:14:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can Large Language Model Agents Simulate Human Trust Behavior?", "authors": ["Chengxing Xie", "Canyu Chen", "Feiran Jia", "Ziyu Ye", "Shiyang Lai", "Kai Shu", "Jindong Gu", "Adel Bibi", "Ziniu Hu", "David Jurgens", "James Evans", "Philip Torr", "Bernard Ghanem", "Guohao Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.04559v4", "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.", "source": "arxiv", "arxiv_id": "2402.04559v4", "pdf_url": "https://arxiv.org/pdf/2402.04559v4", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-07T03:37:19Z", "updated": "2024-11-01T16:10:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games", "authors": ["Ji Ma"], "year": 2024, "url": "http://arxiv.org/abs/2410.21359v3", "abstract": "As Large Language Model (LLM)-based agents increasingly engage with human society, how well do we understand their prosocial behaviors? We (1) investigate how LLM agents' prosocial behaviors can be induced by different personas and benchmarked against human behaviors; and (2) introduce a social science approach to evaluate LLM agents' decision-making. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal that merely assigning a human-like identity to LLMs does not produce human-like behaviors. These findings suggest that LLM agents' reasoning does not consistently exhibit textual markers of human decision-making in dictator games and that their alignment with human behavior varies substantially across model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. As society increasingly integrates machine intelligence, \"Prosocial AI\" emerges as a promising and urgent research direction in philanthropic studies.", "source": "arxiv", "arxiv_id": "2410.21359v3", "pdf_url": "https://arxiv.org/pdf/2410.21359v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "econ.GN"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-28T17:47:41Z", "updated": "2025-11-17T20:55:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example", "authors": ["Yanan Chen", "Ali Pesaranghader", "Tanmana Sadhu", "Dong Hoon Yi"], "year": 2024, "url": "http://arxiv.org/abs/2408.06318v1", "abstract": "Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities. There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans. We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning? (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context? (3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement? Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT). Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.", "source": "arxiv", "arxiv_id": "2408.06318v1", "pdf_url": "https://arxiv.org/pdf/2408.06318v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-12T17:39:01Z", "updated": "2024-08-12T17:39:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities", "authors": ["Andrey Anurin", "Jonathan Ng", "Kibo Schaffer", "Jason Schreiber", "Esben Kran"], "year": 2024, "url": "http://arxiv.org/abs/2410.09114v2", "abstract": "LLM agents have the potential to revolutionize defensive cyber operations, but their offensive capabilities are not yet fully understood. To prepare for emerging threats, model developers and governments are evaluating the cyber capabilities of foundation models. However, these assessments often lack transparency and a comprehensive focus on offensive capabilities. In response, we introduce the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of modern LLMs on 3CB reveals that frontier models, such as GPT-4o and Claude 3.5 Sonnet, can perform offensive tasks such as reconnaissance and exploitation across domains ranging from binary analysis to web technologies. Conversely, smaller open-source models exhibit limited offensive capabilities. Our software solution and the corresponding benchmark provides a critical tool to reduce the gap between rapidly improving capabilities and robustness of cyber offense evaluations, aiding in the safer deployment and regulation of these powerful technologies.", "source": "arxiv", "arxiv_id": "2410.09114v2", "pdf_url": "https://arxiv.org/pdf/2410.09114v2", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-10T12:06:48Z", "updated": "2024-11-02T09:35:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary", "authors": ["Yutong Li", "Lu Chen", "Aiwei Liu", "Kai Yu", "Lijie Wen"], "year": 2024, "url": "http://arxiv.org/abs/2403.02574v1", "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.", "source": "arxiv", "arxiv_id": "2403.02574v1", "pdf_url": "https://arxiv.org/pdf/2403.02574v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-03-05T01:13:56Z", "updated": "2024-03-05T01:13:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ChatPattern: Layout Pattern Customization via Natural Language", "authors": ["Zixiao Wang", "Yunheng Shen", "Xufeng Yao", "Wenqian Zhao", "Yang Bai", "Farzan Farnia", "Bei Yu"], "year": 2024, "url": "http://arxiv.org/abs/2403.15434v1", "abstract": "Existing works focus on fixed-size layout pattern generation, while the more practical free-size pattern generation receives limited attention. In this paper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered framework for flexible pattern customization. ChatPattern utilizes a two-part system featuring an expert LLM agent and a highly controllable layout pattern generator. The LLM agent can interpret natural language requirements and operate design tools to meet specified needs, while the generator excels in conditional layout generation, pattern modification, and memory-friendly patterns extension. Experiments on challenging pattern generation setting shows the ability of ChatPattern to synthesize high-quality large-scale patterns.", "source": "arxiv", "arxiv_id": "2403.15434v1", "pdf_url": "https://arxiv.org/pdf/2403.15434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-15T09:15:22Z", "updated": "2024-03-15T09:15:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ChemMiner: A Large Language Model Agent System for Chemical Literature Data Mining", "authors": ["Kexin Chen", "Yuyang Du", "Junyou Li", "Hanqun Cao", "Menghao Guo", "Xilin Dang", "Lanqing Li", "Jiezhong Qiu", "Pheng Ann Heng", "Guangyong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.12993v2", "abstract": "The development of AI-assisted chemical synthesis tools requires comprehensive datasets covering diverse reaction types, yet current high-throughput experimental (HTE) approaches are expensive and limited in scope. Chemical literature represents a vast, underexplored data source containing thousands of reactions published annually. However, extracting reaction information from literature faces significant challenges including varied writing styles, complex coreference relationships, and multimodal information presentation. This paper proposes ChemMiner, a novel end-to-end framework leveraging multiple agents powered by large language models (LLMs) to extract high-fidelity chemical data from literature. ChemMiner incorporates three specialized agents: a text analysis agent for coreference mapping, a multimodal agent for non-textual information extraction, and a synthesis analysis agent for data generation. Furthermore, we developed a comprehensive benchmark with expert-annotated chemical literature to evaluate both extraction efficiency and precision. Experimental results demonstrate reaction identification rates comparable to human chemists while significantly reducing processing time, with high accuracy, recall, and F1 scores. Our open-sourced benchmark facilitates future research in chemical literature data mining.", "source": "arxiv", "arxiv_id": "2402.12993v2", "pdf_url": "https://arxiv.org/pdf/2402.12993v2", "categories": ["cs.IR", "cs.AI", "cs.LG", "q-bio.QM"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-02-20T13:21:46Z", "updated": "2025-06-30T08:19:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation", "authors": ["Xinbei Ma", "Zhuosheng Zhang", "Hai Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2402.11941v3", "abstract": "Multimodal large language models (MLLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose a Comprehensive Cognitive LLM Agent, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our technical design, our agent achieves new state-of-the-art performance on AITW and META-GUI benchmarks, showing promising abilities in realistic scenarios. Code is available at https://github.com/xbmxb/CoCo-Agent.", "source": "arxiv", "arxiv_id": "2402.11941v3", "pdf_url": "https://arxiv.org/pdf/2402.11941v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-19T08:29:03Z", "updated": "2024-06-02T13:25:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CodeNav: Beyond tool-use to using real-world codebases with LLM agents", "authors": ["Tanmay Gupta", "Luca Weihs", "Aniruddha Kembhavi"], "year": 2024, "url": "http://arxiv.org/abs/2406.12276v1", "abstract": "We present CodeNav, an LLM agent that navigates and leverages previously unseen code repositories to solve user queries. In contrast to tool-use LLM agents that require ``registration'' of all relevant tools via manual descriptions within the LLM context, CodeNav automatically indexes and searches over code blocks in the target codebase, finds relevant code snippets, imports them, and uses them to iteratively generate a solution with execution feedback. To highlight the core-capabilities of CodeNav, we first showcase three case studies where we use CodeNav for solving complex user queries using three diverse codebases. Next, on three benchmarks, we quantitatively compare the effectiveness of code-use (which only has access to the target codebase) to tool-use (which has privileged access to all tool names and descriptions). Finally, we study the effect of varying kinds of tool and library descriptions on code-use performance, as well as investigate the advantage of the agent seeing source code as opposed to natural descriptions of code. All code will be made open source under a permissive license.", "source": "arxiv", "arxiv_id": "2406.12276v1", "pdf_url": "https://arxiv.org/pdf/2406.12276v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-18T05:10:38Z", "updated": "2024-06-18T05:10:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases", "authors": ["Xiangyan Liu", "Bo Lan", "Zhiyuan Hu", "Yang Liu", "Zhicheng Zhang", "Fei Wang", "Michael Shieh", "Wenmeng Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2408.03910v2", "abstract": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce CodexGraph, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, CodexGraph enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation. We assess CodexGraph using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding applications. With a unified graph database schema, CodexGraph demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering. Our application demo: https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.", "source": "arxiv", "arxiv_id": "2408.03910v2", "pdf_url": "https://arxiv.org/pdf/2408.03910v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-08-07T17:13:59Z", "updated": "2024-08-11T16:23:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Concept-Guided LLM Agents for Human-AI Safety Codesign", "authors": ["Florian Geissler", "Karsten Roscher", "Mario Trapp"], "year": 2024, "url": "http://arxiv.org/abs/2404.15317v1", "abstract": "Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.", "source": "arxiv", "arxiv_id": "2404.15317v1", "pdf_url": "https://arxiv.org/pdf/2404.15317v1", "categories": ["cs.SE", "cs.HC", "cs.LG"], "primary_category": "cs.SE", "doi": "", "venue": "Proceedings of the AAAI-make Spring Symposium, 2024", "published": "2024-04-03T11:37:01Z", "updated": "2024-04-03T11:37:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ControlAgent: Automating Control System Design via Novel Integration of LLM Agents and Domain Expertise", "authors": ["Xingang Guo", "Darioush Keivan", "Usman Syed", "Lianhui Qin", "Huan Zhang", "Geir Dullerud", "Peter Seiler", "Bin Hu"], "year": 2024, "url": "http://arxiv.org/abs/2410.19811v1", "abstract": "Control system design is a crucial aspect of modern engineering with far-reaching applications across diverse sectors including aerospace, automotive systems, power grids, and robotics. Despite advances made by Large Language Models (LLMs) in various domains, their application in control system design remains limited due to the complexity and specificity of control theory. To bridge this gap, we introduce ControlAgent, a new paradigm that automates control system design via novel integration of LLM agents and control-oriented domain expertise. ControlAgent encodes expert control knowledge and emulates human iterative design processes by gradually tuning controller parameters to meet user-specified requirements for stability, performance, and robustness. ControlAgent integrates multiple collaborative LLM agents, including a central agent responsible for task distribution and task-specific agents dedicated to detailed controller design for various types of systems and requirements. ControlAgent also employs a Python computation agent that performs complex calculations and controller evaluations based on standard design information provided by task-specified LLM agents. Combined with a history and feedback module, the task-specific LLM agents iteratively refine controller parameters based on real-time feedback from prior designs. Overall, ControlAgent mimics the design processes used by (human) practicing engineers, but removes all the human efforts and can be run in a fully automated way to give end-to-end solutions for control system design with user-specified requirements. To validate ControlAgent's effectiveness, we develop ControlEval, an evaluation dataset that comprises 500 control tasks with various specific design goals. The effectiveness of ControlAgent is demonstrated via extensive comparative evaluations between LLM-based and traditional human-involved toolbox-based baselines.", "source": "arxiv", "arxiv_id": "2410.19811v1", "pdf_url": "https://arxiv.org/pdf/2410.19811v1", "categories": ["eess.SY", "cs.AI", "cs.CL", "cs.LG", "math.OC"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-10-17T17:42:48Z", "updated": "2024-10-17T17:42:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Controlling Large Language Model Agents with Entropic Activation Steering", "authors": ["Nate Rahn", "Pierluca D'Oro", "Marc G. Bellemare"], "year": 2024, "url": "http://arxiv.org/abs/2406.00244v2", "abstract": "The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors.", "source": "arxiv", "arxiv_id": "2406.00244v2", "pdf_url": "https://arxiv.org/pdf/2406.00244v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-01T00:25:00Z", "updated": "2024-10-10T20:47:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Conversate: Supporting Reflective Learning in Interview Practice Through Interactive Simulation and Dialogic Feedback", "authors": ["Taufiq Daryanto", "Xiaohan Ding", "Lance T. Wilhelm", "Sophia Stil", "Kirk McInnis Knutsen", "Eugenia H. Rho"], "year": 2024, "url": "http://arxiv.org/abs/2410.05570v2", "abstract": "Job interviews play a critical role in shaping one's career, yet practicing interview skills can be challenging, especially without access to human coaches or peers for feedback. Recent advancements in large language models (LLMs) present an opportunity to enhance the interview practice experience. Yet, little research has explored the effectiveness and user perceptions of such systems or the benefits and challenges of using LLMs for interview practice. Furthermore, while prior work and recent commercial tools have demonstrated the potential of AI to assist with interview practice, they often deliver one-way feedback, where users only receive information about their performance. By contrast, dialogic feedback, a concept developed in learning sciences, is a two-way interaction feedback process that allows users to further engage with and learn from the provided feedback through interactive dialogue. This paper introduces Conversate, a web-based application that supports reflective learning in job interview practice by leveraging large language models (LLMs) for interactive interview simulations and dialogic feedback. To start the interview session, the user provides the title of a job position (e.g., entry-level software engineer) in the system. Then, our system will initialize the LLM agent to start the interview simulation by asking the user an opening interview question and following up with questions carefully adapted to subsequent user responses. After the interview session, our back-end LLM framework will then analyze the user's responses and highlight areas for improvement. Users can then annotate the transcript by selecting specific sections and writing self-reflections. Finally, the user can interact with the system for dialogic feedback, conversing with the LLM agent to learn from and iteratively refine their answers based on the agent's guidance.", "source": "arxiv", "arxiv_id": "2410.05570v2", "pdf_url": "https://arxiv.org/pdf/2410.05570v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3701188", "venue": "", "published": "2024-10-08T00:12:11Z", "updated": "2024-11-01T21:03:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents", "authors": ["Giorgio Piatti", "Zhijing Jin", "Max Kleiman-Weiner", "Bernhard SchÃ¶lkopf", "Mrinmaya Sachan", "Rada Mihalcea"], "year": 2024, "url": "http://arxiv.org/abs/2404.16698v4", "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.", "source": "arxiv", "arxiv_id": "2404.16698v4", "pdf_url": "https://arxiv.org/pdf/2404.16698v4", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-25T15:59:16Z", "updated": "2024-12-08T11:21:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Cultural Evolution of Cooperation among LLM Agents", "authors": ["Aron Vallinder", "Edward Hughes"], "year": 2024, "url": "http://arxiv.org/abs/2412.10270v1", "abstract": "Large language models (LLMs) provide a compelling foundation for building generally-capable AI agents. These agents may soon be deployed at scale in the real world, representing the interests of individual humans (e.g., AI assistants) or groups of humans (e.g., AI-accelerated corporations). At present, relatively little is known about the dynamics of multiple LLM agents interacting over many generations of iterative deployment. In this paper, we examine whether a \"society\" of LLM agents can learn mutually beneficial social norms in the face of incentives to defect, a distinctive feature of human sociality that is arguably crucial to the success of civilization. In particular, we study the evolution of indirect reciprocity across generations of LLM agents playing a classic iterated Donor Game in which agents can observe the recent behavior of their peers. We find that the evolution of cooperation differs markedly across base models, with societies of Claude 3.5 Sonnet agents achieving significantly higher average scores than Gemini 1.5 Flash, which, in turn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an additional mechanism for costly punishment to achieve yet higher scores, while Gemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also observe variation in emergent behavior across random seeds, suggesting an understudied sensitive dependence on initial conditions. We suggest that our evaluation regime could inspire an inexpensive and informative new class of LLM benchmarks, focussed on the implications of LLM agent deployment for the cooperative infrastructure of society.", "source": "arxiv", "arxiv_id": "2412.10270v1", "pdf_url": "https://arxiv.org/pdf/2412.10270v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-12-13T16:45:49Z", "updated": "2024-12-13T16:45:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning", "authors": ["Siyuan Guo", "Cheng Deng", "Ying Wen", "Hechang Chen", "Yi Chang", "Jun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.17453v5", "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\\% success rate in the development stage, while attaining 36\\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.", "source": "arxiv", "arxiv_id": "2402.17453v5", "pdf_url": "https://arxiv.org/pdf/2402.17453v5", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-27T12:26:07Z", "updated": "2024-05-28T06:50:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Data Interpreter: An LLM Agent For Data Science", "authors": ["Sirui Hong", "Yizhang Lin", "Bang Liu", "Bangbang Liu", "Binhao Wu", "Ceyao Zhang", "Chenxing Wei", "Danyang Li", "Jiaqi Chen", "Jiayi Zhang", "Jinlin Wang", "Li Zhang", "Lingyao Zhang", "Min Yang", "Mingchen Zhuge", "Taicheng Guo", "Tuo Zhou", "Wei Tao", "Xiangru Tang", "Xiangtao Lu", "Xiawu Zheng", "Xinbing Liang", "Yaying Fei", "Yuheng Cheng", "Zhibin Gou", "Zongze Xu", "Chenglin Wu"], "year": 2024, "url": "http://arxiv.org/abs/2402.18679v4", "abstract": "Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.", "source": "arxiv", "arxiv_id": "2402.18679v4", "pdf_url": "https://arxiv.org/pdf/2402.18679v4", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-28T19:49:55Z", "updated": "2024-10-15T15:52:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DebUnc: Improving Large Language Model Agent Communication With Uncertainty Metrics", "authors": ["Luke Yoffe", "Alfonso Amayuelas", "William Yang Wang"], "year": 2024, "url": "http://arxiv.org/abs/2407.06426v2", "abstract": "Multi-agent debates have been introduced to improve the accuracy of Large Language Models (LLMs) by having multiple agents discuss solutions to a problem over several rounds of debate. However, models often generate incorrect yet confident-sounding responses, which can mislead others. This issue arises partly because agents do not consider how confident their peers are. To address this, we propose DebUnc, a debate framework that uses uncertainty metrics to assess agent confidence. Confidence is then conveyed through a modified attention mechanism that adjusts token weights, or through textual prompts. Evaluations across benchmarks show that attention-based methods are particularly effective and that performance continues to improve as uncertainty estimation becomes more reliable. The code is available at https://github.com/lukeyoffe/debunc.", "source": "arxiv", "arxiv_id": "2407.06426v2", "pdf_url": "https://arxiv.org/pdf/2407.06426v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-08T22:15:01Z", "updated": "2025-02-22T02:15:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games", "authors": ["Yikuan Yan", "Yaolun Zhang", "Keman Huang"], "year": 2024, "url": "http://arxiv.org/abs/2403.17674v1", "abstract": "Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, achieving the complementary performance in the cybersecurity games.", "source": "arxiv", "arxiv_id": "2403.17674v1", "pdf_url": "https://arxiv.org/pdf/2403.17674v1", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-03-26T13:02:46Z", "updated": "2024-03-26T13:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis", "authors": ["Frank Xing"], "year": 2024, "url": "http://arxiv.org/abs/2401.05799v1", "abstract": "Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.", "source": "arxiv", "arxiv_id": "2401.05799v1", "pdf_url": "https://arxiv.org/pdf/2401.05799v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "q-fin.GN"], "primary_category": "cs.CL", "doi": "10.1145/3688399", "venue": "", "published": "2024-01-11T10:06:42Z", "updated": "2024-01-11T10:06:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Devil's Advocate: Anticipatory Reflection for LLM Agents", "authors": ["Haoyu Wang", "Tao Li", "Zhiwei Deng", "Dan Roth", "Yang Li"], "year": 2024, "url": "http://arxiv.org/abs/2405.16334v4", "abstract": "In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. %; and when necessary, to explore ``the road not taken.'' We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology -- a zero-shot approach -- within WebArena for practical tasks in web environments, our agent demonstrates superior performance with a success rate of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions by 45% needed to achieve a task.", "source": "arxiv", "arxiv_id": "2405.16334v4", "pdf_url": "https://arxiv.org/pdf/2405.16334v4", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-05-25T19:20:15Z", "updated": "2024-06-20T19:41:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation", "authors": ["Zhenyu Wang", "Enze Xie", "Aoxue Li", "Zhongdao Wang", "Xihui Liu", "Zhenguo Li"], "year": 2024, "url": "http://arxiv.org/abs/2401.15688v2", "abstract": "Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships. In this paper, we propose CompAgent, a training-free approach for compositional text-to-image generation, with a large language model (LLM) agent as its core. The fundamental idea underlying CompAgent is premised on a divide-and-conquer methodology. Given a complex text prompt containing multiple concepts including objects, attributes, and relationships, the LLM agent initially decomposes it, which entails the extraction of individual objects, their associated attributes, and the prediction of a coherent scene layout. These individual objects can then be independently conquered. Subsequently, the agent performs reasoning by analyzing the text, plans and employs the tools to compose these isolated objects. The verification and human feedback mechanism is finally incorporated into our agent to further correct the potential attribute errors and refine the generated images. Guided by the LLM agent, we propose a tuning-free multi-concept customization model and a layout-to-image generation model as the tools for concept composition, and a local image editing method as the tool to interact with the agent for verification. The scene layout controls the image generation process among these tools to prevent confusion among multiple objects. Extensive experiments demonstrate the superiority of our approach for compositional text-to-image generation: CompAgent achieves more than 10\\% improvement on T2I-CompBench, a comprehensive benchmark for open-world compositional T2I generation. The extension to various related tasks also illustrates the flexibility of our CompAgent for potential applications.", "source": "arxiv", "arxiv_id": "2401.15688v2", "pdf_url": "https://arxiv.org/pdf/2401.15688v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-01-28T16:18:39Z", "updated": "2024-01-30T13:05:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games", "authors": ["Chanwoo Park", "Xiangyu Liu", "Asuman Ozdaglar", "Kaiqing Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.16843v5", "abstract": "Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \\emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \\emph{unsupervised} training loss of \\emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.", "source": "arxiv", "arxiv_id": "2403.16843v5", "pdf_url": "https://arxiv.org/pdf/2403.16843v5", "categories": ["cs.LG", "cs.AI", "cs.GT"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-03-25T15:04:11Z", "updated": "2025-10-15T12:44:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DropMicroFluidAgents (DMFAs): Autonomous Droplet Microfluidic Research Framework Through Large Language Model Agents", "authors": ["Dinh-Nguyen Nguyen", "Raymond Kai-Yu Tong", "Ngoc-Duy Dinh"], "year": 2024, "url": "http://arxiv.org/abs/2501.14772v1", "abstract": "Applying Large language models (LLMs) within specific domains requires substantial adaptation to account for the unique terminologies, nuances, and context-specific challenges inherent to those areas. Here, we introduce DropMicroFluidAgents (DMFAs), an advanced language-driven framework leveraging state-of-the-art pre-trained LLMs. DMFAs employs LLM agents to perform two key functions: (1) delivering focused guidance, answers, and suggestions specific to droplet microfluidics and (2) generating machine learning models to optimise and automate the design of droplet microfluidic devices, including the creation of code-based computer-aided design (CAD) scripts to enable rapid and precise design execution. Experimental evaluations demonstrated that the integration of DMFAs with the LLAMA3.1 model yielded the highest accuracy of 76.15%, underscoring the significant performance enhancement provided by agent integration. This effect was particularly pronounced when DMFAs were paired with the GEMMA2 model, resulting in a 34.47% improvement in accuracy compared to the standalone GEMMA2 configuration. This study demonstrates the effective use of LLM agents in droplet microfluidics research as powerful tools for automating workflows, synthesising knowledge, optimising designs, and interacting with external systems. These capabilities enable their application across education and industrial support, driving greater efficiency in scientific discovery and innovation.", "source": "arxiv", "arxiv_id": "2501.14772v1", "pdf_url": "https://arxiv.org/pdf/2501.14772v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-12-30T11:58:52Z", "updated": "2024-12-30T11:58:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "DynaSaur: Large Language Agents Beyond Predefined Actions", "authors": ["Dang Nguyen", "Viet Dac Lai", "Seunghyun Yoon", "Ryan A. Rossi", "Handong Zhao", "Ruiyi Zhang", "Puneet Mathur", "Nedim Lipka", "Yu Wang", "Trung Bui", "Franck Dernoncourt", "Tianyi Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2411.01747v3", "abstract": "Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly scoped environments, it presents two major challenges for real-world, open-ended scenarios: (1) it significantly restricts the planning and acting capabilities of LLM agents, and (2) it requires substantial human effort to enumerate and implement all possible actions, which is impractical in complex environments with a vast number of potential actions. To address these limitations, we propose an LLM agent framework that can dynamically create and compose actions as needed. In this framework, the agent interacts with its environment by generating and executing programs written in a general-purpose programming language. Moreover, generated actions are accumulated over time for future reuse. Our extensive experiments across multiple benchmarks show that this framework significantly improves flexibility and outperforms prior methods that rely on a fixed action set. Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases. Our code can be found in https://github.com/adobe-research/dynasaur.", "source": "arxiv", "arxiv_id": "2411.01747v3", "pdf_url": "https://arxiv.org/pdf/2411.01747v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-04T02:08:59Z", "updated": "2025-09-04T16:22:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "authors": ["Wenqi Shi", "Ran Xu", "Yuchen Zhuang", "Yue Yu", "Jieyu Zhang", "Hang Wu", "Yuanda Zhu", "Joyce Ho", "Carl Yang", "May D. Wang"], "year": 2024, "url": "http://arxiv.org/abs/2401.07128v3", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.", "source": "arxiv", "arxiv_id": "2401.07128v3", "pdf_url": "https://arxiv.org/pdf/2401.07128v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-13T18:09:05Z", "updated": "2024-10-04T05:56:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents", "authors": ["Yuxi Wei", "Zi Wang", "Yifan Lu", "Chenxin Xu", "Changxing Liu", "Hao Zhao", "Siheng Chen", "Yanfeng Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.05746v3", "abstract": "Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data. However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration. To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets. To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework. To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method. Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering. Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos.", "source": "arxiv", "arxiv_id": "2402.05746v3", "pdf_url": "https://arxiv.org/pdf/2402.05746v3", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-02-08T15:26:28Z", "updated": "2024-06-26T10:44:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration", "authors": ["Shiwen Ni", "Min Yang"], "year": 2024, "url": "http://arxiv.org/abs/2412.03847v1", "abstract": "Intelligent dialogue systems are increasingly used in modern education and psychological counseling fields, but most existing systems are limited to a single domain, cannot deal with both educational and psychological issues, and often lack accuracy and professionalism when dealing with complex issues. To address these problems, this paper proposes an intelligent dialog system that combines educational and psychological counseling functions. The system consists of multiple AI agent, including security detection agent, intent identification agent, educational LLM agent, and psychological LLM agent, which work in concert to ensure the provision of accurate educational knowledge Q\\&A and psychological support services. Specifically, the system recognizes user-input intentions through an intention classification model and invokes a retrieval-enhanced educational grand model and a psychological grand model fine-tuned with psychological data in order to provide professional educational advice and psychological support.", "source": "arxiv", "arxiv_id": "2412.03847v1", "pdf_url": "https://arxiv.org/pdf/2412.03847v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "ICSR 2024", "published": "2024-12-05T03:27:02Z", "updated": "2024-12-05T03:27:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation", "authors": ["Mohammadmehdi Ataei", "Hyunmin Cheong", "Daniele Grandi", "Ye Wang", "Nigel Morris", "Alexander Tessier"], "year": 2024, "url": "http://arxiv.org/abs/2404.16045v1", "abstract": "Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs, and classify them as latent or not. Our work highlights the potential of using LLM agents to accelerate early-stage product development, reduce costs, and increase innovation.", "source": "arxiv", "arxiv_id": "2404.16045v1", "pdf_url": "https://arxiv.org/pdf/2404.16045v1", "categories": ["cs.HC", "cs.AI", "cs.MA"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-04T17:36:29Z", "updated": "2024-04-04T17:36:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Embodied LLM Agents Learn to Cooperate in Organized Teams", "authors": ["Xudong Guo", "Kaixuan Huang", "Jiale Liu", "Wenhui Fan", "Natalia VÃ©lez", "Qingyun Wu", "Huazheng Wang", "Thomas L. Griffiths", "Mengdi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2403.12482v2", "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.", "source": "arxiv", "arxiv_id": "2403.12482v2", "pdf_url": "https://arxiv.org/pdf/2403.12482v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-19T06:39:47Z", "updated": "2024-05-23T06:29:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Empirical Asset Pricing with Large Language Model Agents", "authors": ["Junyan Cheng", "Peter Chin"], "year": 2024, "url": "http://arxiv.org/abs/2409.17266v2", "abstract": "In this study, we introduce a novel asset pricing model leveraging the Large Language Model (LLM) agents, which integrates qualitative discretionary investment evaluations from LLM agents with quantitative financial economic factors manually curated, aiming to explain the excess asset returns. The experimental results demonstrate that our methodology surpasses traditional machine learning-based baselines in both portfolio optimization and asset pricing errors. Notably, the Sharpe ratio for portfolio optimization and the mean magnitude of $|Î±|$ for anomaly portfolios experienced substantial enhancements of 10.6\\% and 10.0\\% respectively. Moreover, we performed comprehensive ablation studies on our model and conducted a thorough analysis of the method to extract further insights into the proposed approach. Our results show effective evidence of the feasibility of applying LLMs in empirical asset pricing.", "source": "arxiv", "arxiv_id": "2409.17266v2", "pdf_url": "https://arxiv.org/pdf/2409.17266v2", "categories": ["cs.AI", "cs.CE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-25T18:27:35Z", "updated": "2025-03-28T01:02:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Empowering Large Language Model Agents through Action Learning", "authors": ["Haiteng Zhao", "Chang Ma", "Guoyin Wang", "Jing Su", "Lingpeng Kong", "Jingjing Xu", "Zhi-Hong Deng", "Hongxia Yang"], "year": 2024, "url": "http://arxiv.org/abs/2402.15809v2", "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.", "source": "arxiv", "arxiv_id": "2402.15809v2", "pdf_url": "https://arxiv.org/pdf/2402.15809v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-24T13:13:04Z", "updated": "2024-08-08T07:05:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Enabling Generative Design Tools with LLM Agents for Mechanical Computation Devices: A Case Study", "authors": ["Qiuyu Lu", "Jiawei Fang", "Zhihao Yao", "Yue Yang", "Shiqing Lyu", "Haipeng Mi", "Lining Yao"], "year": 2024, "url": "http://arxiv.org/abs/2405.17837v3", "abstract": "In the field of Human-Computer Interaction (HCI), interactive devices with embedded mechanical computation are gaining attention. The rise of these cutting-edge devices has created a need for specialized design tools that democratize the prototyping process. While current tools streamline prototyping through parametric design and simulation, they often come with a steep learning curve and may not fully support creative ideation. In this study, we use fluidic computation interfaces as a case study to explore how design tools for such devices can be augmented by Large Language Model agents (LLMs). Integrated with LLMs, the Generative Design Tool (GDT) better understands the capabilities and limitations of new technologies, proposes diverse and practical applications, and suggests designs that are technically and contextually appropriate. Additionally, it generates design parameters for visualizing results and producing fabrication-ready support files. This paper details the GDT's framework, implementation, and performance while addressing its potential and challenges.", "source": "arxiv", "arxiv_id": "2405.17837v3", "pdf_url": "https://arxiv.org/pdf/2405.17837v3", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-05-28T05:21:09Z", "updated": "2024-10-29T22:28:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework", "authors": ["Honghao Shi", "Longkai Cheng", "Wenli Wu", "Yuhang Wang", "Xuan Liu", "Shaokai Nie", "Weixv Wang", "Xuebin Min", "Chunlei Men", "Yonghua Lin"], "year": 2024, "url": "http://arxiv.org/abs/2411.05349v1", "abstract": "Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM capabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated the superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting and rectifying performance issues more efficiently and accurately than traditional methods.", "source": "arxiv", "arxiv_id": "2411.05349v1", "pdf_url": "https://arxiv.org/pdf/2411.05349v1", "categories": ["cs.AI", "cs.DC"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-08T06:12:56Z", "updated": "2024-11-08T06:12:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models", "authors": ["Yuanzhao Zhai", "Tingkai Yang", "Kele Xu", "Feng Dawei", "Cheng Yang", "Bo Ding", "Huaimin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2409.09345v1", "abstract": "Agents significantly enhance the capabilities of standalone Large Language Models (LLMs) by perceiving environments, making decisions, and executing actions. However, LLM agents still face challenges in tasks that require multiple decision-making steps. Estimating the value of actions in specific tasks is difficult when intermediate actions are neither appropriately rewarded nor penalized. In this paper, we propose leveraging a task-relevant Q-value model to guide action selection. Specifically, we first collect decision-making trajectories annotated with step-level Q values via Monte Carlo Tree Search (MCTS) and construct preference data. We then use another LLM to fit these preferences through step-level Direct Policy Optimization (DPO), which serves as the Q-value model. During inference, at each decision-making step, LLM agents select the action with the highest Q value before interacting with the environment. We apply our method to various open-source and API-based LLM agents, demonstrating that Q-value models significantly improve their performance. Notably, the performance of the agent built with Phi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when enhanced with Q-value models, even surpassing GPT-4o-mini. Additionally, Q-value models offer several advantages, such as generalization to different LLM agents and seamless integration with existing prompting strategies.", "source": "arxiv", "arxiv_id": "2409.09345v1", "pdf_url": "https://arxiv.org/pdf/2409.09345v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-14T07:32:49Z", "updated": "2024-09-14T07:32:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay", "authors": ["Yuyang Chen", "Kaiyan Zhao", "Yiming Wang", "Ming Yang", "Jian Zhang", "Xiaoguang Niu"], "year": 2024, "url": "http://arxiv.org/abs/2410.12236v2", "abstract": "Nowadays transformer-based Large Language Models (LLM) for code generation tasks usually apply sampling and filtering pipelines. Due to the sparse reward problem in code generation tasks caused by one-token incorrectness, transformer-based models will sample redundant programs till they find a correct one, leading to low efficiency. To overcome the challenge, we incorporate Experience Replay (ER) in the fine-tuning phase, where codes and programs produced are stored and will be replayed to give the LLM agent a chance to learn from past experiences. Based on the spirit of ER, we introduce a novel approach called BTP pipeline which consists of three phases: beam search sampling, testing phase, and prioritized experience replay phase. The approach makes use of failed programs collected by code models and replays programs with high Possibility and Pass-rate Prioritized value (P2Value) from the replay buffer to improve efficiency. P2Value comprehensively considers the possibility of transformers' output and pass rate and can make use of the redundant resources caused by the problem that most programs collected by LLMs fail to pass any tests. We empirically apply our approach in several LLMs, demonstrating that it enhances their performance in code generation tasks and surpasses existing baselines.", "source": "arxiv", "arxiv_id": "2410.12236v2", "pdf_url": "https://arxiv.org/pdf/2410.12236v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-16T04:54:42Z", "updated": "2025-01-11T07:08:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents", "authors": ["Cheng Qian", "Peixuan Han", "Qinyu Luo", "Bingxiang He", "Xiusi Chen", "Yuji Zhang", "Hongyi Du", "Jiarui Yao", "Xiaocheng Yang", "Denghui Zhang", "Yunzhu Li", "Heng Ji"], "year": 2024, "url": "http://arxiv.org/abs/2412.13549v2", "abstract": "Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench, a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.", "source": "arxiv", "arxiv_id": "2412.13549v2", "pdf_url": "https://arxiv.org/pdf/2412.13549v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-18T06:50:39Z", "updated": "2025-05-24T04:56:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information", "authors": ["Yauwai Yim", "Chunkit Chan", "Tianyu Shi", "Zheye Deng", "Wei Fan", "Tianshi Zheng", "Yangqiu Song"], "year": 2024, "url": "http://arxiv.org/abs/2408.02559v1", "abstract": "Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.", "source": "arxiv", "arxiv_id": "2408.02559v1", "pdf_url": "https://arxiv.org/pdf/2408.02559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-05T15:36:46Z", "updated": "2024-08-05T15:36:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evaluation-Driven Development and Operations of LLM Agents: A Process Model and Reference Architecture", "authors": ["Boming Xia", "Qinghua Lu", "Liming Zhu", "Zhenchang Xing", "Dehai Zhao", "Hao Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2411.13768v3", "abstract": "Large Language Models (LLMs) have enabled the emergence of LLM agents, systems capable of pursuing under-specified goals and adapting after deployment. Evaluating such agents is challenging because their behavior is open ended, probabilistic, and shaped by system-level interactions over time. Traditional evaluation methods, built around fixed benchmarks and static test suites, fail to capture emergent behaviors or support continuous adaptation across the lifecycle. To ground a more systematic approach, we conduct a multivocal literature review (MLR) synthesizing academic and industrial evaluation practices. The findings directly inform two empirically derived artifacts: a process model and a reference architecture that embed evaluation as a continuous, governing function rather than a terminal checkpoint. Together they constitute the evaluation-driven development and operations (EDDOps) approach, which unifies offline (development-time) and online (runtime) evaluation within a closed feedback loop. By making evaluation evidence drive both runtime adaptation and governed redevelopment, EDDOps supports safer, more traceable evolution of LLM agents aligned with changing objectives, user needs, and governance constraints.", "source": "arxiv", "arxiv_id": "2411.13768v3", "pdf_url": "https://arxiv.org/pdf/2411.13768v3", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-11-21T00:34:30Z", "updated": "2025-11-17T06:10:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evolution of Social Norms in LLM Agents using Natural Language", "authors": ["Ilya Horiguchi", "Takahide Yoshida", "Takashi Ikegami"], "year": 2024, "url": "http://arxiv.org/abs/2409.00993v1", "abstract": "Recent advancements in Large Language Models (LLMs) have spurred a surge of interest in leveraging these models for game-theoretical simulations, where LLMs act as individual agents engaging in social interactions. This study explores the potential for LLM agents to spontaneously generate and adhere to normative strategies through natural language discourse, building upon the foundational work of Axelrod's metanorm games. Our experiments demonstrate that through dialogue, LLM agents can form complex social norms, such as metanorms-norms enforcing the punishment of those who do not punish cheating-purely through natural language interaction. The results affirm the effectiveness of using LLM agents for simulating social interactions and understanding the emergence and evolution of complex strategies and norms through natural language. Future work may extend these findings by incorporating a wider range of scenarios and agent characteristics, aiming to uncover more nuanced mechanisms behind social norm formation.", "source": "arxiv", "arxiv_id": "2409.00993v1", "pdf_url": "https://arxiv.org/pdf/2409.00993v1", "categories": ["cs.MA"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-09-02T07:15:43Z", "updated": "2024-09-02T07:15:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Executable Code Actions Elicit Better LLM Agents", "authors": ["Xingyao Wang", "Yangyi Chen", "Lifan Yuan", "Yizhe Zhang", "Yunzhu Li", "Hao Peng", "Heng Ji"], "year": 2024, "url": "http://arxiv.org/abs/2402.01030v4", "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.", "source": "arxiv", "arxiv_id": "2402.01030v4", "pdf_url": "https://arxiv.org/pdf/2402.01030v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-01T21:38:58Z", "updated": "2024-06-07T01:53:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration", "authors": ["Yanchu Guan", "Dong Wang", "Yan Wang", "Haiqing Wang", "Renen Sun", "Chenyi Zhuang", "Jinjie Gu", "Zhixuan Chu"], "year": 2024, "url": "http://arxiv.org/abs/2410.22916v1", "abstract": "Autonomous mobile app interaction has become increasingly important with growing complexity of mobile applications. Developing intelligent agents that can effectively navigate and interact with mobile apps remains a significant challenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent (EBC-LLMAgent), a novel approach that combines large language models (LLMs) with behavior cloning by learning demonstrations to create intelligent and explainable agents for autonomous mobile app interaction. EBC-LLMAgent consists of three core modules: Demonstration Encoding, Code Generation, and UI Mapping, which work synergistically to capture user demonstrations, generate executable codes, and establish accurate correspondence between code and UI elements. We introduce the Behavior Cloning Chain Fusion technique to enhance the generalization capabilities of the agent. Extensive experiments on five popular mobile applications from diverse domains demonstrate the superior performance of EBC-LLMAgent, achieving high success rates in task completion, efficient generalization to unseen scenarios, and the generation of meaningful explanations.", "source": "arxiv", "arxiv_id": "2410.22916v1", "pdf_url": "https://arxiv.org/pdf/2410.22916v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-30T11:14:33Z", "updated": "2024-10-30T11:14:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View", "authors": ["Xuan Liu", "Jie Zhang", "Haoyang Shang", "Song Guo", "Chengxu Yang", "Quanyan Zhu"], "year": 2024, "url": "http://arxiv.org/abs/2405.14744v5", "abstract": "Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.", "source": "arxiv", "arxiv_id": "2405.14744v5", "pdf_url": "https://arxiv.org/pdf/2405.14744v5", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-05-23T16:13:33Z", "updated": "2025-03-22T14:15:47Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exploring and Controlling Diversity in LLM-Agent Conversation", "authors": ["KuanChao Chu", "Yi-Pei Chen", "Hideki Nakayama"], "year": 2024, "url": "http://arxiv.org/abs/2412.21102v3", "abstract": "Controlling diversity in LLM-agent simulations is essential for balancing stability in structured tasks with variability in open-ended interactions. However, we observe that dialogue diversity tends to degrade over long-term simulations. To explore the role of prompt design in this phenomenon, we modularized the utterance generation prompt and found that reducing contextual information leads to more diverse outputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a novel method that allows users to control diversity via a single parameter, lambda. APP dynamically prunes prompt segments based on attention scores and is compatible with existing diversity control methods. We demonstrate that APP effectively modulates diversity through extensive experiments and propose a method to balance the control trade-offs. Our analysis reveals that all prompt components impose constraints on diversity, with the Memory being the most influential. Additionally, high-attention contents consistently suppress output diversity.", "source": "arxiv", "arxiv_id": "2412.21102v3", "pdf_url": "https://arxiv.org/pdf/2412.21102v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-30T17:25:58Z", "updated": "2025-10-01T05:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas", "authors": ["Yu Lei", "Hao Liu", "Chengxing Xie", "Songjia Liu", "Zhiyu Yin", "Canyu Chen", "Guohao Li", "Philip Torr", "Zhen Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.10398v2", "abstract": "AI alignment is a pivotal issue concerning AI control and safety. It should consider not only value-neutral human preferences but also moral and ethical considerations. In this study, we introduced FairMindSim, which simulates the moral dilemma through a series of unfair scenarios. We used LLM agents to simulate human behavior, ensuring alignment across various stages. To explore the various socioeconomic motivations, which we refer to as beliefs, that drive both humans and LLM agents as bystanders to intervene in unjust situations involving others, and how these beliefs interact to influence individual behavior, we incorporated knowledge from relevant sociological fields and proposed the Belief-Reward Alignment Behavior Evolution Model (BREM) based on the recursive reward model (RRM). Our findings indicate that, behaviorally, GPT-4o exhibits a stronger sense of social justice, while humans display a richer range of emotions. Additionally, we discussed the potential impact of emotions on behavior. This study provides a theoretical foundation for applications in aligning LLMs with altruistic values.", "source": "arxiv", "arxiv_id": "2410.10398v2", "pdf_url": "https://arxiv.org/pdf/2410.10398v2", "categories": ["cs.CE", "cs.AI"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-10-14T11:39:05Z", "updated": "2024-10-17T15:02:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Federated In-Context LLM Agent Learning", "authors": ["Panlong Wu", "Kangshuo Li", "Junbao Nan", "Fangxin Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.08054v1", "abstract": "Large Language Models (LLMs) have revolutionized intelligent services by enabling logical reasoning, tool use, and interaction with external systems as agents. The advancement of LLMs is frequently hindered by the scarcity of high-quality data, much of which is inherently sensitive. Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding private data. However, FL frameworks face significant bandwidth and computational demands, along with challenges from heterogeneous data distributions. The emerging in-context learning capability of LLMs offers a promising approach by aggregating natural language rather than bulky model parameters. Yet, this method risks privacy leakage, as it necessitates the collection and presentation of data samples from various clients during aggregation. In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for the first work unleashes the power of in-context learning to train diverse LLM agents through FL. In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods. Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools. We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of $\\mathbf{3.33\\times10^5}$ times.", "source": "arxiv", "arxiv_id": "2412.08054v1", "pdf_url": "https://arxiv.org/pdf/2412.08054v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-12-11T03:00:24Z", "updated": "2024-12-11T03:00:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Fixing Security Vulnerabilities with AI in OSS-Fuzz", "authors": ["Yuntong Zhang", "Jiawei Wang", "Dominic Berzin", "Martin Mirchev", "Dongge Liu", "Abhishek Arya", "Oliver Chang", "Abhik Roychoudhury"], "year": 2024, "url": "http://arxiv.org/abs/2411.03346v2", "abstract": "Critical open source software systems undergo significant validation in the form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased random search over the domain of program inputs, to find inputs which crash the software system. Such fuzzing is useful to enhance the security of software systems in general since even closed source software may use open source components. Hence testing open source software is of paramount importance. Currently OSS-Fuzz is the most significant and widely used infrastructure for continuous validation of open source systems. Unfortunately even though OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more software projects, the detected vulnerabilities may remain unpatched, as vulnerability fixing is often manual in practice. In this work, we rely on the recent progress in Large Language Model (LLM) agents for autonomous program improvement including bug fixing. We customise the well-known AutoCodeRover agent for fixing security vulnerabilities. This is because LLM agents like AutoCodeRover fix bugs from issue descriptions via code search. Instead for security patching, we rely on the test execution of the exploit input to extract code elements relevant to the fix. Our experience with OSS-Fuzz vulnerability data shows that LLM agent autonomy is useful for successful security patching, as opposed to approaches like Agentless where the control flow is fixed. More importantly our findings show that we cannot measure quality of patches by code similarity of the patch with reference codes (as in CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores still fail to pass given the given exploit input. Our findings indicate that security patch correctness needs to consider dynamic attributes like test executions as opposed to relying of standard text/code similarity metrics.", "source": "arxiv", "arxiv_id": "2411.03346v2", "pdf_url": "https://arxiv.org/pdf/2411.03346v2", "categories": ["cs.CR", "cs.SE"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-11-03T16:20:32Z", "updated": "2024-11-21T04:35:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists", "authors": ["Tenghao Huang", "Donghee Lee", "John Sweeney", "Jiatong Shi", "Emily Steliotes", "Matthew Lange", "Jonathan May", "Muhao Chen"], "year": 2024, "url": "http://arxiv.org/abs/2409.12832v3", "abstract": "Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address the challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. To facilitate research in this area, we introduce the FoodPuzzle, a challenging benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.", "source": "arxiv", "arxiv_id": "2409.12832v3", "pdf_url": "https://arxiv.org/pdf/2409.12832v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-19T15:07:35Z", "updated": "2024-10-07T01:26:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Barriers to Tactics: A Behavioral Science-Informed Agentic Workflow for Personalized Nutrition Coaching", "authors": ["Eric Yang", "Tomas Garcia", "Hannah Williams", "Bhawesh Kumar", "Martin RamÃ©", "Eileen Rivera", "Yiran Ma", "Jonathan Amar", "Caricia Catalani", "Yugang Jia"], "year": 2024, "url": "http://arxiv.org/abs/2410.14041v1", "abstract": "Effective management of cardiometabolic conditions requires sustained positive nutrition habits, often hindered by complex and individualized barriers. Direct human management is simply not scalable, while previous attempts aimed at automating nutrition coaching lack the personalization needed to address these diverse challenges. This paper introduces a novel LLM-powered agentic workflow designed to provide personalized nutrition coaching by directly targeting and mitigating patient-specific barriers. Grounded in behavioral science principles, the workflow leverages a comprehensive mapping of nutrition-related barriers to corresponding evidence-based strategies. A specialized LLM agent intentionally probes for and identifies the root cause of a patient's dietary struggles. Subsequently, a separate LLM agent delivers tailored tactics designed to overcome those specific barriers with patient context. We designed and validated our approach through a user study with individuals with cardiometabolic conditions, demonstrating the system's ability to accurately identify barriers and provide personalized guidance. Furthermore, we conducted a large-scale simulation study, grounding on real patient vignettes and expert-validated metrics, to evaluate the system's performance across a wide range of scenarios. Our findings demonstrate the potential of this LLM-powered agentic workflow to improve nutrition coaching by providing personalized, scalable, and behaviorally-informed interventions.", "source": "arxiv", "arxiv_id": "2410.14041v1", "pdf_url": "https://arxiv.org/pdf/2410.14041v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-17T21:35:07Z", "updated": "2024-10-17T21:35:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Language Models to Practical Self-Improving Computer Agents", "authors": ["Alex Sheng"], "year": 2024, "url": "http://arxiv.org/abs/2404.11964v1", "abstract": "We develop a simple and straightforward methodology to create AI computer agents that can carry out diverse computer tasks and self-improve by developing tools and augmentations to enable themselves to solve increasingly complex tasks. As large language models (LLMs) have been shown to benefit from non-parametric augmentations, a significant body of recent work has focused on developing software that augments LLMs with various capabilities. Rather than manually developing static software to augment LLMs through human engineering effort, we propose that an LLM agent can systematically generate software to augment itself. We show, through a few case studies, that a minimal querying loop with appropriate prompt engineering allows an LLM to generate and use various augmentations, freely extending its own capabilities to carry out real-world computer tasks. Starting with only terminal access, we prompt an LLM agent to augment itself with retrieval, internet search, web navigation, and text editor capabilities. The agent effectively uses these various tools to solve problems including automated software development and web-based tasks.", "source": "arxiv", "arxiv_id": "2404.11964v1", "pdf_url": "https://arxiv.org/pdf/2404.11964v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-18T07:50:10Z", "updated": "2024-04-18T07:50:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning", "authors": ["Zhirui Deng", "Zhicheng Dou", "Yutao Zhu", "Ji-Rong Wen", "Ruibin Xiong", "Mang Wang", "Weipeng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.03817v3", "abstract": "The outstanding capabilities of large language models (LLMs) render them a crucial component in various autonomous agent systems. While traditional methods depend on the inherent knowledge of LLMs without fine-tuning, more recent approaches have shifted toward the reinforcement learning strategy to further enhance agents' ability to solve complex interactive tasks with environments and tools. However, previous approaches are constrained by the sparse reward issue, where existing datasets solely provide a final scalar reward for each multi-step reasoning chain, potentially leading to ineffectiveness and inefficiency in policy learning. In this paper, we introduce StepAgent, which utilizes step-wise reward to optimize the agent's reinforcement learning process. Inheriting the spirit of novice-to-expert theory, we first compare the actions of the expert and the agent to automatically generate intermediate rewards for fine-grained optimization. Additionally, we propose implicit-reward and inverse reinforcement learning techniques to facilitate agent reflection and policy adjustment. Further theoretical analysis demonstrates that the action distribution of the agent can converge toward the expert action distribution over multiple training cycles. Experimental results across various datasets indicate that StepAgent outperforms existing baseline methods.", "source": "arxiv", "arxiv_id": "2411.03817v3", "pdf_url": "https://arxiv.org/pdf/2411.03817v3", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-06T10:35:11Z", "updated": "2024-12-09T09:20:11Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent", "authors": ["Samuel S. Sohn", "Danrui Li", "Sen Zhang", "Che-Jui Chang", "Mubbasir Kapadia"], "year": 2024, "url": "http://arxiv.org/abs/2406.10478v2", "abstract": "Digital storytelling, essential in entertainment, education, and marketing, faces challenges in production scalability and flexibility. The StoryAgent framework, introduced in this paper, utilizes Large Language Models and generative tools to automate and refine digital storytelling. Employing a top-down story drafting and bottom-up asset generation approach, StoryAgent tackles key issues such as manual intervention, interactive scene orchestration, and narrative consistency. This framework enables efficient production of interactive and consistent narratives across multiple modalities, democratizing content creation and enhancing engagement. Our results demonstrate the framework's capability to produce coherent digital stories without reference videos, marking a significant advancement in automated digital storytelling.", "source": "arxiv", "arxiv_id": "2406.10478v2", "pdf_url": "https://arxiv.org/pdf/2406.10478v2", "categories": ["cs.CL", "cs.AI", "cs.GR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-15T03:03:43Z", "updated": "2024-06-21T08:09:17Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of Language Model Agents", "authors": ["Seth Lazar"], "year": 2024, "url": "http://arxiv.org/abs/2404.06750v2", "abstract": "Some have criticised Generative AI Systems for replicating the familiar pathologies of already widely-deployed AI systems. Other critics highlight how they foreshadow vastly more powerful future systems, which might threaten humanity's survival. The first group says there is nothing new here; the other looks through the present to a perhaps distant horizon. In this paper, I instead pay attention to what makes these particular systems distinctive: both their remarkable scientific achievement, and the most likely and consequential ways in which they will change society over the next five to ten years. In particular, I explore the potential societal impacts and normative questions raised by the looming prospect of 'Language Model Agents', in which multimodal large language models (LLMs) form the executive centre of complex, tool-using AI systems that can take unsupervised sequences of actions towards some goal.", "source": "arxiv", "arxiv_id": "2404.06750v2", "pdf_url": "https://arxiv.org/pdf/2404.06750v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-04-10T05:34:07Z", "updated": "2024-10-18T09:43:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents", "authors": ["Anthony Costarelli", "Mat Allen", "Roman Hauksson", "Grace Sodunke", "Suhas Hariharan", "Carlson Cheng", "Wenjie Li", "Joshua Clymer", "Arjun Yadav"], "year": 2024, "url": "http://arxiv.org/abs/2406.06613v2", "abstract": "Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worst GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.", "source": "arxiv", "arxiv_id": "2406.06613v2", "pdf_url": "https://arxiv.org/pdf/2406.06613v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-07T00:28:43Z", "updated": "2024-07-22T14:32:33Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data Analysis", "authors": ["Haoyang Liu", "Shuyu Chen", "Ye Zhang", "Haohan Wang"], "year": 2024, "url": "http://arxiv.org/abs/2406.15341v3", "abstract": "Recent advancements in machine learning have significantly improved the identification of disease-associated genes from gene expression datasets. However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To support the evaluation and development of such methods, we introduce GenoTEX, a benchmark dataset for the automated analysis of gene expression data. GenoTEX provides analysis code and results for solving a wide range of gene-trait association problems, encompassing dataset selection, preprocessing, and statistical analysis, in a pipeline that follows computational genomics standards. The benchmark includes expert-curated annotations from bioinformaticians to ensure accuracy and reliability. To provide baselines for these tasks, we present GenoAgent, a team of LLM-based agents that adopt a multi-step programming workflow with flexible self-correction, to collaboratively analyze gene expression datasets. Our experiments demonstrate the potential of LLM-based methods in analyzing genomic data, while error analysis highlights the challenges and areas for future improvement. We propose GenoTEX as a promising resource for benchmarking and enhancing automated methods for gene expression data analysis. The benchmark is available at https://github.com/Liu-Hy/GenoTEX.", "source": "arxiv", "arxiv_id": "2406.15341v3", "pdf_url": "https://arxiv.org/pdf/2406.15341v3", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-21T17:55:24Z", "updated": "2025-04-08T17:09:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning", "authors": ["Zhen Xiang", "Linzhi Zheng", "Yanjie Li", "Junyuan Hong", "Qinbin Li", "Han Xie", "Jiawei Zhang", "Zidi Xiong", "Chulin Xie", "Carl Yang", "Dawn Song", "Bo Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.09187v3", "abstract": "The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security. In this paper, we propose GuardAgent, the first guardrail agent to protect target agents by dynamically checking whether their actions satisfy given safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution. By performing the code execution, GuardAgent can deterministically follow the safety guard request and safeguard target agents. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety policies for web agents. We show that GuardAgent effectively moderates the violation actions for different types of agents on these two benchmarks with over 98% and 83% guardrail accuracies, respectively. Project page: https://guardagent.github.io/", "source": "arxiv", "arxiv_id": "2406.09187v3", "pdf_url": "https://arxiv.org/pdf/2406.09187v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-06-13T14:49:26Z", "updated": "2025-05-29T03:09:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments", "authors": ["Sangmim Song", "Sarath Kodagoda", "Amal Gunatilake", "Marc G. Carmichael", "Karthick Thiyagarajan", "Jodi Martin"], "year": 2024, "url": "http://arxiv.org/abs/2410.20666v2", "abstract": "Navigation presents a significant challenge for persons with visual impairments (PVI). While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations. Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation. In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments. Our approach features a novel text-based topological map that enables the LLM to plan global paths using a simplified environmental representation, focusing on straight paths and right-angle turns to facilitate navigation. Additionally, we utilize the LLM's commonsense reasoning for hazard detection and personalized path planning based on user preferences. Simulated experiments demonstrate the system's efficacy in guiding PVI, underscoring its potential as a significant advancement in assistive technology. The results highlight Guide-LLM's ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field.", "source": "arxiv", "arxiv_id": "2410.20666v2", "pdf_url": "https://arxiv.org/pdf/2410.20666v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-10-28T01:58:21Z", "updated": "2025-03-11T23:45:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications", "authors": ["Weijie Xu", "Jay Desai", "Fanyou Wu", "Josef Valvoda", "Srinivasan H. Sengamedu"], "year": 2024, "url": "http://arxiv.org/abs/2410.11239v1", "abstract": "Recent LLM (Large Language Models) advancements benefit many fields such as education and finance, but HR has hundreds of repetitive processes, such as access requests, medical claim filing and time-off submissions, which are unaddressed. We relate these tasks to the LLM agent, which has addressed tasks such as writing assisting and customer support. We present HR-Agent, an efficient, confidential, and HR-specific LLM-based task-oriented dialogue system tailored for automating repetitive HR processes such as medical claims and access requests. Since conversation data is not sent to an LLM during inference, it preserves confidentiality required in HR-related tasks.", "source": "arxiv", "arxiv_id": "2410.11239v1", "pdf_url": "https://arxiv.org/pdf/2410.11239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-15T03:51:08Z", "updated": "2024-10-15T03:51:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent", "authors": ["Weijie Xu", "Zicheng Huang", "Wenxiang Hu", "Xi Fang", "Rajesh Kumar Cherukuri", "Naumaan Nayyar", "Lorenzo Malandri", "Srinivasan H. Sengamedu"], "year": 2024, "url": "http://arxiv.org/abs/2402.01018v1", "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains to evaluate LLM Agent. Our work has the following contributions: (1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.", "source": "arxiv", "arxiv_id": "2402.01018v1", "pdf_url": "https://arxiv.org/pdf/2402.01018v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "EACL 2024", "published": "2024-02-01T21:10:44Z", "updated": "2024-02-01T21:10:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "HackSynth: LLM Agent and Evaluation Framework for Autonomous Penetration Testing", "authors": ["Lajos Muzsai", "David Imolai", "AndrÃ¡s LukÃ¡cs"], "year": 2024, "url": "http://arxiv.org/abs/2412.01778v1", "abstract": "We introduce HackSynth, a novel Large Language Model (LLM)-based agent capable of autonomous penetration testing. HackSynth's dual-module architecture includes a Planner and a Summarizer, which enable it to generate commands and process feedback iteratively. To benchmark HackSynth, we propose two new Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms PicoCTF and OverTheWire. These benchmarks include two hundred challenges across diverse domains and difficulties, providing a standardized framework for evaluating LLM-based penetration testing agents. Based on these benchmarks, extensive experiments are presented, analyzing the core parameters of HackSynth, including creativity (temperature and top-p) and token utilization. Multiple open source and proprietary LLMs were used to measure the agent's capabilities. The experiments show that the agent performed best with the GPT-4o model, better than what the GPT-4o's system card suggests. We also discuss the safety and predictability of HackSynth's actions. Our findings indicate the potential of LLM-based agents in advancing autonomous penetration testing and the importance of robust safeguards. HackSynth and the benchmarks are publicly available to foster research on autonomous cybersecurity solutions.", "source": "arxiv", "arxiv_id": "2412.01778v1", "pdf_url": "https://arxiv.org/pdf/2412.01778v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-02T18:28:18Z", "updated": "2024-12-02T18:28:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Hacking CTFs with Plain Agents", "authors": ["Rustem Turtayev", "Artem Petrov", "Dmitrii Volkov", "Denis Volk"], "year": 2024, "url": "http://arxiv.org/abs/2412.02776v1", "abstract": "We saturate a high-school-level hacking benchmark with plain LLM agent design. Concretely, we obtain 95% performance on InterCode-CTF, a popular offensive security benchmark, using prompting, tool use, and multiple attempts. This beats prior work by Phuong et al. 2024 (29%) and Abramovich et al. 2024 (72%).\n  Our results suggest that current LLMs have surpassed the high school level in offensive cybersecurity. Their hacking capabilities remain underelicited: our ReAct&Plan prompting strategy solves many challenges in 1-2 turns without complex engineering or advanced harnessing.", "source": "arxiv", "arxiv_id": "2412.02776v1", "pdf_url": "https://arxiv.org/pdf/2412.02776v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-03T19:17:45Z", "updated": "2024-12-03T19:17:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments", "authors": ["Maria Rigaki", "Carlos Catania", "Sebastian Garcia"], "year": 2024, "url": "http://arxiv.org/abs/2409.11276v1", "abstract": "Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts", "source": "arxiv", "arxiv_id": "2409.11276v1", "pdf_url": "https://arxiv.org/pdf/2409.11276v1", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-09-17T15:28:25Z", "updated": "2024-09-17T15:28:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model", "authors": ["Mustafa Yildirim", "Barkin Dagda", "Saber Fallah"], "year": 2024, "url": "http://arxiv.org/abs/2405.13547v1", "abstract": "Autonomous driving is a complex task which requires advanced decision making and control algorithms. Understanding the rationale behind the autonomous vehicles' decision is crucial to ensure their safe and effective operation on highway driving. This study presents a novel approach, HighwayLLM, which harnesses the reasoning capabilities of large language models (LLMs) to predict the future waypoints for ego-vehicle's navigation. Our approach also utilizes a pre-trained Reinforcement Learning (RL) model to serve as a high-level planner, making decisions on appropriate meta-level actions. The HighwayLLM combines the output from the RL model and the current state information to make safe, collision-free, and explainable predictions for the next states, thereby constructing a trajectory for the ego-vehicle. Subsequently, a PID-based controller guides the vehicle to the waypoints predicted by the LLM agent. This integration of LLM with RL and PID enhances the decision-making process and provides interpretability for highway autonomous driving.", "source": "arxiv", "arxiv_id": "2405.13547v1", "pdf_url": "https://arxiv.org/pdf/2405.13547v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-05-22T11:32:37Z", "updated": "2024-05-22T11:32:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis", "authors": ["Federico Bianchi", "Patrick John Chia", "Mert Yuksekgonul", "Jacopo Tagliabue", "Dan Jurafsky", "James Zou"], "year": 2024, "url": "http://arxiv.org/abs/2402.05863v1", "abstract": "Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20\\% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, \\NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities.", "source": "arxiv", "arxiv_id": "2402.05863v1", "pdf_url": "https://arxiv.org/pdf/2402.05863v1", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-08T17:51:48Z", "updated": "2024-02-08T17:51:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Human-Centered LLM-Agent User Interface: A Position Paper", "authors": ["Daniel Chin", "Yuxuan Wang", "Gus Xia"], "year": 2024, "url": "http://arxiv.org/abs/2405.13050v2", "abstract": "Large Language Model (LLM) -in-the-loop applications have been shown to effectively interpret the human user's commands, make plans, and operate external tools/systems accordingly. Still, the operation scope of the LLM agent is limited to passively following the user, requiring the user to frame his/her needs with regard to the underlying tools/systems. We note that the potential of an LLM-Agent User Interface (LAUI) is much greater. A user mostly ignorant to the underlying tools/systems should be able to work with a LAUI to discover an emergent workflow. Contrary to the conventional way of designing an explorable GUI to teach the user a predefined set of ways to use the system, in the ideal LAUI, the LLM agent is initialized to be proficient with the system, proactively studies the user and his/her needs, and proposes new interaction schemes to the user. To illustrate LAUI, we present Flute X GPT, a concrete example using an LLM agent, a prompt manager, and a flute-tutoring multi-modal software-hardware system to facilitate the complex, real-time user experience of learning to play the flute.", "source": "arxiv", "arxiv_id": "2405.13050v2", "pdf_url": "https://arxiv.org/pdf/2405.13050v2", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-05-19T13:02:45Z", "updated": "2024-09-23T16:41:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent", "authors": ["Jie JW Wu", "Fatemeh H Fard"], "year": 2024, "url": "http://arxiv.org/abs/2406.00215v3", "abstract": "Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.\n  In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.", "source": "arxiv", "arxiv_id": "2406.00215v3", "pdf_url": "https://arxiv.org/pdf/2406.00215v3", "categories": ["cs.SE"], "primary_category": "cs.SE", "doi": "10.1145/3715109", "venue": "ACM Trans. Softw. Eng. Methodol., Published Jan 2025", "published": "2024-05-31T22:06:18Z", "updated": "2025-01-27T20:54:06Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction", "authors": ["Kaiyu He", "Mian Zhang", "Shuo Yan", "Peilin Wu", "Zhiyu Zoey Chen"], "year": 2024, "url": "http://arxiv.org/abs/2408.10455v6", "abstract": "While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, our study with human participants reveals notable discrepancies in rule-learning behaviors between humans and LLMs. We believe our benchmark will serve as a valuable and challenging resource, and IDEA will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. Our code and data is publicly available.", "source": "arxiv", "arxiv_id": "2408.10455v6", "pdf_url": "https://arxiv.org/pdf/2408.10455v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-19T23:37:07Z", "updated": "2025-05-27T05:26:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents", "authors": ["Zehao Wang", "Dong Jae Kim", "Tse-Hsun Chen"], "year": 2024, "url": "http://arxiv.org/abs/2406.12806v1", "abstract": "Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are widespread, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfSense, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfSense employs LLM agents to simulate interactions between developers and performance engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of seven open-source Java systems demonstrates that PerfSense achieves an average accuracy of 64.77% in classifying performance-sensitive configurations, outperforming both our LLM baseline (50.36%) and the previous state-of-the-art method (61.75%). Notably, our prompt chaining technique improves recall by 10% to 30% while maintaining similar precision levels. Additionally, a manual analysis of 362 misclassifications reveals common issues, including LLMs' misunderstandings of requirements (26.8%). In summary, PerfSense significantly reduces manual effort in classifying performance-sensitive configurations and offers valuable insights for future LLM-based code analysis research.", "source": "arxiv", "arxiv_id": "2406.12806v1", "pdf_url": "https://arxiv.org/pdf/2406.12806v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-06-18T17:22:48Z", "updated": "2024-06-18T17:22:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Imprompter: Tricking LLM Agents into Improper Tool Use", "authors": ["Xiaohan Fu", "Shuheng Li", "Zihan Wang", "Yihao Liu", "Rajesh K. Gupta", "Taylor Berg-Kirkpatrick", "Earlence Fernandes"], "year": 2024, "url": "http://arxiv.org/abs/2410.14923v2", "abstract": "Large Language Model (LLM) Agents are an emerging computing paradigm that blends generative machine learning with tools such as code interpreters, web browsing, email, and more generally, external resources. These agent-based systems represent an emerging shift in personal computing. We contribute to the security foundations of agent-based systems and surface a new class of automatically computed obfuscated adversarial prompt attacks that violate the confidentiality and integrity of user resources connected to an LLM agent. We show how prompt optimization techniques can find such prompts automatically given the weights of a model. We demonstrate that such attacks transfer to production-level agents. For example, we show an information exfiltration attack on Mistral's LeChat agent that analyzes a user's conversation, picks out personally identifiable information, and formats it into a valid markdown command that results in leaking that data to the attacker's server. This attack shows a nearly 80% success rate in an end-to-end evaluation. We conduct a range of experiments to characterize the efficacy of these attacks and find that they reliably work on emerging agent-based systems like Mistral's LeChat, ChatGLM, and Meta's Llama. These attacks are multimodal, and we show variants in the text-only and image domains.", "source": "arxiv", "arxiv_id": "2410.14923v2", "pdf_url": "https://arxiv.org/pdf/2410.14923v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-19T01:00:57Z", "updated": "2024-10-22T00:53:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility", "authors": ["Yuchen Xia", "Jize Zhang", "Nasser Jazdi", "Michael Weyrich"], "year": 2024, "url": "http://arxiv.org/abs/2407.08550v1", "abstract": "This paper introduces a novel approach to integrating large language model (LLM) agents into automated production systems, aimed at enhancing task automation and flexibility. We organize production operations within a hierarchical framework based on the automation pyramid. Atomic operation functionalities are modeled as microservices, which are executed through interface invocation within a dedicated digital twin system. This allows for a scalable and flexible foundation for orchestrating production processes. In this digital twin system, low-level, hardware-specific data is semantically enriched and made interpretable for LLMs for production planning and control tasks. Large language model agents are systematically prompted to interpret these production-specific data and knowledge. Upon receiving a user request or identifying a triggering event, the LLM agents generate a process plan. This plan is then decomposed into a series of atomic operations, executed as microservices within the real-world automation system. We implement this overall approach on an automated modular production facility at our laboratory, demonstrating how the LLMs can handle production planning and control tasks through a concrete case study. This results in an intuitive production facility with higher levels of task automation and flexibility. Finally, we reveal the several limitations in realizing the full potential of the large language models in autonomous systems and point out promising benefits. Demos of this series of ongoing research series can be accessed at: https://github.com/YuchenXia/GPT4IndustrialAutomation", "source": "arxiv", "arxiv_id": "2407.08550v1", "pdf_url": "https://arxiv.org/pdf/2407.08550v1", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.RO", "eess.SY"], "primary_category": "cs.AI", "doi": "10.51202/9783181024379", "venue": "", "published": "2024-07-11T14:34:43Z", "updated": "2024-07-11T14:34:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents", "authors": ["Qiusi Zhan", "Zhixiang Liang", "Zifan Ying", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2403.02691v3", "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.\n  In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.", "source": "arxiv", "arxiv_id": "2403.02691v3", "pdf_url": "https://arxiv.org/pdf/2403.02691v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-05T06:21:45Z", "updated": "2024-08-04T04:52:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Interacting Large Language Model Agents. Interpretable Models and Social Learning", "authors": ["Adit Jain", "Vikram Krishnamurthy"], "year": 2024, "url": "http://arxiv.org/abs/2411.01271v2", "abstract": "This paper discusses the theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making involving interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors. This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under 2 settings: (a) centrally controlled LLMAs (b) autonomous LLMAs with incentives. We demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like LLaMA and closed-source models like ChatGPT. The main takeaway of this paper, based on empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting.", "source": "arxiv", "arxiv_id": "2411.01271v2", "pdf_url": "https://arxiv.org/pdf/2411.01271v2", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.MA", "eess.SY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-11-02T14:49:34Z", "updated": "2025-05-25T12:58:44Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations", "authors": ["Yucheng Jiang", "Yijia Shao", "Dekun Ma", "Sina J. Semnani", "Monica S. Lam"], "year": 2024, "url": "http://arxiv.org/abs/2408.15232v2", "abstract": "While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.", "source": "arxiv", "arxiv_id": "2408.15232v2", "pdf_url": "https://arxiv.org/pdf/2408.15232v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-27T17:50:03Z", "updated": "2024-10-17T20:43:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild", "authors": ["Reworr", "Dmitrii Volkov"], "year": 2024, "url": "http://arxiv.org/abs/2410.13919v2", "abstract": "Attacks powered by Large Language Model (LLM) agents represent a growing threat to modern cybersecurity. To address this concern, we present LLM Honeypot, a system designed to monitor autonomous AI hacking agents. By augmenting a standard SSH honeypot with prompt injection and time-based analysis techniques, our framework aims to distinguish LLM agents among all attackers. Over a trial deployment of about three months in a public environment, we collected 8,130,731 hacking attempts and 8 potential AI agents. Our work demonstrates the emergence of AI-driven threats and their current level of usage, serving as an early warning of malicious LLM agents in the wild.", "source": "arxiv", "arxiv_id": "2410.13919v2", "pdf_url": "https://arxiv.org/pdf/2410.13919v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-17T09:25:28Z", "updated": "2025-02-10T22:06:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agent for Fire Dynamics Simulations", "authors": ["Leidong Xu", "Danyal Mohaddes", "Yi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2412.17146v1", "abstract": "Significant advances have been achieved in leveraging foundation models, such as large language models (LLMs), to accelerate complex scientific workflows. In this work we introduce FoamPilot, a proof-of-concept LLM agent designed to enhance the usability of FireFOAM, a specialized solver for fire dynamics and fire suppression simulations built using OpenFOAM, a popular open-source toolbox for computational fluid dynamics (CFD). FoamPilot provides three core functionalities: code insight, case configuration and simulation evaluation. Code insight is an alternative to traditional keyword searching leveraging retrieval-augmented generation (RAG) and aims to enable efficient navigation and summarization of the FireFOAM source code for developers and experienced users. For case configuration, the agent interprets user requests in natural language and aims to modify existing simulation setups accordingly to support intermediate users. FoamPilot's job execution functionality seeks to manage the submission and execution of simulations in high-performance computing (HPC) environments and provide preliminary analysis of simulation results to support less experienced users. Promising results were achieved for each functionality, particularly for simple tasks, and opportunities were identified for significant further improvement for more complex tasks. The integration of these functionalities into a single LLM agent is a step aimed at accelerating the simulation workflow for engineers and scientists employing FireFOAM for complex simulations critical for improving fire safety.", "source": "arxiv", "arxiv_id": "2412.17146v1", "pdf_url": "https://arxiv.org/pdf/2412.17146v1", "categories": ["cs.AI", "physics.flu-dyn"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-22T20:03:35Z", "updated": "2024-12-22T20:03:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents as 6G Orchestrator: A Paradigm for Task-Oriented Physical-Layer Automation", "authors": ["Zhuoran Xiao", "Chenhui Ye", "Yunbo Hu", "Honggang Yuan", "Yihang Huang", "Yijia Feng", "Liyu Cai", "Jiang Chang"], "year": 2024, "url": "http://arxiv.org/abs/2410.03688v1", "abstract": "The rapid advancement in generative pre-training models is propelling a paradigm shift in technological progression from basic applications such as chatbots towards more sophisticated agent-based systems. It is with huge potential and necessity that the 6G system be combined with the copilot of large language model (LLM) agents and digital twins (DT) to manage the highly complicated communication system with new emerging features such as native AI service and sensing. With the 6G-oriented agent, the base station could understand the transmission requirements of various dynamic upper-layer tasks, automatically orchestrate the optimal system workflow. Through continuously get feedback from the 6G DT for reinforcement, the agents can finally raise the performance of practical system accordingly. Differing from existing LLM agents designed for general application, the 6G-oriented agent aims to make highly rigorous and precise planning with a vast amount of extra expert knowledge, which inevitably requires a specific system design from model training to implementation. This paper proposes a novel comprehensive approach for building task-oriented 6G LLM agents. We first propose a two-stage continual pre-training and fine-tuning scheme to build the field basic model and diversities of specialized expert models for meeting the requirements of various application scenarios. Further, a novel inference framework based on semantic retrieval for leveraging the existing communication-related functions is proposed. Experiment results of exemplary tasks, such as physical-layer task decomposition, show the proposed paradigm's feasibility and effectiveness.", "source": "arxiv", "arxiv_id": "2410.03688v1", "pdf_url": "https://arxiv.org/pdf/2410.03688v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI", "doi": "", "venue": "", "published": "2024-09-21T05:08:29Z", "updated": "2024-09-21T05:08:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents can Autonomously Exploit One-day Vulnerabilities", "authors": ["Richard Fang", "Rohan Bindu", "Akul Gupta", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2404.08144v2", "abstract": "LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.\n  In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.", "source": "arxiv", "arxiv_id": "2404.08144v2", "pdf_url": "https://arxiv.org/pdf/2404.08144v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-04-11T22:07:19Z", "updated": "2024-04-17T04:34:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents can Autonomously Hack Websites", "authors": ["Richard Fang", "Rohan Bindu", "Akul Gupta", "Qiusi Zhan", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2402.06664v3", "abstract": "In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.\n  In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.", "source": "arxiv", "arxiv_id": "2402.06664v3", "pdf_url": "https://arxiv.org/pdf/2402.06664v3", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-02-06T14:46:08Z", "updated": "2024-02-16T04:02:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models", "authors": ["Ivar Frisch", "Mario Giulianelli"], "year": 2024, "url": "http://arxiv.org/abs/2402.02896v1", "abstract": "While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.", "source": "arxiv", "arxiv_id": "2402.02896v1", "pdf_url": "https://arxiv.org/pdf/2402.02896v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-05T11:05:20Z", "updated": "2024-02-05T11:05:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Design of Multi Active/Passive Core-Agent Architectures", "authors": ["Amine Ben Hassouna", "Hana Chaari", "Ines Belhaj"], "year": 2024, "url": "http://arxiv.org/abs/2409.11393v3", "abstract": "In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand for sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture, resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that establishes a clear foundation for agent development from both functional and software architectural perspectives, developed and evaluated using the Architecture Tradeoff and Risk Analysis Framework (ATRAF). Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs and tools apart from a new element, the core-agent, which plays the role of central coordinator. This pivotal entity comprises five modules: planning, memory, profile, action, and security -- the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying overlooked architectural aspects. Moreover, we thoroughly assess five architecture variants of our framework by designing new agent architectures that combine characteristics of state-of-the-art agents to address specific goals. ...", "source": "arxiv", "arxiv_id": "2409.11393v3", "pdf_url": "https://arxiv.org/pdf/2409.11393v3", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.SE", "doi": "10.1016/j.inffus.2025.103865", "venue": "Information Fusion 127 (2026) 103865", "published": "2024-09-17T17:54:17Z", "updated": "2025-11-21T13:25:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game Difficulty with LLM Agents", "authors": ["Chang Xiao", "Brenda Z. Yang"], "year": 2024, "url": "http://arxiv.org/abs/2410.02829v1", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated their potential as autonomous agents across various tasks. One emerging application is the use of LLMs in playing games. In this work, we explore a practical problem for the gaming industry: Can LLMs be used to measure game difficulty? We propose a general game-testing framework using LLM agents and test it on two widely played strategy games: Wordle and Slay the Spire. Our results reveal an interesting finding: although LLMs may not perform as well as the average human player, their performance, when guided by simple, generic prompting techniques, shows a statistically significant and strong correlation with difficulty indicated by human players. This suggests that LLMs could serve as effective agents for measuring game difficulty during the development process. Based on our experiments, we also outline general principles and guidelines for incorporating LLMs into the game testing process.", "source": "arxiv", "arxiv_id": "2410.02829v1", "pdf_url": "https://arxiv.org/pdf/2410.02829v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-01T18:40:43Z", "updated": "2024-10-01T18:40:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Language agents achieve superhuman synthesis of scientific knowledge", "authors": ["Michael D. Skarlinski", "Sam Cox", "Jon M. Laurent", "James D. Braza", "Michaela Hinks", "Michael J. Hammerling", "Manvitha Ponnapati", "Samuel G. Rodriques", "Andrew D. White"], "year": 2024, "url": "http://arxiv.org/abs/2409.13740v2", "abstract": "Language models are known to hallucinate incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of which 70% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.", "source": "arxiv", "arxiv_id": "2409.13740v2", "pdf_url": "https://arxiv.org/pdf/2409.13740v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "physics.soc-ph"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-10T16:37:58Z", "updated": "2024-09-26T15:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent as a Mechanical Designer", "authors": ["Yayati Jadhav", "Amir Barati Farimani"], "year": 2024, "url": "http://arxiv.org/abs/2404.17525v3", "abstract": "Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.", "source": "arxiv", "arxiv_id": "2404.17525v3", "pdf_url": "https://arxiv.org/pdf/2404.17525v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-04-26T16:41:24Z", "updated": "2025-04-30T18:23:36Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent for Fake News Detection", "authors": ["Xinyi Li", "Yongfeng Zhang", "Edward C. Malthouse"], "year": 2024, "url": "http://arxiv.org/abs/2405.01593v1", "abstract": "In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.", "source": "arxiv", "arxiv_id": "2405.01593v1", "pdf_url": "https://arxiv.org/pdf/2405.01593v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-04-30T06:55:27Z", "updated": "2024-04-30T06:55:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent for Hyper-Parameter Optimization", "authors": ["Siyi Liu", "Chen Gao", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.01881v3", "abstract": "Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted on 12 representative machine-learning tasks indicate that AgentHPO not only matches but also often surpasses the best human trials in terms of performance while simultaneously providing explainable results. Further analysis sheds light on the strategies employed by the LLM in optimizing these tasks, highlighting its effectiveness and adaptability in various scenarios.", "source": "arxiv", "arxiv_id": "2402.01881v3", "pdf_url": "https://arxiv.org/pdf/2402.01881v3", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-02T20:12:05Z", "updated": "2025-02-26T13:57:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agent in Financial Trading: A Survey", "authors": ["Han Ding", "Yinheng Li", "Junhao Wang", "Hang Chen"], "year": 2024, "url": "http://arxiv.org/abs/2408.06361v1", "abstract": "Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.", "source": "arxiv", "arxiv_id": "2408.06361v1", "pdf_url": "https://arxiv.org/pdf/2408.06361v1", "categories": ["q-fin.TR", "cs.CL"], "primary_category": "q-fin.TR", "doi": "", "venue": "", "published": "2024-07-26T08:53:05Z", "updated": "2024-07-26T08:53:05Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness", "authors": ["Harsh Kumar", "Suhyeon Yoo", "Angela Zavaleta Bernuy", "Jiakai Shi", "Huayin Luo", "Joseph Williams", "Anastasia Kuzminykh", "Ashton Anderson", "Rachel Kornfield"], "year": 2024, "url": "http://arxiv.org/abs/2407.13067v1", "abstract": "Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.", "source": "arxiv", "arxiv_id": "2407.13067v1", "pdf_url": "https://arxiv.org/pdf/2407.13067v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-07-03T15:43:16Z", "updated": "2024-07-03T15:43:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification", "authors": ["Guang Lin", "Toshihisa Tanaka", "Qibin Zhao"], "year": 2024, "url": "http://arxiv.org/abs/2405.20770v4", "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.", "source": "arxiv", "arxiv_id": "2405.20770v4", "pdf_url": "https://arxiv.org/pdf/2405.20770v4", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-24T07:23:56Z", "updated": "2025-04-23T05:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Models Are Neurosymbolic Reasoners", "authors": ["Meng Fang", "Shilong Deng", "Yudi Zhang", "Zijing Shi", "Ling Chen", "Mykola Pechenizkiy", "Jun Wang"], "year": 2024, "url": "http://arxiv.org/abs/2401.09334v1", "abstract": "A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.", "source": "arxiv", "arxiv_id": "2401.09334v1", "pdf_url": "https://arxiv.org/pdf/2401.09334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-17T16:57:19Z", "updated": "2024-01-17T16:57:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation", "authors": ["Jiawei Wang", "Renhe Jiang", "Chuang Yang", "Zengqing Wu", "Makoto Onizuka", "Ryosuke Shibasaki", "Noboru Koshizuka", "Chuan Xiao"], "year": 2024, "url": "http://arxiv.org/abs/2402.14744v3", "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.", "source": "arxiv", "arxiv_id": "2402.14744v3", "pdf_url": "https://arxiv.org/pdf/2402.14744v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-22T18:03:14Z", "updated": "2024-10-27T20:02:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics", "authors": ["Chenggang Cui", "Jiaming Liu", "Junkang Feng", "Peifeng Hui", "Amer M. Y. M. Ghias", "Chuanlin Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2406.12628v1", "abstract": "Power electronics, a critical component in modern power systems, face several challenges in control design, including model uncertainties, and lengthy and costly design cycles. This paper is aiming to propose a Large Language Models (LLMs) based multi-agent framework for objective-oriented control design in power electronics. The framework leverages the reasoning capabilities of LLMs and a multi-agent workflow to develop an efficient and autonomous controller design process. The LLM agent is able to understand and respond to high-level instructions in natural language, adapting its behavior based on the task's specific requirements and constraints from a practical implementation point of view. This novel and efficient approach promises a more flexible and adaptable controller design process in power electronics that will largely facilitate the practitioners.", "source": "arxiv", "arxiv_id": "2406.12628v1", "pdf_url": "https://arxiv.org/pdf/2406.12628v1", "categories": ["eess.SY"], "primary_category": "eess.SY", "doi": "", "venue": "", "published": "2024-06-18T13:54:12Z", "updated": "2024-06-18T13:54:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large language model empowered participatory urban planning", "authors": ["Zhilun Zhou", "Yuming Lin", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2402.01698v1", "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.", "source": "arxiv", "arxiv_id": "2402.01698v1", "pdf_url": "https://arxiv.org/pdf/2402.01698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-24T10:50:01Z", "updated": "2024-01-24T10:50:01Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LegalAgentBench: Evaluating LLM Agents in Legal Domain", "authors": ["Haitao Li", "Junjie Chen", "Jingli Yang", "Qingyao Ai", "Wei Jia", "Youfeng Liu", "Kai Lin", "Yueyue Wu", "Guozhi Yuan", "Yiran Hu", "Wuyue Wang", "Yiqun Liu", "Minlie Huang"], "year": 2024, "url": "http://arxiv.org/abs/2412.17259v1", "abstract": "With the increasing intelligence and autonomy of LLM agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks cannot fully capture the complexity and subtle nuances of real-world judicial cognition and decision-making. Therefore, we propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. We designed a scalable task construction framework and carefully annotated 300 tasks. These tasks span various types, including multi-hop reasoning and writing, and range across different difficulty levels, effectively reflecting the complexity of real-world legal scenarios. Moreover, beyond evaluating final success, LegalAgentBench incorporates keyword analysis during intermediate processes to calculate progress rates, enabling more fine-grained evaluation. We evaluated eight popular LLMs, highlighting the strengths, limitations, and potential areas for improvement of existing models and methods. LegalAgentBench sets a new benchmark for the practical application of LLMs in the legal domain, with its code and data available at \\url{https://github.com/CSHaitao/LegalAgentBench}.", "source": "arxiv", "arxiv_id": "2412.17259v1", "pdf_url": "https://arxiv.org/pdf/2412.17259v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-23T04:02:46Z", "updated": "2024-12-23T04:02:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution", "authors": ["Yuheng Zhao", "Junjie Wang", "Linbin Xiang", "Xiaowen Zhang", "Zifei Guo", "Cagatay Turkay", "Yu Zhang", "Siming Chen"], "year": 2024, "url": "http://arxiv.org/abs/2411.05651v2", "abstract": "Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.", "source": "arxiv", "arxiv_id": "2411.05651v2", "pdf_url": "https://arxiv.org/pdf/2411.05651v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1109/TVCG.2024.3496112", "venue": "IEEE Transactions on Visualization and Computer Graphics 2024", "published": "2024-11-08T15:46:10Z", "updated": "2025-06-21T06:39:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents", "authors": ["Yun Xing", "Nhat Chung", "Jie Zhang", "Yue Cao", "Ivor Tsang", "Yang Liu", "Lei Ma", "Qing Guo"], "year": 2024, "url": "http://arxiv.org/abs/2412.08014v2", "abstract": "Physical adversarial attacks in driving scenarios can expose critical vulnerabilities in visual perception models. However, developing such attacks remains challenging due to diverse real-world environments and the requirement for maintaining visual naturality. Building upon this challenge, we reformulate physical adversarial attacks as a one-shot patch generation problem. Our approach generates adversarial patches through a deep generative model that considers the specific scene context, enabling direct physical deployment in matching environments. The primary challenge lies in simultaneously achieving two objectives: generating adversarial patches that effectively mislead object detection systems while determining contextually appropriate deployment within the scene. We propose MAGIC (Mastering Physical Adversarial Generation In Context), a novel framework powered by multi-modal LLM agents to address these challenges. MAGIC automatically understands scene context and generates adversarial patch through the synergistic interaction of language and vision capabilities. In particular, MAGIC orchestrates three specialized LLM agents: The adv-patch generation agent (GAgent) masters the creation of deceptive patches through strategic prompt engineering for text-to-image models. The adv-patch deployment agent (DAgent) ensures contextual coherence by determining optimal deployment strategies based on scene understanding. The self-examination agent (EAgent) completes this trilogy by providing critical oversight and iterative refinement of both processes. We validate our method on both digital and physical levels, i.e., nuImage and manually captured real-world scenes, where both statistical and visual results prove that our MAGIC is powerful and effective for attacking widely applied object detection systems, i.e., YOLO and DETR series.", "source": "arxiv", "arxiv_id": "2412.08014v2", "pdf_url": "https://arxiv.org/pdf/2412.08014v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-12-11T01:41:19Z", "updated": "2025-03-11T07:15:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MEGAnno+: A Human-LLM Collaborative Annotation System", "authors": ["Hannah Kim", "Kushan Mitra", "Rafael Li Chen", "Sajjadur Rahman", "Dan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2402.18050v1", "abstract": "Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.", "source": "arxiv", "arxiv_id": "2402.18050v1", "pdf_url": "https://arxiv.org/pdf/2402.18050v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-28T04:58:07Z", "updated": "2024-02-28T04:58:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MIRAI: Evaluating LLM Agents for Event Forecasting", "authors": ["Chenchen Ye", "Ziniu Hu", "Yihe Deng", "Zijie Huang", "Mingyu Derek Ma", "Yanqiao Zhu", "Wei Wang"], "year": 2024, "url": "http://arxiv.org/abs/2407.01231v1", "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.", "source": "arxiv", "arxiv_id": "2407.01231v1", "pdf_url": "https://arxiv.org/pdf/2407.01231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-01T12:22:46Z", "updated": "2024-07-01T12:22:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents", "authors": ["Ruochen Li", "Teerth Patel", "Qingyun Wang", "Xinya Du"], "year": 2024, "url": "http://arxiv.org/abs/2408.14033v3", "abstract": "Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.", "source": "arxiv", "arxiv_id": "2408.14033v3", "pdf_url": "https://arxiv.org/pdf/2408.14033v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-26T05:55:48Z", "updated": "2025-11-14T19:05:10Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MathLearner: A Large Language Model Agent Framework for Learning to Solve Mathematical Problems", "authors": ["Wenbei Xie", "Donglin Liu", "Haoran Yan", "Wenjie Wu", "Zongyang Liu"], "year": 2024, "url": "http://arxiv.org/abs/2408.01779v1", "abstract": "With the development of artificial intelligence (AI), large language models (LLM) are widely used in many fields. However, the reasoning ability of LLM is still very limited when it comes to mathematical reasoning. Mathematics plays an important role in all aspects of human society and is a technical guarantee in the fields of healthcare, transport and aerospace, for this reason, the development of AI big language models in the field of mathematics has great potential significance. To improve the mathematical reasoning ability of large language models, we proposed an agent framework for learning to solve mathematical problems based on inductive reasoning. By emulating the human learning process of generalization of learned information and effective application of previous knowledge in new reasoning tasks, this framework has great performance in the mathematical reasoning process. It improves global accuracy over the baseline method (chain-of-thought) by 20.96% and solves 17.54% of the mathematical problems that the baseline cannot solve. Benefiting from the efficient RETRIEVAL method, our model improves the ability of large language models to efficiently use external knowledge, i.e., the mathematical computation of the model can be based on written procedures. In education, our model can be used as a personalised learning aid, thus reducing the inequality of educational resources.", "source": "arxiv", "arxiv_id": "2408.01779v1", "pdf_url": "https://arxiv.org/pdf/2408.01779v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-08-03T13:28:19Z", "updated": "2024-08-03T13:28:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents", "authors": ["Arya Bulusu", "Brandon Man", "Ashish Jagmohan", "Aditya Vempaty", "Jennifer Mari-Wyka", "Deepak Akkil"], "year": 2024, "url": "http://arxiv.org/abs/2407.17544v1", "abstract": "There has been significant recent interest in harnessing LLMs to control software systems through multi-step reasoning, planning and tool-usage. While some promising results have been obtained, application to specific domains raises several general issues including the control of specialized domain tools, the lack of existing datasets for training and evaluation, and the non-triviality of automated system evaluation and improvement. In this paper, we present a case-study where we examine these issues in the context of a specific domain. Specifically, we present an automated math visualizer and solver system for mathematical pedagogy. The system orchestrates mathematical solvers and math graphing tools to produce accurate visualizations from simple natural language commands. We describe the creation of specialized data-sets, and also develop an auto-evaluator to easily evaluate the outputs of our system by comparing them to ground-truth expressions. We have open sourced the data-sets and code for the proposed system.", "source": "arxiv", "arxiv_id": "2407.17544v1", "pdf_url": "https://arxiv.org/pdf/2407.17544v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-07-24T15:45:07Z", "updated": "2024-07-24T15:45:07Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling", "authors": ["Yakun Zhu", "Shaohang Wei", "Xu Wang", "Kui Xue", "Xiaofan Zhang", "Shaoting Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.13610v3", "abstract": "Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual's health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.", "source": "arxiv", "arxiv_id": "2410.13610v3", "pdf_url": "https://arxiv.org/pdf/2410.13610v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-17T14:46:22Z", "updated": "2025-05-23T07:21:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Memory-Augmented Agent Training for Business Document Understanding", "authors": ["Jiale Liu", "Yifan Zeng", "Malte HÃ¸jmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "year": 2024, "url": "http://arxiv.org/abs/2412.15274v1", "abstract": "Traditional enterprises face significant challenges in processing business documents, where tasks like extracting transport references from invoices remain largely manual despite their crucial role in logistics operations. While Large Language Models offer potential automation, their direct application to specialized business domains often yields unsatisfactory results. We introduce Matrix (Memory-Augmented agent Training through Reasoning and Iterative eXploration), a novel paradigm that enables LLM agents to progressively build domain expertise through experience-driven memory refinement and iterative learning. To validate this approach, we collaborate with one of the world's largest logistics companies to create a dataset of Universal Business Language format invoice documents, focusing on the task of transport reference extraction. Experiments demonstrate that Matrix outperforms prompting a single LLM by 30.3%, vanilla LLM agent by 35.2%. We further analyze the metrics of the optimized systems and observe that the agent system requires less API calls, fewer costs and can analyze longer documents on average. Our methods establish a new approach to transform general-purpose LLMs into specialized business tools through systematic memory enhancement in document processing tasks.", "source": "arxiv", "arxiv_id": "2412.15274v1", "pdf_url": "https://arxiv.org/pdf/2412.15274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-17T18:35:04Z", "updated": "2024-12-17T18:35:04Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Molly: Making Large Language Model Agents Solve Python Problem More Logically", "authors": ["Rui Xiao", "Jiong Wang", "Lu Han", "Na Zong", "Han Wu"], "year": 2024, "url": "http://arxiv.org/abs/2412.18093v1", "abstract": "Applying large language models (LLMs) as teaching assists has attracted much attention as an integral part of intelligent education, particularly in computing courses. To reduce the gap between the LLMs and the computer programming education expert, fine-tuning and retrieval augmented generation (RAG) are the two mainstream methods in existing researches. However, fine-tuning for specific tasks is resource-intensive and may diminish the model`s generalization capabilities. RAG can perform well on reducing the illusion of LLMs, but the generation of irrelevant factual content during reasoning can cause significant confusion for learners. To address these problems, we introduce the Molly agent, focusing on solving the proposed problem encountered by learners when learning Python programming language. Our agent automatically parse the learners' questioning intent through a scenario-based interaction, enabling precise retrieval of relevant documents from the constructed knowledge base. At generation stage, the agent reflect on the generated responses to ensure that they not only align with factual content but also effectively answer the user's queries. Extensive experimentation on a constructed Chinese Python QA dataset shows the effectiveness of the Molly agent, indicating an enhancement in its performance for providing useful responses to Python questions.", "source": "arxiv", "arxiv_id": "2412.18093v1", "pdf_url": "https://arxiv.org/pdf/2412.18093v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-24T02:08:38Z", "updated": "2024-12-24T02:08:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Moral Alignment for LLM Agents", "authors": ["Elizaveta Tennant", "Stephen Hailes", "Mirco Musolesi"], "year": 2024, "url": "http://arxiv.org/abs/2410.01639v4", "abstract": "Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are underway to develop more generalist agents. As LLM-based systems become more agentic, their influence on human activity will grow and their transparency will decrease. Consequently, developing effective methods for aligning them to human values is vital.\n  The prevailing practice in alignment often relies on human preference data (e.g., in RLHF or DPO), in which values are implicit, opaque and are essentially deduced from relative preferences over different model outputs. In this work, instead of relying on human feedback, we introduce the design of reward functions that explicitly and transparently encode core human values for Reinforcement Learning-based fine-tuning of foundation agent models. Specifically, we use intrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of Deontological Ethics and Utilitarianism, quantifying moral rewards for agents in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD) environment. We also show how moral fine-tuning can be deployed to enable an agent to unlearn a previously developed selfish strategy. Finally, we find that certain moral strategies learned on the IPD game generalize to several other matrix game environments. In summary, we demonstrate that fine-tuning with intrinsic rewards is a promising general solution for aligning LLM agents to human values, and it might represent a more transparent and cost-effective alternative to currently predominant alignment techniques.", "source": "arxiv", "arxiv_id": "2410.01639v4", "pdf_url": "https://arxiv.org/pdf/2410.01639v4", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-02T15:09:36Z", "updated": "2025-05-11T19:14:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion", "authors": ["Sen Li", "Ruochen Wang", "Cho-Jui Hsieh", "Minhao Cheng", "Tianyi Zhou"], "year": 2024, "url": "http://arxiv.org/abs/2402.12741v2", "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. To efficiently address these challenges, we develop a training-free Multimodal-LLM agent (MuLan), as a human painter, that can progressively generate multi-object with intricate planning and feedback control. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object by stable diffusion, conditioned on previously generated objects. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined upon each sub-task by an LLM and attention guidance. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. The multi-step process also allows human users to monitor the generation process and make preferred changes at any intermediate step via text prompts, thereby improving the human-AI collaboration experience. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines and its creativity when collaborating with human users. The code is available at https://github.com/measure-infinity/mulan-code.", "source": "arxiv", "arxiv_id": "2402.12741v2", "pdf_url": "https://arxiv.org/pdf/2402.12741v2", "categories": ["cs.CV"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-02-20T06:14:30Z", "updated": "2024-05-24T15:56:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Planning Case Study", "authors": ["Shangding Gu"], "year": 2024, "url": "http://arxiv.org/abs/2401.06603v2", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as \"I help you help I help.\" The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization, exploration, and mutual improvement for both agents, enabling them to accomplish increasingly challenging tasks. Remarkably, we propose a practical algorithm to address the problem and conduct empirical experiments to evaluate the effectiveness of our method.", "source": "arxiv", "arxiv_id": "2401.06603v2", "pdf_url": "https://arxiv.org/pdf/2401.06603v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-12T14:35:57Z", "updated": "2025-03-02T01:46:57Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Offline Training of Language Model Agents with Functions as Learnable Weights", "authors": ["Shaokun Zhang", "Jieyu Zhang", "Jiale Liu", "Linxin Song", "Chi Wang", "Ranjay Krishna", "Qingyun Wu"], "year": 2024, "url": "http://arxiv.org/abs/2402.11359v4", "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.", "source": "arxiv", "arxiv_id": "2402.11359v4", "pdf_url": "https://arxiv.org/pdf/2402.11359v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-17T18:31:21Z", "updated": "2024-07-30T18:22:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "On the Structural Memory of LLM Agents", "authors": ["Ruihong Zeng", "Jinyuan Fang", "Siwei Liu", "Zaiqiao Meng"], "year": 2024, "url": "http://arxiv.org/abs/2412.15266v1", "abstract": "Memory plays a pivotal role in enabling large language model~(LLM)-based agents to engage in complex and long-term interactions, such as question answering (QA) and dialogue systems. While various memory modules have been proposed for these tasks, the impact of different memory structures across tasks remains insufficiently explored. This paper investigates how memory structures and memory retrieval methods affect the performance of LLM-based agents. Specifically, we evaluate four types of memory structures, including chunks, knowledge triples, atomic facts, and summaries, along with mixed memory that combines these components. In addition, we evaluate three widely used memory retrieval methods: single-step retrieval, reranking, and iterative retrieval. Extensive experiments conducted across four tasks and six datasets yield the following key insights: (1) Different memory structures offer distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed memory structures demonstrate remarkable resilience in noisy environments; (3) Iterative retrieval consistently outperforms other methods across various scenarios. Our investigation aims to inspire further research into the design of memory systems for LLM-based agents.", "source": "arxiv", "arxiv_id": "2412.15266v1", "pdf_url": "https://arxiv.org/pdf/2412.15266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-17T04:30:00Z", "updated": "2024-12-17T04:30:00Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "year": 2024, "url": "http://arxiv.org/abs/2401.07115v3", "abstract": "The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.", "source": "arxiv", "arxiv_id": "2401.07115v3", "pdf_url": "https://arxiv.org/pdf/2401.07115v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-13T16:41:40Z", "updated": "2025-03-22T22:45:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM Agents", "authors": ["Yuwei Yan", "Qingbin Zeng", "Zhiheng Zheng", "Jingzhe Yuan", "Jie Feng", "Jun Zhang", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2410.21286v1", "abstract": "Agent-based models (ABMs) have long been employed to explore how individual behaviors aggregate into complex societal phenomena in urban space. Unlike black-box predictive models, ABMs excel at explaining the micro-macro linkages that drive such emergent behaviors. The recent rise of Large Language Models (LLMs) has led to the development of LLM agents capable of simulating urban activities with unprecedented realism. However, the extreme high computational cost of LLMs presents significant challenges for scaling up the simulations of LLM agents. To address this problem, we propose OpenCity, a scalable simulation platform optimized for both system and prompt efficiencies. Specifically, we propose a LLM request scheduler to reduce communication overhead by parallelizing requests through IO multiplexing. Besides, we deisgn a \"group-and-distill\" prompt optimization strategy minimizes redundancy by clustering agents with similar static attributes. Through experiments on six global cities, OpenCity achieves a 600-fold acceleration in simulation time per agent, a 70% reduction in LLM requests, and a 50% reduction in token usage. These improvements enable the simulation of 10,000 agents' daily activities in 1 hour on commodity hardware. Besides, the substantial speedup of OpenCity allows us to establish a urban simulation benchmark for LLM agents for the first time, comparing simulated urban activities with real-world data in 6 major cities around the globe. We believe our OpenCity platform provides a critical infrastructure to harness the power of LLMs for interdisciplinary studies in urban space, fostering the collective efforts of broader research communities. Code repo is available at https://anonymous.4open.science/r/Anonymous-OpenCity-42BD.", "source": "arxiv", "arxiv_id": "2410.21286v1", "pdf_url": "https://arxiv.org/pdf/2410.21286v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-10-11T13:52:35Z", "updated": "2024-10-11T13:52:35Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents", "authors": ["Xiangyu Yin", "Chuqiao Shi", "Yimo Han", "Yi Jiang"], "year": 2024, "url": "http://arxiv.org/abs/2410.09034v1", "abstract": "Ptychography is an advanced computational imaging technique in X-ray and electron microscopy. It has been widely adopted across scientific research fields, including physics, chemistry, biology, and materials science, as well as in industrial applications such as semiconductor characterization. In practice, obtaining high-quality ptychographic images requires simultaneous optimization of numerous experimental and algorithmic parameters. Traditionally, parameter selection often relies on trial and error, leading to low-throughput workflows and potential human bias. In this work, we develop the \"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that leverages large language models (LLMs) to automate data analysis in ptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM agents for tasks including knowledge retrieval, code generation, parameter recommendation, and image reasoning. Our study demonstrates that PEAR's multi-agent design significantly improves the workflow success rate, even with smaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various automation levels and is designed to work with customized local knowledge bases, ensuring flexibility and adaptability across different research environments.", "source": "arxiv", "arxiv_id": "2410.09034v1", "pdf_url": "https://arxiv.org/pdf/2410.09034v1", "categories": ["cs.CE", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.CE", "doi": "", "venue": "", "published": "2024-10-11T17:50:59Z", "updated": "2024-10-11T17:50:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PRACT: Optimizing Principled Reasoning and Acting of LLM Agent", "authors": ["Zhiwei Liu", "Weiran Yao", "Jianguo Zhang", "Rithesh Murthy", "Liangwei Yang", "Zuxin Liu", "Tian Lan", "Ming Zhu", "Juntao Tan", "Shirley Kokane", "Thai Hoang", "Juan Carlos Niebles", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "year": 2024, "url": "http://arxiv.org/abs/2410.18528v1", "abstract": "We introduce the Principled Reasoning and Acting (PRAct) framework, a novel method for learning and enforcing action principles from trajectory data. Central to our approach is the use of text gradients from a reflection and optimization engine to derive these action principles. To adapt action principles to specific task requirements, we propose a new optimization framework, Reflective Principle Optimization (RPO). After execution, RPO employs a reflector to critique current action principles and an optimizer to update them accordingly. We develop the RPO framework under two scenarios: Reward-RPO, which uses environmental rewards for reflection, and Self-RPO, which conducts self-reflection without external rewards. Additionally, two RPO methods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings. Experimental results across four environments demonstrate that the PRAct agent, leveraging the RPO framework, effectively learns and applies action principles to enhance performance.", "source": "arxiv", "arxiv_id": "2410.18528v1", "pdf_url": "https://arxiv.org/pdf/2410.18528v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-24T08:21:51Z", "updated": "2024-10-24T08:21:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions", "authors": ["Qingbin Zeng", "Qinglong Yang", "Shunan Dong", "Heming Du", "Liang Zheng", "Fengli Xu", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2408.04168v3", "abstract": "This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions. This problem is very challenging, because it requires agent to establish self-position and acquire spatial representation of complex urban environment, where landmarks are often invisible. In the absence of navigation instructions, such abilities are vital for the agent to make high-quality decisions in long-range city navigation. With the emergent reasoning ability of large language models (LLMs), a tempting baseline is to prompt LLMs to \"react\" on each observation and make decisions accordingly. However, this baseline has very poor performance that the agent often repeatedly visits same locations and make short-sighted, inconsistent decisions. To address these issues, this paper introduces a novel agentic workflow featured by its abilities to perceive, reflect and plan. Specifically, we find LLaVA-7B can be fine-tuned to perceive the direction and distance of landmarks with sufficient accuracy for city navigation. Moreover, reflection is achieved through a memory mechanism, where past experiences are stored and can be retrieved with current perception for effective decision argumentation. Planning uses reflection results to produce long-term plans, which can avoid short-sighted decisions in long-range navigation. We show the designed workflow significantly improves navigation ability of the LLM agent compared with the state-of-the-art baselines.", "source": "arxiv", "arxiv_id": "2408.04168v3", "pdf_url": "https://arxiv.org/pdf/2408.04168v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-08T02:28:43Z", "updated": "2024-10-17T06:43:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security", "authors": ["Yuanchun Li", "Hao Wen", "Weijun Wang", "Xiangyu Li", "Yizhen Yuan", "Guohong Liu", "Jiacheng Liu", "Wenxing Xu", "Xiang Wang", "Yi Sun", "Rui Kong", "Yile Wang", "Hanfei Geng", "Jian Luan", "Xuefeng Jin", "Zilong Ye", "Guanjing Xiong", "Fan Zhang", "Xiang Li", "Mengwei Xu", "Zhijun Li", "Peng Li", "Yang Liu", "Ya-Qin Zhang", "Yunxin Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.05459v2", "abstract": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.", "source": "arxiv", "arxiv_id": "2401.05459v2", "pdf_url": "https://arxiv.org/pdf/2401.05459v2", "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-01-10T09:25:45Z", "updated": "2024-05-08T06:16:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Personalizing Prostate Cancer Education for Patients Using an EHR-Integrated LLM Agent", "authors": ["Yuexing Hao", "Jason Holmes", "Mark R. Waddle", "Brian J. Davis", "Nathan Y. Yu", "Kristin Vickers", "Heather Preston", "Drew Margolin", "Corinna E. Lockenhoff", "Aditya Vashistha", "Saleh Kalantari", "Marzyeh Ghassemi", "Wei Liu"], "year": 2024, "url": "http://arxiv.org/abs/2409.19100v2", "abstract": "Cancer patients often lack timely education and personalized support due to clinician workload. This quality improvement study develops and evaluates a Large Language Model (LLM) agent, MedEduChat, which is integrated with the clinic's electronic health records (EHR) and designed to enhance prostate cancer patient education. Fifteen non-metastatic prostate cancer patients and three clinicians recruited from the Mayo Clinic interacted with the agent between May 2024 and April 2025. Findings showed that MedEduChat has a high usability score (UMUX 83.7 out of 100) and improves patients' health confidence (Health Confidence Score rose from 9.9 to 13.9). Clinicians evaluated the patient-chat interaction history and rated MedEduChat as highly correct (2.9 out of 3), complete (2.7 out of 3), and safe (2.7 out of 3), with moderate personalization (2.3 out of 3). This study highlights the potential of LLM agents to improve patient engagement and health education.", "source": "arxiv", "arxiv_id": "2409.19100v2", "pdf_url": "https://arxiv.org/pdf/2409.19100v2", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "", "venue": "npj Digital Medicine 2025", "published": "2024-09-27T19:04:11Z", "updated": "2025-11-17T16:06:12Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Plancraft: an evaluation dataset for planning with LLM agents", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "year": 2024, "url": "http://arxiv.org/abs/2412.21033v2", "abstract": "We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as a handcrafted planner and Oracle Retriever, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and compare their performance and efficiency to a handcrafted planner. Overall, we find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and offer suggestions on how to improve their capabilities.", "source": "arxiv", "arxiv_id": "2412.21033v2", "pdf_url": "https://arxiv.org/pdf/2412.21033v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-30T15:58:41Z", "updated": "2025-07-15T09:27:28Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents", "authors": ["Haishuo Fang", "Xiaodan Zhu", "Iryna Gurevych"], "year": 2024, "url": "http://arxiv.org/abs/2407.11843v4", "abstract": "Deploying LLM-based agents in real-life applications often faces a critical challenge: the misalignment between agents' behavior and user intent. Such misalignment may lead agents to unintentionally execute critical actions that carry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web shopping), resulting in undesirable or even irreversible consequences. Although addressing these issues is crucial, the preemptive detection and correction of misaligned actions remains relatively underexplored. To fill this gap, we introduce InferAct, a novel approach that leverages the belief reasoning ability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions before execution. Once the misalignment is detected, InferAct alerts users for timely correction, preventing adverse outcomes and enhancing the reliability of LLM agents' decision-making processes. Experiments on three widely used tasks demonstrate that InferAct achieves up to 20% improvements on Marco-F1 against baselines in misaligned action detection. An in-depth evaluation of misalignment correction further highlights InferAct's effectiveness in improving agent alignment.", "source": "arxiv", "arxiv_id": "2407.11843v4", "pdf_url": "https://arxiv.org/pdf/2407.11843v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-16T15:24:44Z", "updated": "2025-09-30T13:12:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Privacy Leakage Overshadowed by Views of AI: A Study on Human Oversight of Privacy in Language Model Agent", "authors": ["Zhiping Zhang", "Bingcan Guo", "Tianshi Li"], "year": 2024, "url": "http://arxiv.org/abs/2411.01344v3", "abstract": "Language model (LM) agents that act on users' behalf for personal tasks (e.g., replying emails) can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey ($N=300$), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further identified six privacy behavior patterns reflecting varying concerns, trust levels, and privacy preferences underlying people's oversight of LM agents' actions. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.", "source": "arxiv", "arxiv_id": "2411.01344v3", "pdf_url": "https://arxiv.org/pdf/2411.01344v3", "categories": ["cs.HC", "cs.AI", "cs.CR"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-11-02T19:15:42Z", "updated": "2025-10-06T04:47:18Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance", "authors": ["Yaxi Lu", "Shenzhi Yang", "Cheng Qian", "Guirong Chen", "Qinyu Luo", "Yesai Wu", "Huadong Wang", "Xin Cong", "Zhong Zhang", "Yankai Lin", "Weiwen Liu", "Yasheng Wang", "Zhiyuan Liu", "Fangming Liu", "Maosong Sun"], "year": 2024, "url": "http://arxiv.org/abs/2410.12361v3", "abstract": "Agents powered by large language models have shown remarkable abilities in solving complex tasks. However, most agent systems remain reactive, limiting their effectiveness in scenarios requiring foresight and autonomous decision-making. In this paper, we tackle the challenge of developing proactive agents capable of anticipating and initiating tasks without explicit human instructions. We propose a novel data-driven approach for this problem. Firstly, we collect real-world human activities to generate proactive task predictions. These predictions are then labeled by human annotators as either accepted or rejected. The labeled data is used to train a reward model that simulates human judgment and serves as an automatic evaluator of the proactiveness of LLM agents. Building on this, we develop a comprehensive data generation pipeline to create a diverse dataset, ProactiveBench, containing 6,790 events. Finally, we demonstrate that fine-tuning models with the proposed ProactiveBench can significantly elicit the proactiveness of LLM agents. Experimental results show that our fine-tuned model achieves an F1-Score of 66.47% in proactively offering assistance, outperforming all open-source and close-source models. These results highlight the potential of our method in creating more proactive and effective agent systems, paving the way for future advancements in human-agent collaboration.", "source": "arxiv", "arxiv_id": "2410.12361v3", "pdf_url": "https://arxiv.org/pdf/2410.12361v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-16T08:24:09Z", "updated": "2024-12-03T04:34:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction", "authors": ["Sonny George", "Chris Sypherd", "Dylan Cashman"], "year": 2024, "url": "http://arxiv.org/abs/2411.12828v1", "abstract": "Large language model (LLM) agents show promise in an increasing number of domains. In many proposed applications, it is expected that the agent reasons over accumulated experience presented in an input prompt. We propose the OEDD (Operationalize Experience Despite Distraction) corpus, a human-annotator-validated body of scenarios with pre-scripted agent histories where the agent must make a decision based on disparate experiential information in the presence of a distractor. We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal chain-of-thought prompting strategy and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions. Our code and test corpus are publicly available at: https://github.com/sonnygeorge/OEDD .", "source": "arxiv", "arxiv_id": "2411.12828v1", "pdf_url": "https://arxiv.org/pdf/2411.12828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "Findings Assoc. Comput. Linguistics: EMNLP 2024 15447-15459 (2024)", "published": "2024-11-19T19:33:16Z", "updated": "2024-11-19T19:33:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents", "authors": ["Qisen Yang", "Zekun Wang", "Honghui Chen", "Shenzhi Wang", "Yifan Pu", "Xin Gao", "Wenhao Huang", "Shiji Song", "Gao Huang"], "year": 2024, "url": "http://arxiv.org/abs/2402.12326v2", "abstract": "Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.", "source": "arxiv", "arxiv_id": "2402.12326v2", "pdf_url": "https://arxiv.org/pdf/2402.12326v2", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-19T18:00:30Z", "updated": "2024-08-29T08:27:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "PyBench: Evaluating LLM Agent on various real-world coding tasks", "authors": ["Yaolun Zhang", "Yinxu Pan", "Yudong Wang", "Jie Cai"], "year": 2024, "url": "http://arxiv.org/abs/2407.16732v2", "abstract": "The LLM Agent, equipped with a code interpreter, is capable of automatically solving real-world coding tasks, such as data analysis and image editing.\n  However, existing benchmarks primarily focus on either simplistic tasks, such as completing a few lines of code, or on extremely complex and specific tasks at the repository level, neither of which are representative of various daily coding tasks.\n  To address this gap, we introduce \\textbf{PyBench}, a benchmark encompassing five main categories of real-world tasks, covering more than 10 types of files. Given a high-level user query and related files, the LLM Agent needs to reason and execute Python code via a code interpreter for a few turns before making a formal response to fulfill the user's requirements. Successfully addressing tasks in PyBench demands a robust understanding of various Python packages, superior reasoning capabilities, and the ability to incorporate feedback from executed code. Our evaluations indicate that current open-source LLMs are struggling with these tasks. Hence, we conduct analysis and experiments on four kinds of datasets proving that comprehensive abilities are needed for PyBench. Our fine-tuned 8B size model: \\textbf{PyLlama3} achieves an exciting performance on PyBench which surpasses many 33B and 70B size models. Our Benchmark, Training Dataset, and Model are available at: {https://github.com/Mercury7353/PyBench}", "source": "arxiv", "arxiv_id": "2407.16732v2", "pdf_url": "https://arxiv.org/pdf/2407.16732v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-07-23T15:23:14Z", "updated": "2024-08-03T03:00:43Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "authors": ["Tongxin Yuan", "Zhiwei He", "Lingzhong Dong", "Yiming Wang", "Ruijie Zhao", "Tian Xia", "Lizhen Xu", "Binglin Zhou", "Fangqi Li", "Zhuosheng Zhang", "Rui Wang", "Gongshen Liu"], "year": 2024, "url": "http://arxiv.org/abs/2401.10019v3", "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on the harmlessness of LLM-generated content in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 569 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 5 application categories and 10 risk types. It is of high-quality curation with annotated safety labels and risk descriptions. Evaluation of 11 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models significantly exceed the random. Moreover, we reveal that risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for LLMs. With further experiments, we find that fine-tuning on safety judgment significantly improve model performance while straightforward prompting mechanisms fail. R-Judge is publicly available at https://github.com/Lordog/R-Judge.", "source": "arxiv", "arxiv_id": "2401.10019v3", "pdf_url": "https://arxiv.org/pdf/2401.10019v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-01-18T14:40:46Z", "updated": "2024-10-05T06:50:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents", "authors": ["Tomoyuki Kagaya", "Thong Jing Yuan", "Yuxuan Lou", "Jayashree Karlekar", "Sugiri Pranata", "Akira Kinose", "Koki Oguri", "Felix Wick", "Yang You"], "year": 2024, "url": "http://arxiv.org/abs/2402.03610v1", "abstract": "Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.", "source": "arxiv", "arxiv_id": "2402.03610v1", "pdf_url": "https://arxiv.org/pdf/2402.03610v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-02-06T00:53:27Z", "updated": "2024-02-06T00:53:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "year": 2024, "url": "http://arxiv.org/abs/2406.11132v2", "abstract": "In the past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and their capacity is further expanded into the so-called LLM agents when connected with external tools. In all domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering (APE) has become an important question for many researchers and users of LLMs. However, previous works in APE rely on a final checker to evaluate the performance of the given prompt -- a requirement that is hard to meet in the case of LLM agents, where intermediate feedback is easier to obtain, and the final evaluation could be expensive, inaccurate, or even missing. In this paper, we propose a novel method, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to optimize the step-by-step instructions in the prompts given to LLM agents, based on the chat history obtained from interactions and reflections with LLM agents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the prompt without the need for a final solution checker. We evaluate our approach on PDDL generation, TravelPlanner, and Meeting Planning to show that our method could generally improve performance for different reasoning tasks.", "source": "arxiv", "arxiv_id": "2406.11132v2", "pdf_url": "https://arxiv.org/pdf/2406.11132v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-17T01:23:11Z", "updated": "2025-02-13T21:38:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve", "authors": ["Yuxiao Qu", "Tianjun Zhang", "Naman Garg", "Aviral Kumar"], "year": 2024, "url": "http://arxiv.org/abs/2407.18219v2", "abstract": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.", "source": "arxiv", "arxiv_id": "2407.18219v2", "pdf_url": "https://arxiv.org/pdf/2407.18219v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-07-25T17:35:59Z", "updated": "2024-07-26T17:50:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reframe Anything: LLM Agent for Open World Video Reframing", "authors": ["Jiawang Cao", "Yongliang Wu", "Weiheng Chi", "Wenbo Zhu", "Ziyue Su", "Jay Wu"], "year": 2024, "url": "http://arxiv.org/abs/2403.06070v1", "abstract": "The proliferation of mobile devices and social media has revolutionized content dissemination, with short-form video becoming increasingly prevalent. This shift has introduced the challenge of video reframing to fit various screen aspect ratios, a process that highlights the most compelling parts of a video. Traditionally, video reframing is a manual, time-consuming task requiring professional expertise, which incurs high production costs. A potential solution is to adopt some machine learning models, such as video salient object detection, to automate the process. However, these methods often lack generalizability due to their reliance on specific training data. The advent of powerful large language models (LLMs) open new avenues for AI capabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a LLM-based agent that leverages visual foundation models and human instructions to restructure visual content for video reframing. RAVA operates in three stages: perception, where it interprets user instructions and video content; planning, where it determines aspect ratios and reframing strategies; and execution, where it invokes the editing tools to produce the final video. Our experiments validate the effectiveness of RAVA in video salient object detection and real-world reframing tasks, demonstrating its potential as a tool for AI-powered video editing.", "source": "arxiv", "arxiv_id": "2403.06070v1", "pdf_url": "https://arxiv.org/pdf/2403.06070v1", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-10T03:29:56Z", "updated": "2024-03-10T03:29:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting", "authors": ["Mohamed Salim Aissi", "Clement Romac", "Thomas Carta", "Sylvain Lamprier", "Pierre-Yves Oudeyer", "Olivier Sigaud", "Laure Soulier", "Nicolas Thome"], "year": 2024, "url": "http://arxiv.org/abs/2410.19920v3", "abstract": "Reinforcement learning (RL) is a promising approach for aligning large language models (LLMs) knowledge with sequential decision-making tasks. However, few studies have thoroughly investigated the impact on LLM agents capabilities of fine-tuning them with RL in a specific environment. In this paper, we propose a novel framework to analyze the sensitivity of LLMs to prompt formulations following RL training in a textual environment. Our findings reveal that the performance of LLMs degrades when faced with prompt formulations different from those used during the RL training phase. Besides, we analyze the source of this sensitivity by examining the model's internal representations and salient tokens. Finally, we propose to use a contrastive loss to mitigate this sensitivity and improve the robustness and generalization capabilities of LLMs.", "source": "arxiv", "arxiv_id": "2410.19920v3", "pdf_url": "https://arxiv.org/pdf/2410.19920v3", "categories": ["cs.LG"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-25T18:25:35Z", "updated": "2025-09-05T09:22:59Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models", "authors": ["Daniel Albert", "Stephan Billinger"], "year": 2024, "url": "http://arxiv.org/abs/2410.06932v1", "abstract": "In this study, we propose LLM agents as a novel approach in behavioral strategy research, complementing simulations and laboratory experiments to advance our understanding of cognitive processes in decision-making. Specifically, we reproduce a human laboratory experiment in behavioral strategy using large language model (LLM) generated agents and investigate how LLM agents compare to observed human behavior. Our results show that LLM agents effectively reproduce search behavior and decision-making comparable to humans. Extending our experiment, we analyze LLM agents' simulated \"thoughts,\" discovering that more forward-looking thoughts correlate with favoring exploitation over exploration to maximize wealth. We show how this new approach can be leveraged in behavioral strategy research and address limitations.", "source": "arxiv", "arxiv_id": "2410.06932v1", "pdf_url": "https://arxiv.org/pdf/2410.06932v1", "categories": ["econ.GN", "cs.AI"], "primary_category": "econ.GN", "doi": "", "venue": "", "published": "2024-10-09T14:26:20Z", "updated": "2024-10-09T14:26:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling", "authors": ["Loris Gaven", "Clement Romac", "Thomas Carta", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "year": 2024, "url": "http://arxiv.org/abs/2410.12481v1", "abstract": "The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.", "source": "arxiv", "arxiv_id": "2410.12481v1", "pdf_url": "https://arxiv.org/pdf/2410.12481v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2024-10-16T11:59:27Z", "updated": "2024-10-16T11:59:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SAUP: Situation Awareness Uncertainty Propagation on LLM Agent", "authors": ["Qiwei Zhao", "Xujiang Zhao", "Yanchi Liu", "Wei Cheng", "Yiyou Sun", "Mika Oishi", "Takao Osaki", "Katsushi Matsuda", "Huaxiu Yao", "Haifeng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.01033v1", "abstract": "Large language models (LLMs) integrated into multistep agent systems enable complex decision-making processes across various applications. However, their outputs often lack reliability, making uncertainty estimation crucial. Existing uncertainty estimation methods primarily focus on final-step outputs, which fail to account for cumulative uncertainty over the multistep decision-making process and the dynamic interactions between agents and their environments. To address these limitations, we propose SAUP (Situation Awareness Uncertainty Propagation), a novel framework that propagates uncertainty through each step of an LLM-based agent's reasoning process. SAUP incorporates situational awareness by assigning situational weights to each step's uncertainty during the propagation. Our method, compatible with various one-step uncertainty estimation techniques, provides a comprehensive and accurate uncertainty measure. Extensive experiments on benchmark datasets demonstrate that SAUP significantly outperforms existing state-of-the-art methods, achieving up to 20% improvement in AUROC.", "source": "arxiv", "arxiv_id": "2412.01033v1", "pdf_url": "https://arxiv.org/pdf/2412.01033v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-02T01:31:13Z", "updated": "2024-12-02T01:31:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning", "authors": ["Yizhou Chi", "Yizhang Lin", "Sirui Hong", "Duyi Pan", "Yaying Fei", "Guanghao Mei", "Bangbang Liu", "Tianqi Pang", "Jacky Kwok", "Ceyao Zhang", "Bang Liu", "Chenglin Wu"], "year": 2024, "url": "http://arxiv.org/abs/2410.17238v1", "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-Search Enhanced LLM Agents (SELA), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space. This novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges.", "source": "arxiv", "arxiv_id": "2410.17238v1", "pdf_url": "https://arxiv.org/pdf/2410.17238v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-22T17:56:08Z", "updated": "2024-10-22T17:56:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents", "authors": ["Dawei Li", "Zhen Tan", "Peijia Qian", "Yifan Li", "Kumar Satvik Chaudhary", "Lijie Hu", "Jiayi Shen"], "year": 2024, "url": "http://arxiv.org/abs/2411.03284v1", "abstract": "While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. Code and data will be available at: https://github.com/David-Li0406/SMoA.", "source": "arxiv", "arxiv_id": "2411.03284v1", "pdf_url": "https://arxiv.org/pdf/2411.03284v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-11-05T17:33:39Z", "updated": "2024-11-05T17:33:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents", "authors": ["Feng Lin", "Dong Jae Kim", "Tse-Husn", "Chen"], "year": 2024, "url": "http://arxiv.org/abs/2403.15852v2", "abstract": "Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspired by these software engineering practices, we present FlowGen - a code generation framework that emulates software process models based on multiple Large Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall, FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e., requirement engineer, architect, developer, tester, and scrum master) that correspond to everyday development activities and organize their communication patterns. The agents work collaboratively using chain-of-thought and prompt composition with continuous self-refinement to improve the code quality. We use GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion) to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Our findings show that FlowGenScrum excels compared to other process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in statistically significant improvements, achieving the highest Pass@1 scores. Our analysis also reveals that the development activities impacted code smell and exception handling differently, with design and code review adding more exception handling and reducing code smells. Finally, FlowGen models maintain stable Pass@1 scores across GPT3.5 versions and temperature values, highlighting the effectiveness of software process models in enhancing the quality and stability of LLM-generated code.", "source": "arxiv", "arxiv_id": "2403.15852v2", "pdf_url": "https://arxiv.org/pdf/2403.15852v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-03-23T14:04:48Z", "updated": "2024-10-31T14:43:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "STEER: Assessing the Economic Rationality of Large Language Models", "authors": ["Narun Raman", "Taylor Lundy", "Samuel Amouyal", "Yoav Levine", "Kevin Leyton-Brown", "Moshe Tennenholtz"], "year": 2024, "url": "http://arxiv.org/abs/2402.09552v2", "abstract": "There is increasing interest in using LLMs as decision-making \"agents.\" Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"STEER report card.\" Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior.", "source": "arxiv", "arxiv_id": "2402.09552v2", "pdf_url": "https://arxiv.org/pdf/2402.09552v2", "categories": ["cs.CL", "econ.GN"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-14T20:05:26Z", "updated": "2024-05-28T16:27:56Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making", "authors": ["Chuanhao Li", "Runhan Yang", "Tiankai Li", "Milad Bafarassat", "Kourosh Sharifi", "Dirk Bergemann", "Zhuoran Yang"], "year": 2024, "url": "http://arxiv.org/abs/2405.16376v2", "abstract": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.", "source": "arxiv", "arxiv_id": "2405.16376v2", "pdf_url": "https://arxiv.org/pdf/2405.16376v2", "categories": ["cs.CL", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-05-25T23:25:10Z", "updated": "2024-05-28T01:21:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality", "authors": ["Yiwen Xu", "Qinyang Hou", "Hongyu Wan", "Mirjana Prpa"], "year": 2024, "url": "http://arxiv.org/abs/2409.15623v1", "abstract": "In this paper, we present Safe Guard, an LLM-agent for the detection of hate speech in voice-based interactions in social VR (VRChat). Our system leverages Open AI GPT and audio feature extraction for real-time voice interactions. We contribute a system design and evaluation of the system that demonstrates the capability of our approach in detecting hate speech, and reducing false positives compared to currently available approaches. Our results indicate the potential of LLM-based agents in creating safer virtual environments and set the groundwork for further advancements in LLM-driven moderation approaches.", "source": "arxiv", "arxiv_id": "2409.15623v1", "pdf_url": "https://arxiv.org/pdf/2409.15623v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS", "doi": "", "venue": "", "published": "2024-09-23T23:54:45Z", "updated": "2024-09-23T23:54:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents", "authors": ["Sheng Yin", "Xianghe Pang", "Yuanzhuo Ding", "Menglan Chen", "Yutong Bi", "Yichen Xiong", "Wenhao Huang", "Zhen Xiang", "Jing Shao", "Siheng Chen"], "year": 2024, "url": "http://arxiv.org/abs/2412.13178v5", "abstract": "With the integration of large language models (LLMs), embodied agents have strong capabilities to understand and plan complicated natural language instructions. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in the real world. Existing benchmarks predominantly overlook critical safety risks, focusing solely on planning performance, while a few evaluate LLMs' safety awareness only on non-interactive image-text data. To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 9 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that, although agents based on different design frameworks exhibit substantial differences in task success rates, their overall safety awareness remains weak. The most safety-conscious baseline achieves only a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing the LLM driving the agent does not lead to notable improvements in safety awareness. Dataset and codes are available in https://github.com/shengyin1224/SafeAgentBench and https://huggingface.co/datasets/safeagentbench/SafeAgentBench.", "source": "arxiv", "arxiv_id": "2412.13178v5", "pdf_url": "https://arxiv.org/pdf/2412.13178v5", "categories": ["cs.CR", "cs.AI", "cs.RO"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-17T18:55:58Z", "updated": "2025-10-31T08:18:50Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code", "authors": ["Ziniu Hu", "Ahmet Iscen", "Aashi Jain", "Thomas Kipf", "Yisong Yue", "David A. Ross", "Cordelia Schmid", "Alireza Fathi"], "year": 2024, "url": "http://arxiv.org/abs/2403.01248v1", "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.", "source": "arxiv", "arxiv_id": "2403.01248v1", "pdf_url": "https://arxiv.org/pdf/2403.01248v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2024-03-02T16:16:26Z", "updated": "2024-03-02T16:16:26Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent", "authors": ["Runliang Niu", "Jindong Li", "Shiqi Wang", "Yali Fu", "Xiyu Hu", "Xueyuan Leng", "He Kong", "Yi Chang", "Qi Wang"], "year": 2024, "url": "http://arxiv.org/abs/2402.07945v1", "abstract": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code is available at \\url{https://github.com/niuzaisheng/ScreenAgent}.", "source": "arxiv", "arxiv_id": "2402.07945v1", "pdf_url": "https://arxiv.org/pdf/2402.07945v1", "categories": ["cs.HC", "cs.AI", "cs.CV"], "primary_category": "cs.HC", "doi": "10.24963/ijcai.2024/711", "venue": "Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI 2024)", "published": "2024-02-09T02:33:45Z", "updated": "2024-02-09T02:33:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance", "authors": ["Matthew Renze", "Erhan Guven"], "year": 2024, "url": "http://arxiv.org/abs/2405.06682v3", "abstract": "In this study, we investigated the effects of self-reflection in large language models (LLMs) on problem-solving performance. We instructed nine popular LLMs to answer a series of multiple-choice questions to provide a performance baseline. For each incorrectly answered question, we instructed eight types of self-reflecting LLM agents to reflect on their mistakes and provide themselves with guidance to improve problem-solving. Then, using this guidance, each self-reflecting agent attempted to re-answer the same questions. Our results indicate that LLM agents are able to significantly improve their problem-solving performance through self-reflection ($p < 0.001$). In addition, we compared the various types of self-reflection to determine their individual contribution to performance. All code and data are available on GitHub at https://github.com/matthewrenze/self-reflection", "source": "arxiv", "arxiv_id": "2405.06682v3", "pdf_url": "https://arxiv.org/pdf/2405.06682v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "10.1109/FLLM63129.2024.10852493", "venue": "2nd International Conference on Foundation and Large Language Models (FLLM 2024), pp. 476-483", "published": "2024-05-05T18:56:46Z", "updated": "2024-10-16T23:19:46Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent", "authors": ["Weizhou Shen", "Chenliang Li", "Hongzhan Chen", "Ming Yan", "Xiaojun Quan", "Hehong Chen", "Ji Zhang", "Fei Huang"], "year": 2024, "url": "http://arxiv.org/abs/2401.07324v3", "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "source": "arxiv", "arxiv_id": "2401.07324v3", "pdf_url": "https://arxiv.org/pdf/2401.07324v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-14T16:17:07Z", "updated": "2024-02-16T12:42:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Soft Self-Consistency Improves Language Model Agents", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "year": 2024, "url": "http://arxiv.org/abs/2402.13212v2", "abstract": "Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (SOFT-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. SOFT-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, SOFT-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that SOFT-SC can be applied to both open-source and black-box models.", "source": "arxiv", "arxiv_id": "2402.13212v2", "pdf_url": "https://arxiv.org/pdf/2402.13212v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-02-20T18:22:38Z", "updated": "2024-06-05T19:50:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "SpecRover: Code Intent Extraction via LLMs", "authors": ["Haifeng Ruan", "Yuntong Zhang", "Abhik Roychoudhury"], "year": 2024, "url": "http://arxiv.org/abs/2408.02232v4", "abstract": "Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better \"signal\" to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.", "source": "arxiv", "arxiv_id": "2408.02232v4", "pdf_url": "https://arxiv.org/pdf/2408.02232v4", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-08-05T04:53:01Z", "updated": "2024-12-11T11:18:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning", "authors": ["Hang Zhou", "Yehui Tang", "Haochen Qin", "Yujie Yang", "Renren Jin", "Deyi Xiong", "Kai Han", "Yunhe Wang"], "year": 2024, "url": "http://arxiv.org/abs/2411.14497v1", "abstract": "The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12% and notable gains in specific metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset.", "source": "arxiv", "arxiv_id": "2411.14497v1", "pdf_url": "https://arxiv.org/pdf/2411.14497v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-21T02:30:53Z", "updated": "2024-11-21T02:30:53Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking", "authors": ["Nikolai Rozanov", "Marek Rei"], "year": 2024, "url": "http://arxiv.org/abs/2410.02810v3", "abstract": "Large language models (LLMs) are increasingly used as autonomous agents, tackling tasks from robotics to web navigation. Their performance depends on the underlying base agent. Existing methods, however, struggle with long-context reasoning and goal adherence. We introduce StateAct, a novel and efficient base agent that enhances decision-making through (1) self-prompting, which reinforces task goals at every step, and (2) chain-of-states, an extension of chain-of-thought that tracks state information over time. StateAct outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30% on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also demonstrate that StateAct can be used as a drop-in replacement for ReAct with advanced LLM agent methods such as test-time scaling, yielding an additional 12% gain on Textcraft. By improving efficiency and long-range reasoning without requiring additional training or retrieval, StateAct provides a scalable foundation for LLM agents. We open source our code to support further research at https://github.com/ai-nikolai/stateact .", "source": "arxiv", "arxiv_id": "2410.02810v3", "pdf_url": "https://arxiv.org/pdf/2410.02810v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-09-21T05:54:35Z", "updated": "2025-04-08T06:37:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN", "authors": ["Zhilun Zhou", "Jingyang Fan", "Yu Liu", "Fengli Xu", "Depeng Jin", "Yong Li"], "year": 2024, "url": "http://arxiv.org/abs/2411.00028v2", "abstract": "The fast development of location-based social networks (LBSNs) has led to significant changes in society, resulting in popular studies of using LBSN data for socioeconomic prediction, e.g., regional population and commercial activity estimation. Existing studies design various graphs to model heterogeneous LBSN data, and further apply graph representation learning methods for socioeconomic prediction. However, these approaches heavily rely on heuristic ideas and expertise to extract task-relevant knowledge from diverse data, which may not be optimal for specific tasks. Additionally, they tend to overlook the inherent relationships between different indicators, limiting the prediction accuracy. Motivated by the remarkable abilities of large language models (LLMs) in commonsense reasoning, embedding, and multi-agent collaboration, in this work, we synergize LLM agents and knowledge graph for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to integrate multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to identify relevant meta-paths in the LBKG for each type of socioeconomic prediction task, and design a semantic-guided attention module for knowledge fusion with meta-paths. Moreover, we introduce a cross-task communication mechanism to further enhance performance by enabling knowledge sharing across tasks at both LLM agent and KG levels. On the one hand, the LLM agents for different tasks collaborate to generate more diverse and comprehensive meta-paths. On the other hand, the embeddings from different tasks are adaptively merged for better socioeconomic prediction. Experiments on two datasets demonstrate the effectiveness of the synergistic design between LLM and KG, providing insights for information sharing across socioeconomic prediction tasks.", "source": "arxiv", "arxiv_id": "2411.00028v2", "pdf_url": "https://arxiv.org/pdf/2411.00028v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-10-29T04:03:15Z", "updated": "2024-11-19T14:29:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision", "authors": ["Ruiwen Zhou", "Yingxuan Yang", "Muning Wen", "Ying Wen", "Wenhao Wang", "Chunling Xi", "Guoqiang Xu", "Yong Yu", "Weinan Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2403.06221v1", "abstract": "Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.", "source": "arxiv", "arxiv_id": "2403.06221v1", "pdf_url": "https://arxiv.org/pdf/2403.06221v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-10T13:58:38Z", "updated": "2024-03-10T13:58:38Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TapeAgents: a Holistic Framework for Agent Development and Optimization", "authors": ["Dzmitry Bahdanau", "Nicolas Gontier", "Gabriel Huang", "Ehsan Kamalloo", "Rafael Pardinas", "Alex PichÃ©", "Torsten Scholak", "Oleh Shliazhko", "Jordan Prince Tremblay", "Karam Ghanem", "Soham Parikh", "Mitul Tiwari", "Quaizar Vohra"], "year": 2024, "url": "http://arxiv.org/abs/2412.08445v1", "abstract": "We present TapeAgents, an agent framework built around a granular, structured log tape of the agent session that also plays the role of the session's resumable state. In TapeAgents we leverage tapes to facilitate all stages of the LLM Agent development lifecycle. The agent reasons by processing the tape and the LLM output to produce new thought and action steps and append them to the tape. The environment then reacts to the agent's actions by likewise appending observation steps to the tape. By virtue of this tape-centred design, TapeAgents can provide AI practitioners with holistic end-to-end support. At the development stage, tapes facilitate session persistence, agent auditing, and step-by-step debugging. Post-deployment, one can reuse tapes for evaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from other agents or use revised historical tapes. In this report, we explain the TapeAgents design in detail. We demonstrate possible applications of TapeAgents with several concrete examples of building monolithic agents and multi-agent teams, of optimizing agent prompts and finetuning the agent's LLM. We present tooling prototypes and report a case study where we use TapeAgents to finetune a Llama-3.1-8B form-filling assistant to perform as well as GPT-4o while being orders of magnitude cheaper. Lastly, our comparative analysis shows that TapeAgents's advantages over prior frameworks stem from our novel design of the LLM agent as a resumable, modular state machine with a structured configuration, that generates granular, structured logs and that can transform these logs into training text -- a unique combination of features absent in previous work.", "source": "arxiv", "arxiv_id": "2412.08445v1", "pdf_url": "https://arxiv.org/pdf/2412.08445v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-12-11T15:09:54Z", "updated": "2024-12-11T15:09:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents", "authors": ["Jinyang Li", "Nan Huo", "Yan Gao", "Jiayi Shi", "Yingxiu Zhao", "Ge Qu", "Yurong Wu", "Chenhao Ma", "Jian-Guang Lou", "Reynold Cheng"], "year": 2024, "url": "http://arxiv.org/abs/2403.05307v1", "abstract": "Interactive Data Analysis, the collaboration between humans and LLM agents, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task. To mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts. We evaluate popular and advanced LLM agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis. Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides LLM agents to learn from successful history. Experiments demonstrate that Air can evolve LLMs into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%.", "source": "arxiv", "arxiv_id": "2403.05307v1", "pdf_url": "https://arxiv.org/pdf/2403.05307v1", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-03-08T13:34:20Z", "updated": "2024-03-08T13:34:20Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities", "authors": ["Yuxuan Zhu", "Antony Kellermann", "Akul Gupta", "Philip Li", "Richard Fang", "Rohan Bindu", "Daniel Kang"], "year": 2024, "url": "http://arxiv.org/abs/2406.01637v2", "abstract": "LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 14 real-world vulnerabilities and show that our team of agents improve over prior agent frameworks by up to 4.3X.", "source": "arxiv", "arxiv_id": "2406.01637v2", "pdf_url": "https://arxiv.org/pdf/2406.01637v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA", "doi": "", "venue": "", "published": "2024-06-02T16:25:26Z", "updated": "2025-03-30T00:26:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Testing Uncertainty of Large Language Models for Physics Knowledge and Reasoning", "authors": ["Elizaveta Reganova", "Peter Steinbach"], "year": 2024, "url": "http://arxiv.org/abs/2411.14465v1", "abstract": "Large Language Models (LLMs) have gained significant popularity in recent years for their ability to answer questions in various fields. However, these models have a tendency to \"hallucinate\" their responses, making it challenging to evaluate their performance. A major challenge is determining how to assess the certainty of a model's predictions and how it correlates with accuracy. In this work, we introduce an analysis for evaluating the performance of popular open-source LLMs, as well as gpt-3.5 Turbo, on multiple choice physics questionnaires. We focus on the relationship between answer accuracy and variability in topics related to physics. Our findings suggest that most models provide accurate replies in cases where they are certain, but this is by far not a general behavior. The relationship between accuracy and uncertainty exposes a broad horizontal bell-shaped distribution. We report how the asymmetry between accuracy and uncertainty intensifies as the questions demand more logical reasoning of the LLM agent, while the same relationship remains sharp for knowledge retrieval tasks.", "source": "arxiv", "arxiv_id": "2411.14465v1", "pdf_url": "https://arxiv.org/pdf/2411.14465v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-11-18T13:42:13Z", "updated": "2024-11-18T13:42:13Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Pingchuan Ma", "Zongjie Li", "Shuai Wang"], "year": 2024, "url": "http://arxiv.org/abs/2404.17833v1", "abstract": "Agents based on large language models (LLMs) have demonstrated effectiveness in solving a wide range of tasks by integrating LLMs with key modules such as planning, memory, and tool usage. Increasingly, customers are adopting LLM agents across a variety of commercial applications critical to reliability, including support for mental well-being, chemical synthesis, and software development. Nevertheless, our observations and daily use of LLM agents indicate that they are prone to making erroneous plans, especially when the tasks are complex and require long-term planning.\n  In this paper, we propose PDoctor, a novel and automated approach to testing LLM agents and understanding their erroneous planning. As the first work in this direction, we formulate the detection of erroneous planning as a constraint satisfiability problem: an LLM agent's plan is considered erroneous if its execution violates the constraints derived from the user inputs. To this end, PDoctor first defines a domain-specific language (DSL) for user queries and synthesizes varying inputs with the assistance of the Z3 constraint solver. These synthesized inputs are natural language paragraphs that specify the requirements for completing a series of tasks. Then, PDoctor derives constraints from these requirements to form a testing oracle. We evaluate PDoctor with three mainstream agent frameworks and two powerful LLMs (GPT-3.5 and GPT-4). The results show that PDoctor can effectively detect diverse errors in agent planning and provide insights and error characteristics that are valuable to both agent developers and users. We conclude by discussing potential alternative designs and directions to extend PDoctor.", "source": "arxiv", "arxiv_id": "2404.17833v1", "pdf_url": "https://arxiv.org/pdf/2404.17833v1", "categories": ["cs.AI", "cs.PL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-27T08:56:45Z", "updated": "2024-04-27T08:56:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Drama Machine: Simulating Character Development with LLM Agents", "authors": ["Liam Magee", "Vanicka Arora", "Gus Gollings", "Norma Lam-Saw"], "year": 2024, "url": "http://arxiv.org/abs/2408.01725v2", "abstract": "This paper explores use of multiple large language model (LLM) agents to simulate complex, dynamic characters in dramatic scenarios. We introduce a drama machine framework that coordinates interactions between LLM agents playing different 'Ego' and 'Superego' psychological roles. In roleplay simulations, this design allows intersubjective dialogue and intra-subjective internal monologue to develop in parallel. We apply this framework to two dramatic scenarios - an interview and a detective story - and compare character development with and without the Superego's influence. Though exploratory, results suggest this multi-agent approach can produce more nuanced, adaptive narratives that evolve over a sequence of dialogical turns. We discuss different modalities of LLM-based roleplay and character development, along with what this might mean for conceptualization of AI subjectivity. The paper concludes by considering how this approach opens possibilities for thinking of the roles of internal conflict and social performativity in AI-based simulation.", "source": "arxiv", "arxiv_id": "2408.01725v2", "pdf_url": "https://arxiv.org/pdf/2408.01725v2", "categories": ["cs.CY"], "primary_category": "cs.CY", "doi": "", "venue": "", "published": "2024-08-03T09:40:26Z", "updated": "2024-08-31T04:27:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies", "authors": ["Feng He", "Tianqing Zhu", "Dayong Ye", "Bo Liu", "Wanlei Zhou", "Philip S. Yu"], "year": 2024, "url": "http://arxiv.org/abs/2407.19354v2", "abstract": "Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.", "source": "arxiv", "arxiv_id": "2407.19354v2", "pdf_url": "https://arxiv.org/pdf/2407.19354v2", "categories": ["cs.CR"], "primary_category": "cs.CR", "doi": "10.1145/3773080", "venue": "", "published": "2024-07-28T00:26:24Z", "updated": "2025-11-02T06:38:40Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Future of Scientific Publishing: Automated Article Generation", "authors": ["Jeremy R. Harper"], "year": 2024, "url": "http://arxiv.org/abs/2404.17586v1", "abstract": "This study introduces a novel software tool leveraging large language model (LLM) prompts, designed to automate the generation of academic articles from Python code a significant advancement in the fields of biomedical informatics and computer science. Selected for its widespread adoption and analytical versatility, Python served as a foundational proof of concept; however, the underlying methodology and framework exhibit adaptability across various GitHub repo's underlining the tool's broad applicability (Harper 2024). By mitigating the traditionally time-intensive academic writing process, particularly in synthesizing complex datasets and coding outputs, this approach signifies a monumental leap towards streamlining research dissemination. The development was achieved without reliance on advanced language model agents, ensuring high fidelity in the automated generation of coherent and comprehensive academic content. This exploration not only validates the successful application and efficiency of the software but also projects how future integration of LLM agents which could amplify its capabilities, propelling towards a future where scientific findings are disseminated more swiftly and accessibly.", "source": "arxiv", "arxiv_id": "2404.17586v1", "pdf_url": "https://arxiv.org/pdf/2404.17586v1", "categories": ["cs.HC", "cs.AI", "cs.ET"], "primary_category": "cs.HC", "doi": "", "venue": "", "published": "2024-04-11T16:47:02Z", "updated": "2024-04-11T16:47:02Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Stepwise Deception: Simulating the Evolution from True News to Fake News with LLM Agents", "authors": ["Yuhan Liu", "Zirui Song", "Juntian Zhang", "Xiaoqing Zhang", "Xiuying Chen", "Rui Yan"], "year": 2024, "url": "http://arxiv.org/abs/2410.19064v2", "abstract": "With the growing spread of misinformation online, understanding how true news evolves into fake news has become crucial for early detection and prevention. However, previous research has often assumed fake news inherently exists rather than exploring its gradual formation. To address this gap, we propose FUSE (Fake news evolUtion Simulation framEwork), a novel Large Language Model (LLM)-based simulation approach explicitly focusing on fake news evolution from real news. Our framework model a social network with four distinct types of LLM agents commonly observed in daily interactions: spreaders who propagate information, commentators who provide interpretations, verifiers who fact-check, and bystanders who observe passively to simulate realistic daily interactions that progressively distort true news. To quantify these gradual distortions, we develop FUSE-EVAL, a comprehensive evaluation framework measuring truth deviation along multiple linguistic and semantic dimensions. Results show that FUSE effectively captures fake news evolution patterns and accurately reproduces known fake news, aligning closely with human evaluations. Experiments demonstrate that FUSE accurately reproduces known fake news evolution scenarios, aligns closely with human judgment, and highlights the importance of timely intervention at early stages. Our framework is extensible, enabling future research on broader scenarios of fake news.", "source": "arxiv", "arxiv_id": "2410.19064v2", "pdf_url": "https://arxiv.org/pdf/2410.19064v2", "categories": ["cs.SI", "cs.AI"], "primary_category": "cs.SI", "doi": "", "venue": "", "published": "2024-10-24T18:17:16Z", "updated": "2025-05-28T08:26:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents", "authors": ["Feiran Jia", "Tong Wu", "Xin Qin", "Anna Squicciarini"], "year": 2024, "url": "http://arxiv.org/abs/2412.16682v1", "abstract": "Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses based on rule constraints, source spotlighting, and authentication protocols show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07\\%) while maintaining high task utility (69.79\\%) on GPT-4o.", "source": "arxiv", "arxiv_id": "2412.16682v1", "pdf_url": "https://arxiv.org/pdf/2412.16682v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-12-21T16:17:48Z", "updated": "2024-12-21T16:17:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "authors": ["Frank F. Xu", "Yufan Song", "Boxuan Li", "Yuxuan Tang", "Kritanjali Jain", "Mengxue Bao", "Zora Z. Wang", "Xuhui Zhou", "Zhitong Guo", "Murong Cao", "Mingyang Yang", "Hao Yang Lu", "Amaad Martin", "Zhe Su", "Leander Maben", "Raj Mehta", "Wayne Chi", "Lawrence Jang", "Yiqing Xie", "Shuyan Zhou", "Graham Neubig"], "year": 2024, "url": "http://arxiv.org/abs/2412.14161v3", "abstract": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.", "source": "arxiv", "arxiv_id": "2412.14161v3", "pdf_url": "https://arxiv.org/pdf/2412.14161v3", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-12-18T18:55:40Z", "updated": "2025-09-10T08:35:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent", "authors": ["Shanbo Cheng", "Zhichao Huang", "Tom Ko", "Hang Li", "Ningxin Peng", "Lu Xu", "Qini Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2407.21646v2", "abstract": "In this paper, we present Cross Language Agent -- Simultaneous Interpretation, CLASI, a high-quality and human-like Simultaneous Speech Translation (SiST) System. Inspired by professional human interpreters, we utilize a novel data-driven read-write strategy to balance the translation quality and latency. To address the challenge of translating in-domain terminologies, CLASI employs a multi-modal retrieving module to obtain relevant information to augment the translation. Supported by LLMs, our approach can generate error-tolerated translation by considering the input audio, historical context, and retrieved information. Experimental results show that our system outperforms other systems by significant margins. Aligned with professional human interpreters, we evaluate CLASI with a better human evaluation metric, valid information proportion (VIP), which measures the amount of information that can be successfully conveyed to the listeners. In the real-world scenarios, where the speeches are often disfluent, informal, and unclear, CLASI achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese translation directions, respectively. In contrast, state-of-the-art commercial or open-source systems only achieve 35.4% and 41.6%. On the extremely hard dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70% VIP.", "source": "arxiv", "arxiv_id": "2407.21646v2", "pdf_url": "https://arxiv.org/pdf/2407.21646v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-07-31T14:48:27Z", "updated": "2024-08-30T06:50:51Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents", "authors": ["Agnieszka Mensfelt", "Kostas Stathis", "Vince Trencsenyi"], "year": 2024, "url": "http://arxiv.org/abs/2408.16081v2", "abstract": "Large language models (LLMs) are increasingly explored as general-purpose reasoners, particularly in agentic contexts. However, their outputs remain prone to mathematical and logical errors. This is especially challenging in open-ended tasks, where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies. To address this issue, we propose Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to enable validation and refinement of natural language reasoning. LELMA comprises three components: an LLM-Reasoner, an LLM-Translator, and a Solver, and employs autoformalization to translate reasoning into logic representations, which are then used to assess logical validity. Using game-theoretic scenarios such as the Prisoner's Dilemma as testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro) and advanced (GPT-4o) models in generating logically sound reasoning. LELMA achieves high accuracy in error detection and improves reasoning correctness via self-refinement, particularly in GPT-4o. The study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous open-ended reasoning tasks.", "source": "arxiv", "arxiv_id": "2408.16081v2", "pdf_url": "https://arxiv.org/pdf/2408.16081v2", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-08-28T18:25:35Z", "updated": "2025-05-29T14:53:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards Robust Automation of Surgical Systems via Digital Twin-based Scene Representations from Foundation Models", "authors": ["Hao Ding", "Lalithkumar Seenivasan", "Hongchao Shu", "Grayson Byrd", "Han Zhang", "Pu Xiao", "Juan Antonio Barragan", "Russell H. Taylor", "Peter Kazanzides", "Mathias Unberath"], "year": 2024, "url": "http://arxiv.org/abs/2409.13107v2", "abstract": "Large language model-based (LLM) agents are emerging as a powerful enabler of robust embodied intelligence due to their capability of planning complex action sequences. Sound planning ability is necessary for robust automation in many task domains, but especially in surgical automation. These agents rely on a highly detailed natural language representation of the scene. Thus, to leverage the emergent capabilities of LLM agents for surgical task planning, developing similarly powerful and robust perception algorithms is necessary to derive a detailed scene representation of the environment from visual input. Previous research has focused primarily on enabling LLM-based task planning while adopting simple yet severely limited perception solutions to meet the needs for bench-top experiments but lack the critical flexibility to scale to less constrained settings. In this work, we propose an alternate perception approach -- a digital twin-based machine perception approach that capitalizes on the convincing performance and out-of-the-box generalization of recent vision foundation models. Integrating our digital twin-based scene representation and LLM agent for planning with the dVRK platform, we develop an embodied intelligence system and evaluate its robustness in performing peg transfer and gauze retrieval tasks. Our approach shows strong task performance and generalizability to varied environment settings. Despite convincing performance, this work is merely a first step towards the integration of digital twin-based scene representations. Future studies are necessary for the realization of a comprehensive digital twin framework to improve the interpretability and generalizability of embodied intelligence in surgery.", "source": "arxiv", "arxiv_id": "2409.13107v2", "pdf_url": "https://arxiv.org/pdf/2409.13107v2", "categories": ["cs.RO"], "primary_category": "cs.RO", "doi": "", "venue": "", "published": "2024-09-19T22:24:46Z", "updated": "2024-09-24T15:08:03Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents", "authors": ["Peter MÃ¼hlbacher", "Nikos I. Bosse", "Lawrence Phillips"], "year": 2024, "url": "http://arxiv.org/abs/2409.14913v2", "abstract": "We present initial results of a forthcoming benchmark for evaluating LLM agents on white-collar tasks of economic value. We evaluate agents on real-world \"messy\" open-web research tasks of the type that are routine in finance and consulting. In doing so, we lay the groundwork for an LLM agent evaluation suite where good performance directly corresponds to a large economic and societal impact. We built and tested several agent architectures with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini. On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview substantially outperformed agents using GPT-4o, with agents based on Llama 3.1 (405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct architecture with the ability to delegate subtasks to subagents performed best. In addition to quantitative evaluations, we qualitatively assessed the performance of the LLM agents by inspecting their traces and reflecting on their observations. Our evaluation represents the first in-depth assessment of agents' abilities to conduct challenging, economically valuable analyst-style research on the real open web.", "source": "arxiv", "arxiv_id": "2409.14913v2", "pdf_url": "https://arxiv.org/pdf/2409.14913v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-09-23T11:08:04Z", "updated": "2024-09-25T08:52:49Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Transforming Wearable Data into Personal Health Insights using Large Language Model Agents", "authors": ["Mike A. Merrill", "Akshay Paruchuri", "Naghmeh Rezaei", "Geza Kovacs", "Javier Perez", "Yun Liu", "Erik Schenck", "Nova Hammerquist", "Jake Sunshine", "Shyam Tailor", "Kumar Ayush", "Hao-Wei Su", "Qian He", "Cory Y. McLean", "Mark Malhotra", "Shwetak Patel", "Jiening Zhan", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "year": 2024, "url": "http://arxiv.org/abs/2406.06464v4", "abstract": "Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.", "source": "arxiv", "arxiv_id": "2406.06464v4", "pdf_url": "https://arxiv.org/pdf/2406.06464v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-06-10T17:00:54Z", "updated": "2025-09-08T17:59:48Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Tree Search for Language Model Agents", "authors": ["Jing Yu Koh", "Stephen McAleer", "Daniel Fried", "Ruslan Salakhutdinov"], "year": 2024, "url": "http://arxiv.org/abs/2407.01476v3", "abstract": "Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at https://jykoh.com/search-agents.", "source": "arxiv", "arxiv_id": "2407.01476v3", "pdf_url": "https://arxiv.org/pdf/2407.01476v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-07-01T17:07:55Z", "updated": "2025-09-24T05:46:23Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents", "authors": ["Yifan Song", "Da Yin", "Xiang Yue", "Jie Huang", "Sujian Li", "Bill Yuchen Lin"], "year": 2024, "url": "http://arxiv.org/abs/2403.02502v2", "abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.", "source": "arxiv", "arxiv_id": "2403.02502v2", "pdf_url": "https://arxiv.org/pdf/2403.02502v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-03-04T21:50:29Z", "updated": "2024-07-10T17:36:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment", "authors": ["Mingzhe Xing", "Rongkai Zhang", "Hui Xue", "Qi Chen", "Fan Yang", "Zhen Xiao"], "year": 2024, "url": "http://arxiv.org/abs/2402.06596v1", "abstract": "Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.", "source": "arxiv", "arxiv_id": "2402.06596v1", "pdf_url": "https://arxiv.org/pdf/2402.06596v1", "categories": ["cs.AI", "cs.HC", "cs.SE"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-09T18:19:25Z", "updated": "2024-02-09T18:19:25Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Understanding the planning of LLM agents: A survey", "authors": ["Xu Huang", "Weiwen Liu", "Xiaolong Chen", "Xingmei Wang", "Hao Wang", "Defu Lian", "Yasheng Wang", "Ruiming Tang", "Enhong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2402.02716v1", "abstract": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.", "source": "arxiv", "arxiv_id": "2402.02716v1", "pdf_url": "https://arxiv.org/pdf/2402.02716v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-05T04:25:24Z", "updated": "2024-02-05T04:25:24Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction", "authors": ["Yansong Ning", "Hao Liu"], "year": 2024, "url": "http://arxiv.org/abs/2402.06861v2", "abstract": "Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.", "source": "arxiv", "arxiv_id": "2402.06861v2", "pdf_url": "https://arxiv.org/pdf/2402.06861v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-02-10T01:50:19Z", "updated": "2024-10-06T03:40:58Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Using a Feedback Loop for LLM-based Infrastructure as Code Generation", "authors": ["Mayur Amarnath Palavalli", "Mark Santolucito"], "year": 2024, "url": "http://arxiv.org/abs/2411.19043v1", "abstract": "Code generation with Large Language Models (LLMs) has helped to increase software developer productivity in coding tasks, but has yet to have significant impact on the tasks of software developers that surround this code. In particular, the challenge of infrastructure management remains an open question. We investigate the ability of an LLM agent to construct infrastructure using the Infrastructure as Code (IaC) paradigm. We particularly investigate the use of a feedback loop that returns errors and warnings on the generated IaC to allow the LLM agent to improve the code. We find that, for each iteration of the loop, its effectiveness decreases exponentially until it plateaus at a certain point and becomes ineffective.", "source": "arxiv", "arxiv_id": "2411.19043v1", "pdf_url": "https://arxiv.org/pdf/2411.19043v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "", "published": "2024-11-28T10:40:55Z", "updated": "2024-11-28T10:40:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "year": 2024, "url": "http://arxiv.org/abs/2410.07484v2", "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such \"world alignment\" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.", "source": "arxiv", "arxiv_id": "2410.07484v2", "pdf_url": "https://arxiv.org/pdf/2410.07484v2", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-10-09T23:37:36Z", "updated": "2024-10-11T23:32:09Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WESE: Weak Exploration to Strong Exploitation for LLM Agents", "authors": ["Xu Huang", "Weiwen Liu", "Xiaolong Chen", "Xingmei Wang", "Defu Lian", "Yasheng Wang", "Ruiming Tang", "Enhong Chen"], "year": 2024, "url": "http://arxiv.org/abs/2404.07456v1", "abstract": "Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.", "source": "arxiv", "arxiv_id": "2404.07456v1", "pdf_url": "https://arxiv.org/pdf/2404.07456v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-04-11T03:31:54Z", "updated": "2024-04-11T03:31:54Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "WaitGPT: Monitoring and Steering Conversational LLM Agent in Data Analysis with On-the-Fly Code Visualization", "authors": ["Liwenhan Xie", "Chengbo Zheng", "Haijun Xia", "Huamin Qu", "Chen Zhu-Tian"], "year": 2024, "url": "http://arxiv.org/abs/2408.01703v1", "abstract": "Large language models (LLMs) support data analysis through conversational user interfaces, as exemplified in OpenAI's ChatGPT (formally known as Advanced Data Analysis or Code Interpreter). Essentially, LLMs produce code for accomplishing diverse analysis tasks. However, presenting raw code can obscure the logic and hinder user verification. To empower users with enhanced comprehension and augmented control over analysis conducted by LLMs, we propose a novel approach to transform LLM-generated code into an interactive visual representation. In the approach, users are provided with a clear, step-by-step visualization of the LLM-generated code in real time, allowing them to understand, verify, and modify individual data operations in the analysis. Our design decisions are informed by a formative study (N=8) probing into user practice and challenges. We further developed a prototype named WaitGPT and conducted a user study (N=12) to evaluate its usability and effectiveness. The findings from the user study reveal that WaitGPT facilitates monitoring and steering of data analysis performed by LLMs, enabling participants to enhance error detection and increase their overall confidence in the results.", "source": "arxiv", "arxiv_id": "2408.01703v1", "pdf_url": "https://arxiv.org/pdf/2408.01703v1", "categories": ["cs.HC"], "primary_category": "cs.HC", "doi": "10.1145/3654777.3676374", "venue": "", "published": "2024-08-03T07:51:08Z", "updated": "2024-08-03T07:51:08Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement", "authors": ["Weimin Xiong", "Yifan Song", "Xiutian Zhao", "Wenhao Wu", "Xun Wang", "Ke Wang", "Cheng Li", "Wei Peng", "Sujian Li"], "year": 2024, "url": "http://arxiv.org/abs/2406.11176v2", "abstract": "Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.", "source": "arxiv", "arxiv_id": "2406.11176v2", "pdf_url": "https://arxiv.org/pdf/2406.11176v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2024-06-17T03:29:13Z", "updated": "2024-09-24T10:01:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Web Retrieval Agents for Evidence-Based Misinformation Detection", "authors": ["Jacob-Junqi Tian", "Hao Yu", "Yury Orlovskiy", "Tyler Vergho", "Mauricio Rivera", "Mayank Goel", "Zachary Yang", "Jean-Francois Godbout", "Reihaneh Rabbany", "Kellin Pelrine"], "year": 2024, "url": "http://arxiv.org/abs/2409.00009v2", "abstract": "This paper develops an agent-based automated fact-checking approach for detecting misinformation. We demonstrate that combining a powerful LLM agent, which does not have access to the internet for searches, with an online web search agent yields better results than when each tool is used independently. Our approach is robust across multiple models, outperforming alternatives and increasing the macro F1 of misinformation detection by as much as 20 percent compared to LLMs without search. We also conduct extensive analyses on the sources our system leverages and their biases, decisions in the construction of the system like the search tool and the knowledge base, the type of evidence needed and its impact on the results, and other parts of the overall process. By combining strong performance with in-depth understanding, we hope to provide building blocks for future search-enabled misinformation mitigation systems.", "source": "arxiv", "arxiv_id": "2409.00009v2", "pdf_url": "https://arxiv.org/pdf/2409.00009v2", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR", "doi": "", "venue": "", "published": "2024-08-15T15:13:16Z", "updated": "2024-10-09T19:13:41Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs", "authors": ["Hanna Kim", "Minkyoo Song", "Seung Ho Na", "Seungwon Shin", "Kimin Lee"], "year": 2024, "url": "http://arxiv.org/abs/2410.14569v3", "abstract": "Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyberattacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, generated impersonation posts where 93.9% of them were deemed authentic, and boosted click rate of phishing links in spear phishing emails by 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents.", "source": "arxiv", "arxiv_id": "2410.14569v3", "pdf_url": "https://arxiv.org/pdf/2410.14569v3", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR", "doi": "", "venue": "", "published": "2024-10-18T16:16:34Z", "updated": "2025-02-03T06:52:55Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment", "authors": ["Minrui Xu", "Dusit Niyato", "Jiawen Kang", "Zehui Xiong", "Shiwen Mao", "Zhu Han", "Dong In Kim", "Khaled B. Letaief"], "year": 2024, "url": "http://arxiv.org/abs/2401.07764v2", "abstract": "AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.", "source": "arxiv", "arxiv_id": "2401.07764v2", "pdf_url": "https://arxiv.org/pdf/2401.07764v2", "categories": ["cs.AI", "cs.NI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2024-01-15T15:20:59Z", "updated": "2024-02-16T19:15:31Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects", "authors": ["Islem Bouzenia", "Michael Pradel"], "year": 2024, "url": "http://arxiv.org/abs/2412.10133v2", "abstract": "The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.", "source": "arxiv", "arxiv_id": "2412.10133v2", "pdf_url": "https://arxiv.org/pdf/2412.10133v2", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE", "doi": "", "venue": "ISSTA 2025", "published": "2024-12-13T13:30:51Z", "updated": "2025-04-30T10:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents", "authors": ["Shaoguang Mao", "Yuzhe Cai", "Yan Xia", "Wenshan Wu", "Xun Wang", "Fengyi Wang", "Tao Ge", "Furu Wei"], "year": 2023, "url": "http://arxiv.org/abs/2311.03220v4", "abstract": "This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the \"Water Allocation Challenge,\" we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also highlight their potential in advancing game theory knowledge, thereby enriching our understanding of both game theory and empowering further research into strategic decision-making domains with LLM agents. Codes, prompts, and all related resources are available at https://github.com/microsoft/Alympics.", "source": "arxiv", "arxiv_id": "2311.03220v4", "pdf_url": "https://arxiv.org/pdf/2311.03220v4", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-11-06T16:03:46Z", "updated": "2024-01-16T07:12:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent", "authors": ["Ziniu Hu", "Ahmet Iscen", "Chen Sun", "Kai-Wei Chang", "Yizhou Sun", "David A Ross", "Cordelia Schmid", "Alireza Fathi"], "year": 2023, "url": "http://arxiv.org/abs/2306.08129v3", "abstract": "In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as \"What event is commemorated by the building depicted in this image?\", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-intensive visual question answering benchmarks such as Infoseek and OK-VQA.", "source": "arxiv", "arxiv_id": "2306.08129v3", "pdf_url": "https://arxiv.org/pdf/2306.08129v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV", "doi": "", "venue": "", "published": "2023-06-13T20:50:22Z", "updated": "2023-11-02T07:54:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "AdaPlanner: Adaptive Planning from Feedback with Language Models", "authors": ["Haotian Sun", "Yuchen Zhuang", "Lingkai Kong", "Bo Dai", "Chao Zhang"], "year": 2023, "url": "http://arxiv.org/abs/2305.16653v1", "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.", "source": "arxiv", "arxiv_id": "2305.16653v1", "pdf_url": "https://arxiv.org/pdf/2305.16653v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-05-26T05:52:27Z", "updated": "2023-05-26T05:52:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Adapting LLM Agents with Universal Feedback in Communication", "authors": ["Kuan Wang", "Yadong Lu", "Michael Santacroce", "Yeyun Gong", "Chao Zhang", "Yelong Shen"], "year": 2023, "url": "http://arxiv.org/abs/2310.01444v3", "abstract": "Recent advances in large language models (LLMs) have demonstrated potential for LLM agents. To facilitate the training for these agents with both linguistic feedback and non-linguistic reward signals, we introduce Learning through Communication (LTC). We design a universal buffer to store all the feedback, and an iterative pipeline to enable an LLM agent to explore and update its policy in an given environment. To optimize agent interactions for task-specific learning with our universal buffer and pipeline, we introduce diverse communication patterns tailored for both single-agent and multi-agent environments. We evaluate the efficacy of our LTC approach on four diverse datasets: ALFWorld (single-agent), HotpotQA (multi-agent collaboration), Chameleon (multi-agent competition), and GSM8k (multi-agent teacher-student). On these data sets, LTC outperforms the supervised instruction fine-tuning baselines by 3.6% to 12%. These results highlight the versatility and efficiency of LTC in facilitating online adaptation for LLM agents.", "source": "arxiv", "arxiv_id": "2310.01444v3", "pdf_url": "https://arxiv.org/pdf/2310.01444v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-01T07:50:30Z", "updated": "2024-04-14T03:47:19Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Agent-OM: Leveraging LLM Agents for Ontology Matching", "authors": ["Zhangcheng Qiang", "Weiqing Wang", "Kerry Taylor"], "year": 2023, "url": "http://arxiv.org/abs/2312.00326v24", "abstract": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.", "source": "arxiv", "arxiv_id": "2312.00326v24", "pdf_url": "https://arxiv.org/pdf/2312.00326v24", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-01T03:44:54Z", "updated": "2025-12-18T11:37:29Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Empowering Working Memory for Large Language Model Agents", "authors": ["Jing Guo", "Nan Li", "Jianchuan Qi", "Hang Yang", "Ruiqiao Li", "Yuzhen Feng", "Si Zhang", "Ming Xu"], "year": 2023, "url": "http://arxiv.org/abs/2312.17259v2", "abstract": "Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.", "source": "arxiv", "arxiv_id": "2312.17259v2", "pdf_url": "https://arxiv.org/pdf/2312.17259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-22T05:59:00Z", "updated": "2024-05-28T05:34:52Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks", "authors": ["Megan Kinniment", "Lucas Jun Koba Sato", "Haoxing Du", "Brian Goodrich", "Max Hasin", "Lawrence Chan", "Luke Harold Miles", "Tao R. Lin", "Hjalmar Wijk", "Joel Burget", "Aaron Ho", "Elizabeth Barnes", "Paul Christiano"], "year": 2023, "url": "http://arxiv.org/abs/2312.11671v2", "abstract": "In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as \"autonomous replication and adaptation\" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult.\n  We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.", "source": "arxiv", "arxiv_id": "2312.11671v2", "pdf_url": "https://arxiv.org/pdf/2312.11671v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-18T19:27:09Z", "updated": "2024-01-04T18:46:39Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View", "authors": ["Jintian Zhang", "Xin Xu", "Ningyu Zhang", "Ruibo Liu", "Bryan Hooi", "Shumin Deng"], "year": 2023, "url": "http://arxiv.org/abs/2310.02124v3", "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\\footnote{\\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.", "source": "arxiv", "arxiv_id": "2310.02124v3", "pdf_url": "https://arxiv.org/pdf/2310.02124v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-10-03T15:05:52Z", "updated": "2024-05-27T11:12:45Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web", "authors": ["Hiroki Furuta", "Yutaka Matsuo", "Aleksandra Faust", "Izzeddin Gur"], "year": 2023, "url": "http://arxiv.org/abs/2311.18751v3", "abstract": "Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise of small-scale finetuned and transferred models for task compositionality, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment.", "source": "arxiv", "arxiv_id": "2311.18751v3", "pdf_url": "https://arxiv.org/pdf/2311.18751v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG", "doi": "", "venue": "", "published": "2023-11-30T17:50:47Z", "updated": "2024-12-31T04:24:16Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "GestureGPT: Toward Zero-Shot Free-Form Hand Gesture Understanding with Large Language Model Agents", "authors": ["Xin Zeng", "Xiaoyu Wang", "Tengxiang Zhang", "Chun Yu", "Shengdong Zhao", "Yiqiang Chen"], "year": 2023, "url": "http://arxiv.org/abs/2310.12821v5", "abstract": "Existing gesture interfaces only work with a fixed set of gestures defined either by interface designers or by users themselves, which introduces learning or demonstration efforts that diminish their naturalness. Humans, on the other hand, understand free-form gestures by synthesizing the gesture, context, experience, and common sense. In this way, the user does not need to learn, demonstrate, or associate gestures. We introduce GestureGPT, a free-form hand gesture understanding framework that mimics human gesture understanding procedures to enable a natural free-form gestural interface. Our framework leverages multiple Large Language Model agents to manage and synthesize gesture and context information, then infers the interaction intent by associating the gesture with an interface function. More specifically, our triple-agent framework includes a Gesture Description Agent that automatically segments and formulates natural language descriptions of hand poses and movements based on hand landmark coordinates. The description is deciphered by a Gesture Inference Agent through self-reasoning and querying about the interaction context (e.g., interaction history, gaze data), which is managed by a Context Management Agent. Following iterative exchanges, the Gesture Inference Agent discerns the user's intent by grounding it to an interactive function. We validated our framework offline under two real-world scenarios: smart home control and online video streaming. The average zero-shot Top-1/Top-5 grounding accuracies are 44.79%/83.59% for smart home tasks and 37.50%/73.44% for video streaming tasks. We also provide an extensive discussion that includes rationale for model selection, generalizability, and future research directions for a practical system etc.", "source": "arxiv", "arxiv_id": "2310.12821v5", "pdf_url": "https://arxiv.org/pdf/2310.12821v5", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL", "doi": "10.1145/3698145", "venue": "Proc. ACM Hum.-Comput. Interact., Vol. 8, No. ISS, Article 545, 2024, 38 pages", "published": "2023-10-19T15:17:34Z", "updated": "2024-11-04T02:48:42Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "LASER: LLM Agent with State-Space Exploration for Web Navigation", "authors": ["Kaixin Ma", "Hongming Zhang", "Hongwei Wang", "Xiaoman Pan", "Wenhao Yu", "Dong Yu"], "year": 2023, "url": "http://arxiv.org/abs/2309.08172v2", "abstract": "Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible backtracking, allowing the model to recover from errors easily. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com. Experimental results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task.", "source": "arxiv", "arxiv_id": "2309.08172v2", "pdf_url": "https://arxiv.org/pdf/2309.08172v2", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-09-15T05:44:08Z", "updated": "2024-02-21T17:42:32Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning", "authors": ["Zhiting Hu", "Tianmin Shu"], "year": 2023, "url": "http://arxiv.org/abs/2312.05230v1", "abstract": "Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities. In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of Language models, Agent models, and World models, for more robust and versatile reasoning capabilities. In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning. Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability. We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework.", "source": "arxiv", "arxiv_id": "2312.05230v1", "pdf_url": "https://arxiv.org/pdf/2312.05230v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-08T18:25:22Z", "updated": "2023-12-08T18:25:22Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach", "authors": ["Weiyu Ma", "Qirui Mi", "Yongcheng Zeng", "Xue Yan", "Yuqiao Wu", "Runji Lin", "Haifeng Zhang", "Jun Wang"], "year": 2023, "url": "http://arxiv.org/abs/2312.11865v3", "abstract": "StarCraft II is a challenging benchmark for AI agents due to the necessity of both precise micro level operations and strategic macro awareness. Previous works, such as Alphastar and SCC, achieve impressive performance on tackling StarCraft II , however, still exhibit deficiencies in long term strategic planning and strategy interpretability. Emerging large language model (LLM) agents, such as Voyage and MetaGPT, presents the immense potential in solving intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs on StarCraft II, a highly complex RTS game.To conveniently take full advantage of LLMs` reasoning abilities, we first develop textual StratCraft II environment, called TextStarCraft II, which LLM agent can interact. Secondly, we propose a Chain of Summarization method, including single frame summarization for processing raw observations and multi frame summarization for analyzing game information, providing command recommendations, and generating strategic decisions. Our experiment consists of two parts: first, an evaluation by human experts, which includes assessing the LLMs`s mastery of StarCraft II knowledge and the performance of LLM agents in the game; second, the in game performance of LLM agents, encompassing aspects like win rate and the impact of Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the relevant knowledge and complex planning abilities needed to address StarCraft II scenarios; 2. Human experts consider the performance of LLM agents to be close to that of an average player who has played StarCraft II for eight years; 3. LLM agents are capable of defeating the built in AI at the Harder(Lv5) difficulty level. We have open sourced the code and released demo videos of LLM agent playing StarCraft II.", "source": "arxiv", "arxiv_id": "2312.11865v3", "pdf_url": "https://arxiv.org/pdf/2312.11865v3", "categories": ["cs.AI"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-12-19T05:27:16Z", "updated": "2024-06-18T03:07:37Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents", "authors": ["Yiran Wu", "Feiran Jia", "Shaokun Zhang", "Hangyu Li", "Erkang Zhu", "Yue Wang", "Yin Tat Lee", "Richard Peng", "Qingyun Wu", "Chi Wang"], "year": 2023, "url": "http://arxiv.org/abs/2306.01337v3", "abstract": "Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. LLMs, with their generalized ability, are used as a foundation model to build AI agents for different tasks. In this paper, we study the effectiveness of utilizing LLM agents to solve math problems through conversations. We propose MathChat, a conversational problem-solving framework designed for math problems. MathChat consists of an LLM agent and a user proxy agent which is responsible for tool execution and additional guidance. This synergy facilitates a collaborative problem-solving process, where the agents engage in a dialogue to solve the problems. We perform evaluation on difficult high school competition problems from the MATH dataset. Utilizing Python, we show that MathChat can further improve previous tool-using prompting methods by 6%.", "source": "arxiv", "arxiv_id": "2306.01337v3", "pdf_url": "https://arxiv.org/pdf/2306.01337v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-06-02T08:02:15Z", "updated": "2024-06-28T10:26:27Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent", "authors": ["Renat Aksitov", "Sobhan Miryoosefi", "Zonglin Li", "Daliang Li", "Sheila Babayan", "Kavya Kopparapu", "Zachary Fisher", "Ruiqi Guo", "Sushant Prakash", "Pranesh Srinivasan", "Manzil Zaheer", "Felix Yu", "Sanjiv Kumar"], "year": 2023, "url": "http://arxiv.org/abs/2312.10003v1", "abstract": "Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.", "source": "arxiv", "arxiv_id": "2312.10003v1", "pdf_url": "https://arxiv.org/pdf/2312.10003v1", "categories": ["cs.CL"], "primary_category": "cs.CL", "doi": "", "venue": "", "published": "2023-12-15T18:20:15Z", "updated": "2023-12-15T18:20:15Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
{"title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency", "authors": ["Zhihan Liu", "Hao Hu", "Shenao Zhang", "Hongyi Guo", "Shuqi Ke", "Boyi Liu", "Zhaoran Wang"], "year": 2023, "url": "http://arxiv.org/abs/2309.17382v3", "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call \"reason for future, act for now\" (\\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (\"reason for future\"). At each step, the LLM agent takes the initial action of the planned trajectory (\"act for now\"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.\n  The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an \"in-context\" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. Here, $T$ denotes the number of online interactions. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.", "source": "arxiv", "arxiv_id": "2309.17382v3", "pdf_url": "https://arxiv.org/pdf/2309.17382v3", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI", "doi": "", "venue": "", "published": "2023-09-29T16:36:39Z", "updated": "2024-06-24T05:25:21Z", "provenance": [{"route": "arxiv_query:(all:\"agent survey latex\" OR all:\"LLM agent\" OR all:\"language model agent\" OR all:\"tool-using agent\")", "source": "arxiv", "source_path": "https://export.arxiv.org/api/query", "imported_at": "2026-01-25T04:53:25", "note": ""}]}
