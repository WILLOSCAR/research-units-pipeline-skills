LLM agents are increasingly deployed as closed-loop systems that must decide what to do next, call tools, recover from failures, and complete multi-step tasks under cost and latency constraints. Compared to pure prompting, agent performance depends on the *contract* between the model and its environment: what constitutes an action, what feedback is observed, and how errors are surfaced and handled. Early designs such as ReAct make this loop explicit by interleaving reasoning traces with tool-grounded actions [@Yao2022React], while tool-use training methods aim to expand the action space beyond natural language by teaching explicit tool calls [@Schick2023Toolformer]. Reflection-style loops further highlight that “capability” is not a single number in agentic settings: it can be improved (or destabilized) by how the system stores state, critiques itself, and selects actions over time [@Shinn2023Reflexion].

We adopt a systems-oriented definition: an LLM agent is an LLM embedded in a control loop that (i) maintains an internal state, (ii) selects actions from a defined action space (tool calls, code edits, web interactions, environment actions), and (iii) receives observations that influence subsequent decisions. This definition separates agent work from adjacent areas such as one-off tool invocation or retrieval-only augmentation, and it aligns with how recent surveys frame agentic LLM systems [@Plaat2025Agentic; @Van2025Survey]. Under this lens, the central question becomes less “which model is best?” and more “which loop and interface choices make behavior reliable, debuggable, and comparable under a stated protocol?”

This framing matters because agent results are frequently protocol-sensitive. Cost-aware evaluations show that planning depth, retries, and verification can move a system along a sharp success–cost frontier, so headline success rates can be misleading if budgets are not normalized [@Liu2025Costbench]. Similarly, evaluation suites that model realistic tool ecosystems emphasize that tool access, tool noise, and logging policies can dominate outcomes, which makes “agent progress” inseparable from benchmark design [@Kim2026Beyond; @Zhang2025Datascibench].

Accordingly, this survey emphasizes two objects that are often under-specified in agent papers: evaluation protocols and threat models. Protocol taxonomies make clear why comparisons drift across benchmarks and why success-only reporting is insufficient for synthesis [@Mohammadi2025Evaluation]. Security-oriented benchmarks and monitor red-teaming methods further show that tool use introduces new attack surfaces (prompt/tool injection, tool misuse, data exfiltration) and that defenses are best evaluated as end-to-end system properties rather than as model-only behaviors [@Fu2025Eval; @Zhang2025Security; @Kale2025Reliable].

Evidence policy: we primarily rely on arXiv metadata/abstracts and structured notes to build the taxonomy and evidence packs. This is sufficient to map design axes and identify protocol details, but some quantitative claims and threat-model specifics can remain unclear without fulltext verification. We therefore (i) treat benchmark/protocol descriptions as first-class evidence, (ii) prefer conservative phrasing when protocol details are missing, and (iii) avoid over-interpreting isolated numeric claims unless the evaluation setup is explicit [@Mohammadi2025Evaluation; @Fu2025Eval; @Kale2025Reliable].

The paper proceeds from interface contracts to system components, then to evaluation and risk. We first define agent-loop semantics and tool interfaces (Foundations & Interfaces), then examine core components that dominate behavior in practice—planning/reasoning loops and memory/retrieval mechanisms. Next, we review adaptation and coordination (self-improvement and multi-agent protocols). We conclude by discussing evaluation methodology and governance constraints that reshape what it means to deploy agents safely and reproducibly [@Kim2026Beyond; @Zhang2025Security].
