Agent systems introduce new security and governance problems because they can take actions with real side effects. The core tension is autonomy versus control: broader tool access and longer horizons can unlock capability, whereas they also increase the blast radius of mistakes and adversarial manipulation. Threat models therefore need to include not only prompt injection, but also tool injection, data exfiltration, permission escalation, and monitoring evasion [@Zhang2025Security; @Wang2025Adversarial].

A first comparison is between policy-only defenses and system-level defenses. Policy-only defenses attempt to constrain model behavior via prompts or learned refusal behavior, whereas system defenses constrain the action space via sandboxing, typed interfaces, and permission checks. System-level controls can reduce worst-case risk, but they also shift the evaluation target: success depends on whether the agent can still accomplish tasks under constrained tool access and logging requirements [@Kamath2025Enforcing; @Lichkovski2025Agent].

Empirical security evaluations demonstrate how severe protocol-level vulnerabilities can be. One security taxonomy work reports evaluating nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances [@Zhang2025Security]. Such studies highlight that agent safety is not only about the base model; it is about the full stack: interfaces, tool descriptions, runtime permissions, and monitoring assumptions.

In addition, several lines of work focus on monitoring and red-teaming as operational governance mechanisms. Monitor red-teaming frameworks vary what the monitor knows and how attackers behave to test whether monitoring can detect evasive strategies; in contrast, quitting/degenerate behaviors under adversarial pressure can reveal whether agents fail safely or fail catastrophically [@Kale2025Reliable; @Bonagiri2025Check]. Evaluations that report both attack success and task completion under matched budgets make it easier to compare defenses without overstating robustness [@Fu2025Eval].

Enforcement mechanisms can be grouped by where control is applied: before action (permissioning and interface typing), during action (sandboxing and runtime monitors), and after action (auditing, rollback, and incident response). Pre-action controls can prevent entire classes of tool misuse, whereas they may reduce capability if the action space becomes too constrained; runtime monitoring can preserve capability but depends on what the monitor can observe and how quickly it can intervene [@Kamath2025Enforcing; @Kale2025Reliable]. Post-hoc auditing is essential for governance but is limited when logging is incomplete or when tool side effects are irreversible, which motivates designing interfaces with observability as a first-class goal [@Lichkovski2025Agent].

Another recurring safety failure mode is degenerate autonomy, where agents quit early, loop on unproductive actions, or follow adversarial instructions that appear helpful. Evaluations that explicitly measure quitting behavior and monitor evasion make these modes visible and support more actionable mitigations; in contrast, success-only reporting can hide near-misses or unsafe trajectories that should matter for deployment decisions [@Bonagiri2025Check; @Fu2025Eval]. Finally, governance-oriented benchmarks that stress-test impossible or policy-violating requests can help calibrate refusal and escalation policies, but the evidence remains limited until threat models and permissions are specified consistently across studies [@Zhong2025Impossiblebench; @Fang2025Should].

Open questions remain about how to balance capability with enforceability. Hard constraints (strict permissioning) can prevent severe failures but may reduce usefulness, whereas softer constraints (monitoring and escalation) preserve capability but depend on detection quality and response latency. This suggests evaluating defenses under realistic adversaries and deployment constraints, including impossible requests and policy-violating tool calls, and reporting both task completion and safety outcomes under matched budgets [@Zhong2025Impossiblebench; @Kamath2025Enforcing; @Kale2025Reliable]. Another open issue is responsibility assignment: when an agent violates policy via a tool call, it is unclear whether blame lies in the model, the router, or the permission policy. Auditable logs and clear escalation pathways can reduce this ambiguity, but they require standardization of what is logged and how monitors are evaluated in the first place [@Kale2025Reliable; @Kamath2025Enforcing]. In practice, this requires joint design of policy constraints, system controls, and evaluation protocols. Until such artifacts are standardized, safety and governance claims will remain limited and hard to compare across tool ecosystems.

A practical threat model for agents should treat every interface surface as an injection channel: user prompts, retrieved content, and tool metadata. Work that bridges tool-use and security emphasizes that attacks can live in tool descriptions, schema fields, or tool outputs, and that defenses must assume the agent will faithfully execute what it believes is a valid tool call unless constrained by interface contracts and monitors [@Gasmi2025Bridging; @Zhang2025Security]. Input sanitization and isolation mechanisms are therefore not optional add-ons but core parts of the agent stack, especially when retrieval and tool catalogs are dynamic [@An2025Ipiguard; @Zhang2024Agent].

Several benchmarks also show that governance controls need to be evaluated under realistic interaction formats. RAS-Eval implements tools in multiple formats (JSON, LangGraph, MCP), making it possible to test whether a defense generalizes across interface standards rather than overfitting to a single tool wrapper [@Fu2025Eval; @Hadeliya2025When]. Similarly, enhancements to monitoring and tool-use safety often hinge on what signals the monitor can see (full tool I/O versus summaries), which is why monitor design is inseparable from logging policy and interface structure [@Kale2025Reliable; @Hahm2025Enhancing].

From a governance perspective, the relevant question is often not whether the model is safe, but what the escalation path is when the system is uncertain or under attack. Studies of refusal and policy compliance suggest that agents should be evaluated on impossible or policy-violating requests with explicit escalation policies, and that these evaluations should report both safety outcomes and the cost of intervention (delays, false positives) [@Zhong2025Impossiblebench; @Fang2025Should]. Systems that integrate runtime controls, audit logs, and rollback procedures can reduce operational risk, but they also create new points of failure if the controller is mis-specified or too expensive to run at scale [@Luo2025Agrail; @Wang2023Survey; @Zhou2025Reasoning].

Security also interacts with reasoning and planning: a system that is robust to prompt injection in isolation may still be vulnerable once the planner can be induced to call a dangerous tool, or once an attacker can manipulate intermediate state that the planner treats as trusted. Reasoning failures under adversarial constraints—overconfident tool calls, refusal bypass via decomposition, or exploitation of ambiguous policies—suggest that governance must be evaluated end-to-end, including how the planner selects actions under uncertainty and how monitors interpret partial evidence [@Zhou2025Reasoning; @Wang2025Adversarial]. Enforcement mechanisms that operate at the interface (typed schemas, permission checks) can reduce some classes of attacks, whereas they can also create incentives for agents to find loopholes in the allowed action space unless monitors and audits are aligned with the same policy [@Kamath2025Enforcing; @Lichkovski2025Agent]. Practical tool-use safety work argues for layered defenses: isolate untrusted inputs, constrain tool capabilities, and log enough context for incident response, because no single mitigation is sufficient across threat models [@Sha2025Agent; @Luo2025Agrail]. This reinforces a broader governance principle: what matters is not only preventing bad actions, but making uncertainty visible and recoverable through escalation paths, safe failure modes, and post-hoc accountability artifacts [@Fang2025Should; @Kale2025Reliable].

Finally, safety evaluation benefits from the same protocol discipline as capability evaluation: specify tools, budgets, and observability. Security benchmarks emphasize that many attacks succeed because the agent is allowed to call unsafe tools or because logs are insufficient for monitors to detect manipulation; monitoring and enforcement work likewise shows that defense effectiveness depends on what signals are visible and how quickly interventions can occur [@Zhang2025Security; @Kale2025Reliable]. As a result, comparisons between defenses are only meaningful when evaluated under matched tool access and threat models, as exemplified by end-to-end suites that include both task completion and attack outcomes [@Fu2025Eval].

Taken together, this suggests that governance should be evaluated as an operational pipeline: define the policy, enforce it at the interface, monitor at runtime, and audit afterwards. A defense that looks strong under one tool ecosystem may fail under another unless these layers are explicitly specified and tested [@Zhang2025Security].

A key limitation is that governance claims are often underspecified: papers may not state what permissions the agent had, what logs were retained, or what constitutes a policy violation. Without these details, it remains unclear whether a defense would work in a different tool ecosystem. For practitioners, the pragmatic approach is to treat governance as a contract: define the threat model, specify the defense surface (sandbox, monitoring, policy enforcement), and evaluate under realistic tool access and adversarial conditions before deployment [@Zhong2025Impossiblebench; @Fang2025Should].
