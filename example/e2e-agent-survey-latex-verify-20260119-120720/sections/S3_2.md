Tool interfaces determine how an agent’s intentions become executable actions, and orchestration policies determine how those actions compose over a long horizon. The central tension is that expressive interfaces (many tools, flexible schemas, implicit routing) can increase capability, whereas stricter interfaces (typed arguments, explicit permissions, structured logging) can improve control and verifiability. Because tool calls create real side effects, interface contracts also act as a security boundary: they shape what an agent *can* do and what failures can be detected or prevented [@Dong2025Bench; @Chen2025Agentguard].

A first comparison is between “free-form” tool use and schema-constrained tool use. Schema constraints reduce ambiguity in tool invocation and can support more reliable parsing and validation, but they also restrict the action space and can make generalization to new tools harder. Work on tool-use robustness and tool selection accuracy highlights that interface details—tool descriptions, argument formats, and error surfaces—often dominate downstream performance, especially when many tools overlap semantically [@Du2024Anytool; @Liu2025Toolscope]. In contrast, smaller or simplified interface designs can reduce routing complexity at the cost of expressivity, which shifts the burden to prompting and post-hoc validation [@Shen2024Small].

In addition, orchestration policies introduce additional degrees of freedom: how tools are selected, whether tool calls are retried, and how failures are handled. Evaluations that explicitly target end-to-end orchestration can make these decisions visible; for example, MSC-Bench targets multi-hop tool orchestration in an MCP ecosystem, emphasizing hierarchical tool contexts rather than isolated calls [@Dong2025Bench]. Yet protocol details remain critical: if tool availability or budgets differ across runs, reported gains may be difficult to compare.

Interface design also shapes what “memory” means in tool use. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark reports experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy [@Lumer2025Memtool]. This illustrates that interface/orchestration choices can be evaluated not only by success rates, but also by operational metrics (cost, latency, tool usage) that matter for real deployments.

Across the literature, an emerging pattern is that reliability improvements often come from coupling interface constraints with training or confidence mechanisms. For instance, reported tool-selection accuracy gains and confidence-aware routing can improve robustness when tools are noisy or partially overlapping, whereas prompt-only routing policies can be brittle under distribution shift [@Liu2025Toolscope; @Xuan2026Confidence]. In contrast, self-training or self-challenging approaches can improve tool use on multi-turn benchmarks, but the evidence can be limited when protocols and tool inventories vary across studies [@Zhou2025Self; @Mohammadi2025Evaluation].

A practical way to compare tool orchestration systems is to separate three layers: (i) the interface surface (schemas, error contracts, permissions), (ii) the routing policy (which tool, when, and with what arguments), and (iii) the runtime control policy (retries, fallback tools, and validation). Systems that formalize these layers can report intermediate metrics (tool selection accuracy, failure-recovery rates, or sandbox violation counts) that are otherwise hidden; in contrast, end-to-end task success alone can obscure whether improvements come from better reasoning, better routing heuristics, or more permissive tool access [@Liu2025Toolscope; @Dong2025Bench]. Work that explicitly targets tool-use robustness and interface generalization highlights that routing errors often stem from ambiguous tool descriptions or overlapping affordances, whereas schema-constrained interfaces shift the difficulty toward correct argument filling and validation [@Du2024Anytool; @Fu2024Imprompter; @Shen2024Small]. This decomposition also clarifies what ablations are needed: hold the tool catalog fixed while varying routing, or hold routing fixed while varying schema strictness.

Tool interfaces also define a security boundary. Permissioned tool surfaces and explicit runtime policies can prevent classes of misuse (e.g., disallowed parameters or unsafe side effects), whereas permissive interfaces shift the burden to downstream monitors and post-hoc auditing. Work that studies failures under realistic tool ecosystems highlights that observability (what is logged and validated) is as important as the schema itself, because many interface-level vulnerabilities are only detectable when tool I/O and decision rationales are retained [@Chen2025Agentguard; @Hao2026From; @Cheng2025Your; @Li2024Personal].

In practice, the tool catalog itself becomes a moving target. Tool descriptions can be ambiguous, incomplete, or inconsistent with tool behavior, creating specification gaps that force routing policies to learn undocumented conventions. Analyses of tool-use dissonances highlight that these gaps directly translate into tool selection and argument errors, especially when many tools overlap semantically [@Li2025Dissonances; @Liu2025Toolscope]. Systems that generate or curate tools automatically can widen coverage, but they also amplify the need for versioned schemas and validation because the catalog changes over time [@Jia2025Autotool; @Li2024Personal].

Multi-turn tool use makes these interface choices measurable. Confidence-aware routers can trade off exploration and commitment by deferring uncertain calls, whereas self-challenging or self-repair loops attempt to detect and correct tool errors after the fact [@Xuan2026Confidence; @Zhou2025Self]. However, these gains are hard to interpret if evaluations report only end success: interface health is better captured by intermediate metrics such as invalid-call rate, repair success, and tool-removal or tool-usage ratios in long interactions [@Lumer2025Memtool; @Fu2024Imprompter].

Orchestration benchmarks start to encode these intermediate observables. MSC-Bench, for instance, stresses multi-hop orchestration in an MCP ecosystem and makes hierarchical tool contexts explicit, pushing evaluations away from “single-call accuracy” toward end-to-end tool pipelines [@Dong2025Bench]. At the same time, agent-guard style defenses emphasize that interface contracts are also enforcement points: if tool I/O and routing decisions are not logged and checked, many failures become indistinguishable from adversarial manipulation or user error [@Chen2025Agentguard; @Hao2026From; @Cheng2025Your]. A useful synthesis is to treat interface design, routing policy, and runtime controls as co-designed layers; without that decomposition, cross-paper comparisons collapse into opaque “agent success” scores that hide where reliability is won or lost [@Mohammadi2025Evaluation].

Another under-discussed variable is the human-facing layer. Many deployed agents must interpret ambiguous natural-language requests, choose among tools whose descriptions are partial, and decide when to ask clarifying questions. Avatar and personal-assistant settings suggest that interface contracts extend beyond schemas: the system must decide how to summarize tool outputs, what to expose to users, and what to log for auditing [@Wu2024Avatar; @Li2024Personal]. This interacts with tool catalog evolution: AutoTool-style creation can add new capabilities quickly, whereas it also increases the need for schema versioning and regression tests because routing policies can silently change as tools are added or renamed [@Jia2025Autotool; @Liu2025Toolscope]. Tool-use dissonances further imply that evaluations should fix the tool catalog snapshot (descriptions and behavior) as part of the benchmark artifact; otherwise, improvements may reflect a cleaner tool description rather than a better agent policy [@Li2025Dissonances; @Dong2025Bench]. A pragmatic reporting move is to attach a compact “tool card” to results—schema, expected failure modes, and validation policy—so that interface comparisons remain interpretable as ecosystems evolve [@Mohammadi2025Evaluation; @Cheng2025Your].

Concretely, interface studies become more reusable when they isolate what is being held fixed. When a paper claims an orchestration improvement, readers need to know whether the tool catalog is fixed (same descriptions, same failure behaviors) or whether the catalog is being co-designed alongside the router. Benchmarks that ship the catalog and harness as part of the artifact support this kind of isolation, whereas ad hoc evaluations can conflate better routing with better tool curation [@Dong2025Bench; @Li2025Dissonances]. In practice, adding even a small ablation—same router with perturbed tool descriptions, or same catalog with a different schema strictness—can reveal whether robustness comes from the interface contract or from model-side heuristics [@Fu2024Imprompter; @Liu2025Toolscope].

Finally, interface benchmarks should version their tool descriptions and failure behaviors, because routing policies can overfit to surface forms. Treating tool catalogs as datasets—with changelogs and regression tests—would make orchestration research more cumulative rather than anecdotal [@Jia2025Autotool].

Additionally, treating the tool catalog as a versioned artifact reduces ambiguity; therefore, robustness claims should be paired with ablations that perturb tool descriptions and schema strictness. This suggests that a large fraction of orchestration failures are interface failures, not reasoning failures.

A key limitation is that the threat model is frequently implicit: papers may not specify what the agent is allowed to do, what inputs are adversarial, or how tool outputs can be manipulated. Without explicit sandboxing, permissioning, and observability assumptions, it remains unclear whether a proposed interface/orchestration policy is robust in real settings. As a result, interface comparisons should be read as protocol-scoped: strong claims should be treated as limited until reproduced under matched tool access and security constraints [@Hao2026From; @Cheng2025Your].
