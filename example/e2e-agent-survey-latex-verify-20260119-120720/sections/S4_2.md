Memory mechanisms determine what information an agent can reliably condition on across steps, and retrieval mechanisms determine how that information is selected from a potentially large corpus. The central tension is persistence versus freshness: retaining more context can help long-horizon tasks, whereas persistent state can introduce staleness, contamination, and evaluation leakage that make results harder to reproduce. Because memory participates in the control loop, memory design is an agent-level decision, not merely a retrieval subsystem [@Huang2025Retrieval; @Tawosi2025Meta].

A first comparison is between ephemeral scratchpads and persistent stores. Scratchpad-style memory is tightly coupled to a single episode and supports interpretability and debugging, whereas persistent memory can accumulate long-term traces that change agent behavior across episodes. In contrast, retrieval-centric designs emphasize selecting a small working set of relevant context at each step, which can improve efficiency but may be sensitive to indexing and query generation policies [@Zhang2024Large; @Verma2026Active].

Another comparison concerns the *source* of memory and the write policy. Some agents retrieve from static documents or curated knowledge bases; others retrieve from interaction logs, tool traces, or self-generated notes. These choices change what failure modes are likely: document retrieval can suffer from outdated or irrelevant content, whereas log-based memory can amplify early mistakes or introduce privacy and security concerns if writes are not constrained [@Shi2025Progent; @Yu2026Agentic]. In practice, evaluation protocols rarely standardize these memory sources, which makes cross-paper comparison fragile.

Quantitative results often bundle memory with broader agent design choices. For example, AgentSwift is evaluated across seven benchmarks spanning embodied, math, web, tool, and game domains and reports an average performance gain of 8.34% over existing automated agent search methods and manually designed agents [@Li2025Agentswift]. Such numbers are informative only when the protocol makes clear what memory/retrieval resources are available and how they interact with planning and tool orchestration.

A useful synthesis is to treat memory as a reliability mechanism: the question is not only “does it help?”, but “under what protocol assumptions does it remain trustworthy?” Retrieval from the web can improve coverage, whereas it can also increase the risk of prompt injection and unverifiable claims; persistent self-notes can stabilize behavior, whereas they can also lock in incorrect beliefs. Systems that explicitly measure memory efficiency and tool usage, rather than only end success, can make these trade-offs visible [@Lumer2025Memtool; @Tao2026Membox].

Memory also interacts with planning in a way that many evaluations do not disentangle. When retrieval is treated as free context, planners can appear stronger simply because they see more relevant information; whereas when retrieval has an explicit cost (latency, token budget, or tool-call limits), the agent must trade recall against efficiency and robustness. This makes write policies and index choices decision-relevant: persistent memories can stabilize behavior across episodes, but they can also accumulate stale or incorrect traces that are hard to detect without explicit validation [@Tawosi2025Meta; @Tao2026Membox]. Similarly, active retrieval and provenance-aware designs can reduce hallucinated grounding, whereas they may increase complexity and create new failure modes when the retrieved context is adversarial or mismatched to the tool protocol [@Verma2026Active; @Zhu2025Where; @Shi2025Progent].

Finally, memory is inseparable from safety and governance in deployed settings. Web retrieval and persistent notes can expose agents to retrieval injection and privacy risks, and these risks depend on what gets written, what gets retrieved, and how content is validated before action. A memory system that improves success in benign benchmarks may be risky under adversarial inputs unless the protocol includes sanitization, provenance, and permission checks. This reinforces the need to treat memory policies as part of the interface contract and to evaluate them under explicit threat models [@Tao2026Membox; @Zhu2025Where; @Zhang2025Large]. Even simple ablations (turning off long-term writes, freezing retrieval sources, or measuring sensitivity to stale documents) can clarify whether gains come from retrieval quality or from policy changes. These checks are still uncommon, which makes many reported memory improvements difficult to interpret across benchmarks [@Tawosi2025Meta]. This remains an open question for memory-centric agents evaluated under shifting web/tool distributions.

Agentic retrieval work emphasizes that retrieval is itself a policy: what to query, when to query, and how to validate what comes back. Active retrieval policies can improve usefulness under tight budgets, whereas they also introduce additional failure modes when the agent’s queries are mis-specified or when retrieved context is adversarial [@Verma2026Active; @Xu2025Agentic]. Task-oriented evaluations that stress long-horizon decision making make this explicit by measuring not just final success but the trajectory of retrieval and tool actions over time [@Ye2025Task; @Ye2025Taska].

For long-term memory, the key question becomes what is written and what is retrieved. Systems such as MemBox and MemTool motivate reporting memory-write events, retrieval frequency, and removal/retention behavior in addition to task success, because persistent stores can silently accumulate stale or sensitive content [@Tao2026Membox; @Lumer2025Memtool]. This also clarifies contamination and leakage checks: without logging memory writes and retrieval sources, it is difficult to distinguish genuine agent improvement from evaluation artifacts—especially in web- or code-based settings where corpora evolve over time [@Huang2025Retrieval; @Zhang2025Large].

Finally, memory design varies by domain. Code agents often benefit from structured summaries, dependency graphs, or repository-level indexes, whereas naive text dumps can overwhelm the context window and degrade tool-use reasoning. Meta-RAG style approaches and retrieval-centric code agents illustrate this spectrum: summarization and structured indexes can reduce token cost, but they can also mask provenance and make debugging harder when summaries drift from source [@Wu2025Meta; @Li2025Graphcodeagent]. In these settings, provenance-aware retrieval and adversarially robust memory policies become essential, because incorrect or injected context can directly alter downstream edits and tool calls [@Zhu2025Where; @Shi2025Progent; @Yu2026Agentic].

Memory also creates subtle evaluation leakage channels. When an agent writes persistent notes or caches tool outputs, later episodes may benefit from previously seen answers, inflating apparent generalization—especially on benchmarks with repeated templates or limited task diversity. Large-scale retrieval systems for agents therefore need contamination checks and provenance tracking: what document or trace supported a decision, and whether that support would be available in a clean evaluation setting [@Zhang2024Large; @Huang2025Retrieval]. Long-horizon memory boxes highlight that even benign optimizations (keeping successful plans, storing user preferences) can become liabilities if stale entries are retrieved without validation, or if memory is updated by untrusted inputs [@Tao2026Membox; @Zhu2025Where]. This is why memory evaluation should include drift and adversarial settings: freeze the corpus to test reproducibility, perturb documents to test robustness, and measure how often retrieval changes downstream tool actions [@Zhang2025Large; @Verma2026Active]. Operational metrics such as long-run completion and tool usage ratios can expose whether a memory system is accumulating shortcuts, whereas success-only scores can hide that the agent is simply caching answers [@Lumer2025Memtool; @Yu2026Agentic].

Similarly, memory contributions are most informative when they specify what part of the loop they improve: better retrieval queries, better indexing, or better write policies. Task-centric evaluations already encourage this decomposition by tracking long-horizon trajectories; a memory system that helps early steps but harms later steps (via staleness) should be visible in per-step analyses rather than being averaged away [@Ye2025Task; @Ye2025Taska]. In agent settings, it is also useful to report memory as a resource: how many tokens are retrieved, how often retrieval is invoked, and how the agent trades retrieval against direct tool calls [@Lumer2025Memtool; @Verma2026Active].

A simple but underused diagnostic is to replay the same task under frozen memory: disable writes, or freeze retrieval sources, and measure how sensitive success is to cached context. This kind of ablation helps distinguish retrieval quality from policy changes and makes long-term memory claims easier to interpret [@Tao2026Membox; @Huang2025Retrieval].

More generally, memory papers should report write/read provenance and publish replayable traces, because without them it is difficult to audit whether retrieval improves reasoning or merely caches answers [@Huang2025Retrieval].

This suggests that memory mechanisms should be evaluated under explicit drift and contamination tests, not only on static leaderboards.

As a result, memory results are often under-specified: papers may omit logging of memory writes, do not report contamination checks, or evaluate on benchmarks where retrieval artifacts are hard to detect. This makes it unclear whether improvements are due to better memory algorithms or to evaluation leakage. A pragmatic recommendation is to include memory-write transparency and protocol constraints (what can be written, when it is retrieved, and how it is validated) as part of the benchmark contract for agent evaluations [@Tawosi2025Meta; @Zhang2025Large].
