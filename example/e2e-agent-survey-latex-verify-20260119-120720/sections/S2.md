Our scope sits at the intersection of LLM reasoning methods, tool use, and interactive evaluation. On the reasoning side, agent loops are often built on structured deliberation patterns that extend beyond single-pass chain-of-thought. For example, ReAct combines reasoning with grounded action to make intermediate tool calls explicit [@Yao2022React], while tree/search-style approaches explore multiple candidates or trajectories before committing to an action sequence [@Yao2023Tree]. These methods motivate protocol-aware comparisons: changing the deliberation policy can change both performance and cost.

Planning-specific work sharpens this point by treating “better reasoning” as a budgeted systems choice. Cost-focused benchmarks measure success alongside queries/tokens and highlight that many planning gains come from spending more interaction budget (more branches, more verification, more retries) rather than from algorithmic improvements at a fixed budget [@Liu2025Costbench]. API-call oriented evaluation further stresses that agent performance is bounded by tool-call validity and repair behavior, not only by the quality of intermediate thoughts [@Mudur2025Feabench; @Nakano2025Guided].

On the tool-use side, training and interface work expands what counts as an action. Toolformer demonstrates that a model can self-supervise tool use by inserting tool calls into text and learning when and how to invoke external APIs [@Schick2023Toolformer]. More recent work focuses on tool selection, routing, and robustness under noisy tool outputs or tool availability changes, which becomes central once agents must orchestrate multiple tools over long horizons [@Du2024Anytool; @Liu2025Toolscope]. These lines of work inform an “interface contract” framing: a tool schema and orchestration policy jointly determine what failures are observable and what claims are verifiable.

Several surveys summarize the emerging landscape of agentic LLM systems and provide high-level taxonomies [@Plaat2025Agentic; @Van2025Survey]. We align with their definition of agents as looped systems, but we put more weight on two aspects that repeatedly determine whether synthesis is meaningful: (i) evaluation protocols (task suites, budgets, tool access, logging), and (ii) threat models for deployed agents. This yields an organization closer to a systems paper: interfaces and protocols first, followed by component-level design levers, then evaluation and risk.

Related threads also study how agents change over time and how multiple agents interact. Self-improvement methods revise prompts or policies using self-generated feedback, while multi-agent protocols use debate, aggregation, or role specialization as verification layers; both lines of work stress that results are sensitive to evaluator design, message budgets, and diversity assumptions [@Wu2025Evolver; @Chuang2025Debate]. We treat these mechanisms as part of the broader agent loop, because they affect not only outcomes but also failure modes and governance requirements.

Evaluation and safety are increasingly treated as their own research threads. Dedicated evaluation overviews propose taxonomies of objectives (capability, reliability, safety) and emphasize protocol heterogeneity as a core obstacle to synthesis [@Mohammadi2025Evaluation]. Security-oriented benchmarks and red-teaming methods show that agent loops introduce new attack surfaces (prompt/tool injection, tool misuse, data exfiltration) that are not well captured by classic NLP benchmarks [@Fu2025Eval; @Zhang2025Security; @Kale2025Reliable]. We therefore integrate evaluation and threat-model discussions throughout rather than treating them as a final afterthought.

Finally, adjacent work on retrieval augmentation and memory systems provides building blocks for agent state, but not all RAG systems are agents. We treat RAG as an agent component when it participates in a closed-loop policy (retrieval decisions affect downstream actions and the environment responds). This distinction matters for interpreting empirical results and for deciding when benchmarks truly measure agentic behavior rather than retrieval quality [@Huang2025Retrieval; @Lumer2025Memtool].
