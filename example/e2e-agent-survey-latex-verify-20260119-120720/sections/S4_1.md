Planning mechanisms decide how an agent selects action sequences under uncertainty, and reasoning-loop designs decide how deliberation interleaves with tool calls and observations. The practical tension is that deeper deliberation and explicit search can improve correctness on multi-step tasks, whereas it can also increase cost and latency and introduce new failure modes (e.g., overthinking, inconsistent intermediate states). Consequently, planning results are only interpretable when paired with evaluation constraints such as budget, tool access, and logging [@Yao2022React; @Liu2025Costbench]. The key point is that planning gains should be read as protocol-scoped budget trade-offs, a pattern emphasized in cost-aware evaluations [@Liu2025Costbench].

One design axis is the control-loop decomposition: planner/executor splits versus monolithic policies. Planner/executor designs can isolate long-horizon reasoning from low-level tool invocation, enabling verification and recovery policies; in contrast, monolithic designs can be simpler but may entangle reasoning errors with tool misuse. Empirical studies of planning-oriented agents often highlight that the same base model can behave very differently depending on how plans are represented and validated [@Shang2024Agentsquare; @Choi2025Reactree].

Moreover, another axis is search versus single-trajectory deliberation. Search-style methods explore multiple candidates or branches and can hedge against early commitment errors, whereas single-trajectory methods rely on strong priors encoded in prompts or learned policies. In practice, this trade-off is often a cost–reliability curve rather than a binary choice: more search can help, but only until budget constraints or tool costs dominate [@Mudur2025Feabench; @Liu2025Costbench].

Quantitative evidence underscores how planning choices interact with protocol constraints. For example, a state-of-the-art LLM penetration testing tool using self-guided reasoning reports completing only 13.5%, 16.5%, and 75.7% of subtasks and requiring 86.2%, 118.7%, and 205.9% more model queries across settings, illustrating that “stronger reasoning” can still be fragile and costly under realistic tool-use tasks [@Nakano2025Guided]. This kind of result is most informative when the task type, success metric, and query/cost model are made explicit.

Across papers, a robust synthesis is that planning mechanisms should be evaluated with *both* success and cost metrics. Benchmarks that only measure end success can mask pathological behaviors (excessive tool calls, unsafe retries), whereas cost-aware benchmarks make explicit the decision-relevant trade-off between reliability and efficiency [@Liu2025Costbench; @Silva2025Agents]. In contrast, some planning results remain limited when evaluation protocols do not specify tool failure models or when intermediate states are not logged, making replication and comparison unclear.

Cost-aware evaluation reveals an additional contrast between planning as search and planning as policy. Search-heavy planners can improve success under uncertainty, but they also amplify the risk of budget blow-ups when tool calls are expensive or when verification is imperfect; policy-like planners reduce overhead, whereas they may fail catastrophically when an early mistake commits the trajectory. Benchmarks that model budget explicitly make this trade-off measurable by reporting success together with query/token counts and latency proxies, enabling comparisons between algorithms that would look identical under success-only reporting [@Liu2025Costbench; @Mudur2025Feabench]. In practice, many agent papers report qualitative improvements without enough protocol detail to tell whether gains come from search depth, better heuristics for branching/pruning, or simply more generous budgets; this leaves an open question about how planning methods generalize once budgets are normalized [@Shang2024Agentsquare; @Choi2025Reactree].

Planning quality is often bottlenecked by verification. When tools are unreliable or observations are partial, planners that include explicit checks can avoid compounding errors, whereas unchecked deliberation can confidently pursue invalid trajectories. This makes ablations on verification and error-handling essential: a planner that appears stronger under idealized tools may fail under realistic noise models. Several planning-oriented evaluations therefore emphasize protocol specification (failure modes, retries, budget) as part of the planning method itself, not a downstream implementation choice [@Nakano2025Guided; @Hu2025Training; @Kim2025Bridging]. Reporting the full cost–success curve (rather than a single operating point) can expose when an approach is dominated across budgets, which is common once prompt and tool costs are significant. Without such curves, it remains unclear whether a method is practically useful or only competitive under a narrow, underreported budget setting [@Liu2025Costbench].

Recent planning work increasingly treats the agent as an architecture problem: how to decompose deliberation, execution, and verification into components with explicit responsibilities. Architectural overviews and planning taxonomies distinguish between policy-like planners (single-pass, learned heuristics) and deliberative planners (iterative planning with validation hooks), and argue that the choice should follow the observability and budget constraints of the environment [@Rosario2025Architecting; @Hatalis2025Review]. In contrast, “planning” framed purely as a prompting pattern can look strong on small tasks yet degrade under long-horizon tool use when intermediate states are not validated or cached [@Hong2025Planning].

Reasoning-centric benchmarks further show that the failure mode is often not a lack of ideas but a mismatch between search depth and verification. Methods that explore branches can reduce early commitment errors, whereas they can also amplify inconsistency when branches are not grounded in stable intermediate representations [@Choi2025Reactree; @Zhou2025Reasoning]. Novel planning objectives and learned planners can improve task planning under constrained compute, but their reported gains are most convincing when accompanied by ablations that separate planning quality from tool/interface changes [@Kiruluta2025Novel; @Hu2025Training].

A practical reporting upgrade is to treat planning as a cost–success curve rather than a single operating point. CostBench-style evaluations and API-call focused benchmarks already motivate this view by measuring success alongside queries/tokens, tool-call counts, and latency proxies [@Liu2025Costbench; @Mudur2025Feabench]. Extending that idea to planning requires exposing plan representations (tree depth, branch factor, pruning rules) and failure recovery statistics, so that a reader can tell whether improved success comes from better search, better verification, or simply larger budgets. Without these details, even strong results remain protocol-scoped and hard to transfer to deployments where tool access and budget constraints differ [@Nakano2025Guided; @Hong2025Planning].

Domain-specific agents highlight another confound: planners are often evaluated in environments where domain knowledge is embedded in the tool layer. Medical or enterprise agents can appear to “reason” well because tools encapsulate workflows; in contrast, open-ended planning benchmarks force the planner to construct the workflow from primitive actions, which shifts the error distribution from knowledge gaps to search and verification mistakes [@Shi2024Ehragent; @Shang2024Agentsquare]. This difference matters because planning loops frequently rely on intermediate representations—task graphs, action templates, or structured plans—that are not shared across papers, making it hard to distinguish algorithmic improvements from representation engineering [@Hong2025Planning; @Rosario2025Architecting]. Even when the same reasoning pattern is used, execution-time constraints can invert rankings: a method that is superior under generous budgets can be dominated under tight budgets once tool latency and verification overhead are accounted for [@Liu2025Costbench; @Mudur2025Feabench]. Therefore, planning results are most comparable when papers publish plan traces and branching statistics, tool-call logs with failure codes, and ablations that hold tool inventories fixed while varying planning depth and verification policies [@Hu2025Training; @Zhou2025Reasoning; @Wu2025Agents].

Because planning failures are often systematic (not random), reporting which subskills fail under budget is as important as reporting aggregate success. Reviews of planning-oriented agents emphasize recurring failure modes—hallucinated intermediate states, brittle recovery, and verification that does not match the task constraints—and argue that these should be surfaced in evaluation reports alongside the cost–success curve [@Hatalis2025Review; @Nakano2025Guided]. CostBench-style reporting provides a natural place to attach this diagnosis: for each budget regime, report what fraction of failures are due to wrong tool calls, wrong plan structure, or premature termination [@Liu2025Costbench].

Additionally, publishing plan traces enables failure analysis beyond single scores. This suggests that planning progress will be easier to compare once benchmarks standardize the cost model and the verification hooks that planners rely on.

A limitation for the field is that planning improvements are often confounded with interface and memory choices: a “better planner” may simply have access to a different tool set or better retrieval. Without protocol alignment and ablations, it remains unclear which gains reflect planning algorithms versus environment engineering. For practitioners, a practical takeaway is to treat planning as part of a system contract: choose a planning loop that fits your budget and observability constraints, and validate claims under matched tool access and logging settings [@Hu2025Training; @Kim2025Bridging].
