Evaluation is the bottleneck for evidence-backed progress in agent research: results are rarely comparable unless protocols align on tool access, budgets, observability/logging, and the definition of success. At the same time, deployed agents face new security and governance constraints that directly reshape both interfaces and evaluation objectives [@Mohammadi2025Evaluation; @Zhang2025Security].

We first survey benchmarks and evaluation protocols, emphasizing what they measure and what they leave ambiguous (leakage, reproducibility, cost accounting, threat models). We then synthesize safety, security, and governance work, treating threat models and defense surfaces (sandboxing, monitoring, policy enforcement) as first-class design axes rather than “post-hoc guardrails” [@Fu2025Eval; @Kale2025Reliable].
