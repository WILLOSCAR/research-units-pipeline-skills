Agent evaluation asks a deceptively simple question—did the system solve the task?—but in agentic settings the answer depends on protocol details: tool access, budgets, observability/logging, and what constitutes a valid action sequence. A key tension is realism versus comparability: more realistic environments (noisy tools, partial observability) can expose failure modes, whereas they also introduce confounds that make cross-paper synthesis difficult. This motivates evaluation taxonomies that explicitly separate objectives (capability, reliability, safety) from protocol instantiations [@Mohammadi2025Evaluation; @Plaat2025Agentic].

Benchmarks differ in what they operationalize. Some suites emphasize end-to-end task completion, while others stress robustness to attacks, tool misuse, or protocol deviations. As a result, “success rate” is not a universal metric: it must be interpreted alongside cost (token/query/tool usage), latency, and failure recovery behavior. Protocol-aware benchmarks and reviews therefore recommend reporting both outcome and resource usage, and making tool inventories and budgets explicit [@Kim2026Beyond; @Chen2025Towards].

Moreover, security and robustness benchmarks make the protocol contract concrete by specifying adversarial objectives and tool-level constraints. For example, RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats [@Fu2025Eval]. Such designs highlight that evaluation artifacts can (and should) encode interface formats and threat models, not only task descriptions.

Reliability work further emphasizes that evaluation should measure *behavior under monitoring* and *distribution shift*. Monitor red-teaming workflows vary agent and monitor situational awareness and model adversarial strategies (e.g., prompt injection) to test whether monitoring is effective under realistic assumptions [@Kale2025Reliable]. In contrast, benchmarks for context-aware proactive agents focus on whether the agent can act appropriately given evolving context and tool availability, which requires careful specification of what context is observable and when [@Yang2025Contextagent; @Mohammadi2025Evaluation].

From a measurement standpoint, three protocol dimensions repeatedly dominate variance: (i) tool access and reliability (idealized APIs vs noisy tools), (ii) resource budgets (tokens, queries, latency/cost), and (iii) observability/logging (whether intermediate tool I/O is retained and audited). Benchmarks that control these factors can attribute improvements to agent policies more credibly; whereas benchmarks that leave them implicit can conflate policy changes with protocol engineering [@Mohammadi2025Evaluation; @Kim2026Beyond]. This also explains why leaderboards alone are insufficient for survey synthesis: without protocol metadata, the same system can occupy different design points depending on deployment assumptions.

Reproducibility and leakage are additional evaluation axes that are especially acute for web- or tool-based tasks. When agents interact with changing external resources, results can drift over time, and hidden leakage channels (cached answers, retriever contamination, tool output changes) can inflate success. Dataset- and benchmark-building work therefore increasingly treats environment snapshots, tool mocks, and evaluation harnesses as part of the artifact, aiming to make agent evaluation closer to systems benchmarking rather than one-off demonstrations [@Zhang2025Datascibench; @Fu2025Eval]. In contrast, broad surveys note that many published agent results remain difficult to compare precisely because these artifacts are missing or not standardized [@Van2025Survey; @Plaat2025Agentic].

A practical implication for survey writing is to treat evaluation artifacts as comparable objects. When a paper reports only a benchmark name without describing tool access or budgets, the result is often unclear: the same benchmark can yield different outcomes depending on tool catalogs, prompt policies, and cost models. Evaluation surveys therefore recommend minimum reporting standards (task suite, metric definition, budget, tool access, logging) and encourage publishing harness code so that comparisons become reproducible rather than rhetorical [@Mohammadi2025Evaluation; @Yang2025Contextagent; @Kim2026Beyond]. Standardizing these reporting fields would also support better meta-analysis: surveys could group results by tool realism or budget class and avoid mixing incomparable settings. In the near term, even a lightweight protocol card accompanying each benchmark would make it easier to align evaluation objectives with deployment needs [@Kim2026Beyond]. This is especially important for interactive web and tool benchmarks whose environments evolve over time.

Several works argue that agent evaluation needs a shared vocabulary before it needs more leaderboards. Taxonomies of agent objectives and failure modes propose organizing benchmarks by the capability being measured (planning, tool use, robustness, safety) and by the protocol knobs (tool realism, budgets, observability), so that synthesis can compare like with like [@Ji2025Taxonomy; @Wang2023Survey]. This complements broader agent surveys that catalog systems but may not normalize protocols, and it motivates reporting templates that record the key protocol fields alongside results [@Plaat2025Agentic; @Mohammadi2025Evaluation].

Budget normalization is particularly important for agents because the dominant cost is often interaction, not inference alone. Evaluations increasingly track token/query counts, tool latency, and explicit cost models, and show that the same planner can move along a Pareto frontier depending on budget settings and tool availability [@Kim2026Beyond; @Liu2026Agents]. In contrast, benchmarks that ignore budgets can reward brittle strategies that spam tools or rely on expensive verification, which is unlikely to transfer to constrained deployments.

Benchmark construction is becoming a research contribution in its own right. BuildBench-style efforts treat benchmark design as engineering: define realistic tool stacks, specify failure models, and ship harness code so that results are reproducible over time [@Zhang2025Buildbench; @Hadeliya2025When]. For web and data-science tasks, snapshotting environments and auditing leakage channels are necessary to prevent silent drift; suites that standardize these artifacts make progress easier to interpret across years [@Zhang2025Datascibench; @Chen2025Towards]. Security benchmarks can also be read as evaluation design work, because they operationalize adversarial objectives and tool constraints in a way that generic leaderboards do not [@Zhang2025Security; @Zhang2024Agent].

Even within a single benchmark name, protocol variants can dominate. An agent evaluated under a cached environment, generous budgets, and permissive tool access is solving a different problem than the same agent under strict budgets, noisy tools, and full logging requirements. Evaluation papers increasingly recommend publishing protocol descriptors (tool catalog snapshot, budget model, logging schema, failure model) alongside leaderboard scores so that surveys can normalize across these axes [@Kim2026Beyond; @Ji2025Taxonomy]. This aligns with benchmark-as-system efforts: ship the harness, not just the dataset, so reproduction is possible across time and across tool ecosystems [@Zhang2025Buildbench; @Chen2025Towards]. In practice, these protocol descriptors are also what enable safety scores to be comparable: without an explicit interface and threat model, a security result cannot be interpreted across agent frameworks [@Zhang2025Security; @Wang2023Survey].

One implication is that evaluation-first work should provide not only tasks but also protocol defaults. When a benchmark release defines a reference tool catalog snapshot, a budget model, and a logging schema, later papers can report results without re-implementing the environment, and surveys can aggregate evidence without guessing hidden assumptions [@Zhang2025Buildbench; @Kim2026Beyond]. As benchmarks become more systems-like, harness-level decisions (timeouts, retries, caching) become part of the scientific claim, which is why papers that treat the harness as the deliverable are increasingly valuable for reproducible agent research [@Chen2025Towards].

Two practical reporting artifacts would make benchmarks easier to compare in surveys. First, a protocol card describing tool access, budget model, logging, and environment snapshot, so that results can be grouped by deployment realism rather than by benchmark name alone [@Ji2025Taxonomy; @Kim2026Beyond]. Second, an error taxonomy and audit log template—what failures were tool-selection errors, argument errors, retrieval errors, or monitor failures—so that improvements can be attributed to specific system components [@Mohammadi2025Evaluation; @Kale2025Reliable]. Without these artifacts, benchmark-driven progress will continue to mix incomparable settings and invite over-interpretation of headline metrics [@Wang2023Survey].

In practice, even a small set of standardized protocol fields would let meta-analyses separate tool realism from planner quality and reduce over-interpretation of headline metrics [@Mohammadi2025Evaluation].

Additionally, treating protocol descriptors as first-class benchmark outputs enables survey-level aggregation across benchmarks without guessing hidden assumptions.

A recurring limitation is that many published results remain hard to reproduce: papers may omit tool versions, budgets, or logging, and may not clearly distinguish between “policy improvements” and “protocol engineering.” This makes it unclear which gains will transfer to new tool ecosystems or deployment settings. A practical recommendation is to treat protocol files (tool schema, permission policy, budget, logging spec) as part of the benchmark release, enabling direct comparisons and reducing accidental drift across evaluations [@Kim2026Beyond; @Zhang2025Agents].
