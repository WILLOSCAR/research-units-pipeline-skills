## Abstract

Large language model (LLM) agents couple a foundation model with a control loop that iteratively observes, decides, and acts through tools or environments. This looped setting makes agents qualitatively different from single-shot prompting: success depends on interface contracts, orchestration policies, and evaluation protocols as much as on model capability. We survey recent work on tool-using LLM agents with an evidence-first lens, organizing the design space from foundations and interfaces to planning, memory, adaptation, multi-agent coordination, and evaluation and risk. Throughout, we emphasize what claims are comparable across papers (and which are not) by explicitly calling out benchmark/protocol choices and security threat models. Our goal is to help practitioners reason about agent design decisions and help researchers identify evaluation gaps that currently limit reproducible progress [@Yao2022React; @Schick2023Toolformer; @Plaat2025Agentic; @Mohammadi2025Evaluation; @Zhang2025Security].
