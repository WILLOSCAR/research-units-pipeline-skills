{"sub_id": "3.1", "title": "Agent loop and action spaces", "anchors": [{"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "paper_id": "P0090", "evidence_id": "E-P0090-1d5f67b08e", "pointer": "papers/paper_notes.jsonl:paper_id=P0090#key_results[0]"}, {"hook_type": "quant", "text": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["Xi2026Toolgym"], "paper_id": "P0158", "evidence_id": "E-P0158-895b04aa5c", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0165", "evidence_id": "E-P0165-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0165#key_results[0]"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "paper_id": "P0085", "evidence_id": "E-P0085-f7a14123f9", "pointer": "papers/paper_notes.jsonl:paper_id=P0085#limitations[1]"}, {"hook_type": "quant", "text": "We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design.", "citations": ["Fumero2025Cybersleuth"], "paper_id": "P0070", "evidence_id": "E-P0070-c8c4670812", "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0081", "evidence_id": "E-P0081-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "anchors": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["Liu2025Toolscope"], "paper_id": "P0113", "evidence_id": "E-P0113-468a77ff1d", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}, {"hook_type": "eval", "text": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Bench"], "paper_id": "P0086", "evidence_id": "E-P0086-c522619cea", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#method"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "paper_id": "P0220", "evidence_id": "E-P0220-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0220#key_results[0]"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0040", "evidence_id": "E-P0040-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#limitations[1]"}, {"hook_type": "quant", "text": "This attack shows a nearly 80% success rate in an end-to-end evaluation.", "citations": ["Fu2024Imprompter"], "paper_id": "P0136", "evidence_id": "E-P0136-0ea7b39672", "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0040", "evidence_id": "E-P0040-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "3", "section_title": "Foundations & Interfaces"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "anchors": [{"hook_type": "quant", "text": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["Nakano2025Guided"], "paper_id": "P0023", "evidence_id": "E-P0023-99359acdd7", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"hook_type": "quant", "text": "Our best performing strategy generates executable API calls 88% of the time.", "citations": ["Mudur2025Feabench"], "paper_id": "P0078", "evidence_id": "E-P0078-e4ac5005b3", "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "paper_id": "P0023", "evidence_id": "E-P0023-741891e300", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0121", "evidence_id": "E-P0121-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"hook_type": "limitation", "text": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate.", "citations": ["Shi2024Ehragent"], "paper_id": "P0129", "evidence_id": "E-P0129-2a7ea60588", "pointer": "papers/paper_notes.jsonl:paper_id=P0129#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and p", "citations": ["Liu2025Costbench"], "paper_id": "P0186", "evidence_id": "E-P0186-32d61c8fae", "pointer": "papers/paper_notes.jsonl:paper_id=P0186#key_results[0]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "anchors": [{"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "paper_id": "P0028", "evidence_id": "E-P0028-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"hook_type": "quant", "text": "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability.", "citations": ["Huang2025Retrieval"], "paper_id": "P0116", "evidence_id": "E-P0116-c9781caf3b", "pointer": "papers/paper_notes.jsonl:paper_id=P0116#limitations[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0029", "evidence_id": "E-P0029-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "citations": ["Huang2025Retrieval"], "paper_id": "P0116", "evidence_id": "E-P0116-4af0cf3c02", "pointer": "papers/paper_notes.jsonl:paper_id=P0116#key_results[1]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0146", "evidence_id": "E-P0146-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0146#key_results[1]"}, {"hook_type": "eval", "text": "We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effe", "citations": ["Zhang2024Large"], "paper_id": "P0044", "evidence_id": "E-P0044-52fea1d199", "pointer": "papers/paper_notes.jsonl:paper_id=P0044#key_results[1]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "4", "section_title": "Core Components (Planning + Memory)"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "anchors": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "citations": ["Li2026Autonomous"], "paper_id": "P0048", "evidence_id": "E-P0048-67ea29ce26", "pointer": "papers/paper_notes.jsonl:paper_id=P0048#method"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0165", "evidence_id": "E-P0165-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0165#key_results[0]"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0040", "evidence_id": "E-P0040-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#limitations[1]"}, {"hook_type": "quant", "text": "Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential.", "citations": ["Li2026Autonomous"], "paper_id": "P0048", "evidence_id": "E-P0048-9980bf7642", "pointer": "papers/paper_notes.jsonl:paper_id=P0048#key_results[1]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0040", "evidence_id": "E-P0040-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "citations": ["Xia2025Sand"], "paper_id": "P0031", "evidence_id": "E-P0031-6273763a98", "pointer": "papers/paper_notes.jsonl:paper_id=P0031#key_results[0]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "anchors": [{"hook_type": "quant", "text": "Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios.", "citations": ["Li2025What"], "paper_id": "P0118", "evidence_id": "E-P0118-ccc7d0cc50", "pointer": "papers/paper_notes.jsonl:paper_id=P0118#key_results[0]"}, {"hook_type": "eval", "text": "To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs.", "citations": ["Chuang2025Debate"], "paper_id": "P0187", "evidence_id": "E-P0187-e372fc0508", "pointer": "papers/paper_notes.jsonl:paper_id=P0187#method"}, {"hook_type": "limitation", "text": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "paper_id": "P0065", "evidence_id": "E-P0065-baa622fa7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[1]"}, {"hook_type": "eval", "text": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "paper_id": "P0181", "evidence_id": "E-P0181-e31a1bbba7", "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0142", "evidence_id": "E-P0142-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[1]"}, {"hook_type": "eval", "text": "First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task.", "citations": ["Shen2024Small"], "paper_id": "P0142", "evidence_id": "E-P0142-8158d9909c", "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "5", "section_title": "Learning, Adaptation & Coordination"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "anchors": [{"hook_type": "quant", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "paper_id": "P0013", "evidence_id": "E-P0013-79f88927fa", "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#method"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "limitation", "text": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "paper_id": "P0006", "evidence_id": "E-P0006-e688e4a34e", "pointer": "papers/paper_notes.jsonl:paper_id=P0006#limitations[1]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["Kale2025Reliable"], "paper_id": "P0099", "evidence_id": "E-P0099-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[1]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "paper_id": "P0096", "evidence_id": "E-P0096-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools.", "citations": ["Yang2025Contextagent"], "paper_id": "P0069", "evidence_id": "E-P0069-0fc82f8849", "pointer": "papers/paper_notes.jsonl:paper_id=P0069#key_results[0]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "6", "section_title": "Evaluation & Risks"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "anchors": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "paper_id": "P0096", "evidence_id": "E-P0096-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#method"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["Kale2025Reliable"], "paper_id": "P0099", "evidence_id": "E-P0099-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[1]"}, {"hook_type": "quant", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "citations": ["Bonagiri2025Check"], "paper_id": "P0067", "evidence_id": "E-P0067-7edb91824f", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}], "generated_at": "2026-01-19T12:09:29", "section_id": "6", "section_title": "Evaluation & Risks"}
