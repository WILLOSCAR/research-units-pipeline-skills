# Evidence draft: 6.1 Benchmarks and evaluation protocols

## Evidence snippets (with provenance)
- (E-P0083-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#method)
- (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
- (E-P0006-e688e4a34e) We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0006#limitations[1])
- (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
- (E-P0099-e0345118bc) To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. Kale2025Reliable (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0099#key_results[0])
- (E-P0083-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#key_results[1])
- (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
- (E-P0096-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- (E-P0069-0fc82f8849) To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Yang2025Contextagent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0069#key_results[0])
- (E-P0013-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0013#key_results[0])

## Definitions / setup

- Setup: Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Benchmarks and evaluation protocols'.. Axes: tool interface (function calling, schemas, protocols); tool selection / routing policy; sandboxing / permissions / observability; task suites (web / code / embodied / tools); metrics (success, cost, reliability, safety). Kim2026Beyond Liu2026Agents Plaat2025Agentic

## Claim candidates

- We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security
- MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP Zhang2025Security
- We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic
- RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval
- To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. Kale2025Reliable

## Concrete comparisons

- Axis: tool interface (function calling, schemas, protocols); A: Agent frameworks / architectures: `P0013`, `P0154`, `P0006`; B: Evaluation / benchmark-focused works: `P0013`, `P0034`, `P0035`. Kim2026Beyond Plaat2025Agentic Fu2025Eval Zhang2025Security
  - A highlight: (E-P0013-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0006-e688e4a34e) We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0006#limitations[1])
  - B highlight: (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0013`, `P0154`, `P0006`; B: Evaluation / benchmark-focused works: `P0013`, `P0034`, `P0035`. Kim2026Beyond Plaat2025Agentic Fu2025Eval Zhang2025Security
  - A highlight: (E-P0013-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0006-e688e4a34e) We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0006#limitations[1])
  - B highlight: (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0013`, `P0154`, `P0006`; B: Evaluation / benchmark-focused works: `P0013`, `P0034`, `P0035`. Plaat2025Agentic Kim2026Beyond Zhang2025Security Mohammadi2025Evaluation
  - A highlight: (E-P0006-e688e4a34e) We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0006#limitations[1])
  - A highlight: (E-P0013-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
- Axis: task suites (web / code / embodied / tools); A: Agent frameworks / architectures: `P0013`, `P0154`, `P0006`; B: Evaluation / benchmark-focused works: `P0013`, `P0034`, `P0035`. Kim2026Beyond Plaat2025Agentic Fu2025Eval Zhang2025Security
  - A highlight: (E-P0013-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0006-e688e4a34e) We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0006#limitations[1])
  - B highlight: (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
- Axis: metrics (success, cost, reliability, safety); A: Agent frameworks / architectures: `P0013`, `P0154`, `P0006`; B: Evaluation / benchmark-focused works: `P0013`, `P0034`, `P0035`. Kim2026Beyond Plaat2025Agentic Mohammadi2025Evaluation Zhang2025Security
  - A highlight: (E-P0013-79f88927fa) Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. Kim2026Beyond (pointer: papers/paper_notes.jsonl:paper_id=P0013#key_results[0])
  - A highlight: (E-P0006-e688e4a34e) We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic (pointer: papers/paper_notes.jsonl:paper_id=P0006#limitations[1])
  - B highlight: (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: API; LLMs; WildAGTEval; IPI; IPI-centric; RPA; RPAs; LLM-based; GPT-4; FMs. Kim2026Beyond Liu2026Agents Plaat2025Agentic Ji2025Taxonomy

## Failures / limitations

- Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Liu2026Agents
- We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society. Plaat2025Agentic
- We then thoroughly assess the security and usability of representative defense frameworks. Ji2025Taxonomy
- Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Ji2025Taxonomy

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- compute/training/inference cost
- baseline choices and ablation evidence
