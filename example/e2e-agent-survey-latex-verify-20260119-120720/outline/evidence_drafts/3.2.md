# Evidence draft: 3.2 Tool interfaces and orchestration

## Evidence snippets (with provenance)
- (E-P0086-c522619cea) We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Bench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0086#method)
- (E-P0220-fae121f81b) Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0220#key_results[0])
- (E-P0040-4da9e4ae32) We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0040#limitations[1])
- (E-P0136-0ea7b39672) This attack shows a nearly 80% success rate in an end-to-end evaluation. Fu2024Imprompter (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0136#key_results[0])
- (E-P0032-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
- (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. Mohammadi2025Evaluation (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
- (E-P0113-468a77ff1d) Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0113#key_results[0])
- (E-P0040-d5c234444e) Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. Du2024Anytool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0040#key_results[0])
- (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
- (E-P0086-2b935ec0ac) We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Bench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0086#key_results[0])

## Definitions / setup

- Setup: Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Tool interfaces and orchestration'.. Axes: tool interface (function calling, schemas, protocols); tool selection / routing policy; sandboxing / permissions / observability; mechanism / architecture; data / training setup. Hao2026From Xuan2026Confidence Jia2025Autotool

## Claim candidates

- We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Bench
- Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat. Li2025Dissonances
- We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. Du2024Anytool
- This attack shows a nearly 80% success rate in an end-to-end evaluation. Fu2024Imprompter
- Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self

## Concrete comparisons

- Axis: tool interface (function calling, schemas, protocols); A: Agent frameworks / architectures: `P0151`, `P0157`, `P0019`; B: Tool-use and function calling: `P0151`, `P0157`, `P0019`. Lumer2025Memtool Mohammadi2025Evaluation Dong2025Bench
  - A highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - A highlight: (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0086-c522619cea) We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Bench (pointer: papers/paper_notes.jsonl:paper_id=P0086#method)
- Axis: tool selection / routing policy; A: Agent frameworks / architectures: `P0151`, `P0157`, `P0019`; B: Tool-use and function calling: `P0151`, `P0157`, `P0019`. Lumer2025Memtool Mohammadi2025Evaluation Dong2025Bench
  - A highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - A highlight: (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0086-c522619cea) We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem. Dong2025Bench (pointer: papers/paper_notes.jsonl:paper_id=P0086#method)
- Axis: sandboxing / permissions / observability; A: Agent frameworks / architectures: `P0151`, `P0157`, `P0019`; B: Tool-use and function calling: `P0151`, `P0157`, `P0019`. Mohammadi2025Evaluation Zhou2025Self Lumer2025Memtool Liu2025Toolscope
  - A highlight: (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
  - A highlight: (E-P0032-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0113-468a77ff1d) Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope (pointer: papers/paper_notes.jsonl:paper_id=P0113#key_results[0])
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0151`, `P0157`, `P0019`; B: Tool-use and function calling: `P0151`, `P0157`, `P0019`. Mohammadi2025Evaluation Zhou2025Self Lumer2025Memtool Liu2025Toolscope
  - A highlight: (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
  - A highlight: (E-P0032-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0113-468a77ff1d) Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope (pointer: papers/paper_notes.jsonl:paper_id=P0113#key_results[0])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0151`, `P0157`, `P0019`; B: Tool-use and function calling: `P0151`, `P0157`, `P0019`. Zhou2025Self Mohammadi2025Evaluation Lumer2025Memtool Liu2025Toolscope
  - A highlight: (E-P0032-2e6956a116) Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data. Zhou2025Self (pointer: papers/paper_notes.jsonl:paper_id=P0032#key_results[0])
  - A highlight: (E-P0076-37f9ea924c) This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and Mohammadi2025Evaluation (pointer: papers/paper_notes.jsonl:paper_id=P0076#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0113-468a77ff1d) Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use. Liu2025Toolscope (pointer: papers/paper_notes.jsonl:paper_id=P0113#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: API; GPT-5; HardGen; LLMs; ReAct; AutoTool; TauBench; TicToc; LLM-based; MCP. Hao2026From Xuan2026Confidence Jia2025Autotool Zhou2025Self

## Failures / limitations

- Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Hao2026From
- Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. Xuan2026Confidence
- The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. Zhou2025Self
- However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages. Cheng2025Your

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
