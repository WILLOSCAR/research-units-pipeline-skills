# Evidence draft: 4.2 Memory and retrieval (RAG)

## Evidence snippets (with provenance)
- (E-P0028-69289755a4) In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Tawosi2025Meta (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0028#method)
- (E-P0028-f36b515991) Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. Tawosi2025Meta (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
- (E-P0116-c9781caf3b) SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. Huang2025Retrieval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0116#limitations[1])
- (E-P0058-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0058#key_results[0])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0029-68db58914f) Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Shi2025Progent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0029#key_results[0])
- (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
- (E-P0116-4af0cf3c02) This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. Huang2025Retrieval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0116#key_results[1])
- (E-P0146-9abcf1bf8a) Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. Verma2026Active (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0146#key_results[1])
- (E-P0044-52fea1d199) We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Zhang2024Large (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0044#key_results[1])

## Definitions / setup

- Setup: Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Memory and retrieval (RAG)'.. Axes: memory type (episodic / semantic / scratchpad); retrieval source + index (docs / web / logs); write / update / forgetting policy; mechanism / architecture; data / training setup. Verma2026Active Yu2026Agentic Tao2026Membox

## Claim candidates

- In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Tawosi2025Meta
- Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. Tawosi2025Meta
- SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. Huang2025Retrieval
- Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift
- On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React

## Concrete comparisons

- Axis: memory type (episodic / semantic / scratchpad); A: Agent frameworks / architectures: `P0146`, `P0147`, `P0156`; B: Memory / retrieval augmentation: `P0146`, `P0147`, `P0156`. Li2025Agentswift Lumer2025Memtool Tawosi2025Meta
  - A highlight: (E-P0058-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0058#key_results[0])
  - A highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0028-f36b515991) Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. Tawosi2025Meta (pointer: papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
- Axis: retrieval source + index (docs / web / logs); A: Agent frameworks / architectures: `P0146`, `P0147`, `P0156`; B: Memory / retrieval augmentation: `P0146`, `P0147`, `P0156`. Li2025Agentswift Lumer2025Memtool Tawosi2025Meta
  - A highlight: (E-P0058-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0058#key_results[0])
  - A highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0028-f36b515991) Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. Tawosi2025Meta (pointer: papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
- Axis: write / update / forgetting policy; A: Agent frameworks / architectures: `P0146`, `P0147`, `P0156`; B: Memory / retrieval augmentation: `P0146`, `P0147`, `P0156`. Li2025Agentswift Lumer2025Memtool Tawosi2025Meta
  - A highlight: (E-P0058-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0058#key_results[0])
  - A highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0028-f36b515991) Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. Tawosi2025Meta (pointer: papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0146`, `P0147`, `P0156`; B: Memory / retrieval augmentation: `P0146`, `P0147`, `P0156`. Li2025Agentswift Lumer2025Memtool Tawosi2025Meta
  - A highlight: (E-P0058-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0058#key_results[0])
  - A highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0028-f36b515991) Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. Tawosi2025Meta (pointer: papers/paper_notes.jsonl:paper_id=P0028#key_results[1])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0146`, `P0147`, `P0156`; B: Memory / retrieval augmentation: `P0146`, `P0147`, `P0156`. Li2025Agentswift Lumer2025Memtool Tawosi2025Meta
  - A highlight: (E-P0058-904ba35500) Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\% over both existing automated agent search methods and manually designed agents. Li2025Agentswift (pointer: papers/paper_notes.jsonl:paper_id=P0058#key_results[0])
  - A highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0088-35271418ac) Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. Lumer2025Memtool (pointer: papers/paper_notes.jsonl:paper_id=P0088#key_results[0])
  - B highlight: (E-P0028-f36b515991) Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\%, into a compact, structured, natural language representation. Tawosi2025Meta (pointer: papers/paper_notes.jsonl:paper_id=P0028#key_results[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: SWE-bench; LTM; STM; GRPO; AgeMem; MEM; LoCoMo; ASB; LLMs; AgentDojo. Verma2026Active Yu2026Agentic Tao2026Membox Shi2025Progent

## Failures / limitations

- Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Yu2026Agentic
- Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Shi2025Progent
- LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Shi2025Progent
- Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Shi2025Progent

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
