# Evidence draft: 4.1 Planning and reasoning loops

## Evidence snippets (with provenance)
- (E-P0022-9d9d60644a) We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0022#method)
- (E-P0121-38a26e4777) Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0121#key_results[0])
- (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
- (E-P0001-ca4a00b5cf) On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0001#key_results[0])
- (E-P0114-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0114#key_results[0])
- (E-P0129-2a7ea60588) Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. Shi2024Ehragent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0129#key_results[0])
- (E-P0186-32d61c8fae) Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. Liu2025Costbench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0186#key_results[0])
- (E-P0023-741891e300) To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Nakano2025Guided (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0023#key_results[1])
- (E-P0023-99359acdd7) Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. Nakano2025Guided (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
- (E-P0078-e4ac5005b3) Our best performing strategy generates executable API calls 88% of the time. Mudur2025Feabench (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0078#key_results[0])

## Definitions / setup

- Setup: Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Planning and reasoning loops'.. Axes: control loop design (planner / executor, search); deliberation method (CoT / ToT / MCTS); action grounding (tool calls vs environment actions); mechanism / architecture; data / training setup. Zhou2025Reasoning Kim2025Bridging Nakano2025Guided

## Claim candidates

- We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). Kim2025Bridging
- Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Shang2024Agentsquare
- To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree
- On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Yao2022React
- Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks. Hu2025Training

## Concrete comparisons

- Axis: control loop design (planner / executor, search); A: Planning / reasoning loops: `P0012`, `P0022`, `P0023`; B: Agent frameworks / architectures: `P0012`, `P0022`, `P0030`. Nakano2025Guided Choi2025Reactree Hu2025Training
  - A highlight: (E-P0023-99359acdd7) Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. Nakano2025Guided (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - A highlight: (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (pointer: papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
  - B highlight: (E-P0114-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0114#key_results[0])
  - B highlight: (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (pointer: papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
- Axis: deliberation method (CoT / ToT / MCTS); A: Planning / reasoning loops: `P0012`, `P0022`, `P0023`; B: Agent frameworks / architectures: `P0012`, `P0022`, `P0030`. Nakano2025Guided Choi2025Reactree Hu2025Training
  - A highlight: (E-P0023-99359acdd7) Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. Nakano2025Guided (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - A highlight: (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (pointer: papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
  - B highlight: (E-P0114-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0114#key_results[0])
  - B highlight: (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (pointer: papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
- Axis: action grounding (tool calls vs environment actions); A: Planning / reasoning loops: `P0012`, `P0022`, `P0023`; B: Agent frameworks / architectures: `P0012`, `P0022`, `P0030`. Nakano2025Guided Mudur2025Feabench Hu2025Training Choi2025Reactree
  - A highlight: (E-P0023-99359acdd7) Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. Nakano2025Guided (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - A highlight: (E-P0078-e4ac5005b3) Our best performing strategy generates executable API calls 88% of the time. Mudur2025Feabench (pointer: papers/paper_notes.jsonl:paper_id=P0078#key_results[0])
  - B highlight: (E-P0114-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0114#key_results[0])
  - B highlight: (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (pointer: papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
- Axis: mechanism / architecture; A: Planning / reasoning loops: `P0012`, `P0022`, `P0023`; B: Agent frameworks / architectures: `P0012`, `P0022`, `P0030`. Nakano2025Guided Hu2025Training Choi2025Reactree
  - A highlight: (E-P0023-741891e300) To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Nakano2025Guided (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[1])
  - A highlight: (E-P0023-99359acdd7) Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. Nakano2025Guided (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - B highlight: (E-P0114-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0114#key_results[0])
  - B highlight: (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (pointer: papers/paper_notes.jsonl:paper_id=P0098#limitations[1])
- Axis: data / training setup; A: Planning / reasoning loops: `P0012`, `P0022`, `P0023`; B: Agent frameworks / architectures: `P0012`, `P0022`, `P0030`. Nakano2025Guided Hu2025Training Choi2025Reactree
  - A highlight: (E-P0023-741891e300) To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Nakano2025Guided (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[1])
  - A highlight: (E-P0023-99359acdd7) Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and required 86.2\%, 118.7\%, and 205.9\% more model queries. Nakano2025Guided (pointer: papers/paper_notes.jsonl:paper_id=P0023#key_results[0])
  - B highlight: (E-P0114-771620f84f) Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning Hu2025Training (pointer: papers/paper_notes.jsonl:paper_id=P0114#key_results[0])
  - B highlight: (E-P0098-c782275d8d) To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Choi2025Reactree (pointer: papers/paper_notes.jsonl:paper_id=P0098#limitations[1])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: RSP; GSI; RSV; FEVER; RSP-M; HotpotQA; ReAct; SCL; CCAM; GPT-4o-powered. Zhou2025Reasoning Kim2025Bridging Nakano2025Guided Hatalis2025Review

## Failures / limitations

- While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning
- We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. Zhou2025Reasoning
- This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. Nakano2025Guided
- Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. Hatalis2025Review

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
