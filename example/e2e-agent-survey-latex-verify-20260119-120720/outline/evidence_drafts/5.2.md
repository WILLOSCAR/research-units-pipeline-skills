# Evidence draft: 5.2 Multi-agent coordination

## Evidence snippets (with provenance)
- (E-P0187-e372fc0508) To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. Chuang2025Debate (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0187#method)
- (E-P0065-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0065#key_results[1])
- (E-P0142-c92ed293ba) While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0142#limitations[1])
- (E-P0181-e31a1bbba7) We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. Wu2025Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
- (E-P0065-b35b53de13) We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. Silva2025Agents (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0065#key_results[0])
- (E-P0142-9640816b42) Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0142#key_results[1])
- (E-P0217-0f34c44fa9) To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. Zahedifar2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0217#key_results[1])
- (E-P0142-8158d9909c) First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Shen2024Small (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0142#key_results[0])
- (E-P0188-837ee80fad) We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed Li2025Draft (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0188#key_results[0])
- (E-P0118-ccc7d0cc50) Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. Li2025What (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0118#key_results[0])

## Definitions / setup

- Setup: Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Multi-agent coordination'.. Axes: communication protocol + role assignment; aggregation (vote / debate / referee); stability (collusion, mode collapse, incentives); mechanism / architecture; data / training setup. Sarkar2025Survey Silva2025Agents Cao2025Skyrl

## Claim candidates

- To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. Chuang2025Debate
- This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents
- While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Shen2024Small
- We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. Wu2025Agents
- We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. Silva2025Agents

## Concrete comparisons

- Axis: communication protocol + role assignment; A: Agent frameworks / architectures: `P0033`, `P0065`, `P0106`; B: Multi-agent coordination: `P0065`, `P0164`, `P0181`. Li2025What Silva2025Agents Li2025Draft Wu2025Agents
  - A highlight: (E-P0118-ccc7d0cc50) Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. Li2025What (pointer: papers/paper_notes.jsonl:paper_id=P0118#key_results[0])
  - A highlight: (E-P0065-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0065#key_results[1])
  - B highlight: (E-P0188-837ee80fad) We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed Li2025Draft (pointer: papers/paper_notes.jsonl:paper_id=P0188#key_results[0])
  - B highlight: (E-P0181-e31a1bbba7) We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. Wu2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
- Axis: aggregation (vote / debate / referee); A: Agent frameworks / architectures: `P0033`, `P0065`, `P0106`; B: Multi-agent coordination: `P0065`, `P0164`, `P0181`. Li2025What Silva2025Agents Li2025Draft Wu2025Agents
  - A highlight: (E-P0118-ccc7d0cc50) Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. Li2025What (pointer: papers/paper_notes.jsonl:paper_id=P0118#key_results[0])
  - A highlight: (E-P0065-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0065#key_results[1])
  - B highlight: (E-P0188-837ee80fad) We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed Li2025Draft (pointer: papers/paper_notes.jsonl:paper_id=P0188#key_results[0])
  - B highlight: (E-P0181-e31a1bbba7) We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. Wu2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
- Axis: stability (collusion, mode collapse, incentives); A: Agent frameworks / architectures: `P0033`, `P0065`, `P0106`; B: Multi-agent coordination: `P0065`, `P0164`, `P0181`. Li2025What Silva2025Agents Li2025Draft Wu2025Agents
  - A highlight: (E-P0118-ccc7d0cc50) Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. Li2025What (pointer: papers/paper_notes.jsonl:paper_id=P0118#key_results[0])
  - A highlight: (E-P0065-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0065#key_results[1])
  - B highlight: (E-P0188-837ee80fad) We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed Li2025Draft (pointer: papers/paper_notes.jsonl:paper_id=P0188#key_results[0])
  - B highlight: (E-P0181-e31a1bbba7) We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. Wu2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0033`, `P0065`, `P0106`; B: Multi-agent coordination: `P0065`, `P0164`, `P0181`. Li2025What Silva2025Agents Li2025Draft Wu2025Agents
  - A highlight: (E-P0118-ccc7d0cc50) Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. Li2025What (pointer: papers/paper_notes.jsonl:paper_id=P0118#key_results[0])
  - A highlight: (E-P0065-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0065#key_results[1])
  - B highlight: (E-P0188-837ee80fad) We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed Li2025Draft (pointer: papers/paper_notes.jsonl:paper_id=P0188#key_results[0])
  - B highlight: (E-P0181-e31a1bbba7) We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. Wu2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0181#key_results[0])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0033`, `P0065`, `P0106`; B: Multi-agent coordination: `P0065`, `P0164`, `P0181`. Li2025What Silva2025Agents Li2025Draft Wu2025Agents
  - A highlight: (E-P0118-ccc7d0cc50) Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. Li2025What (pointer: papers/paper_notes.jsonl:paper_id=P0118#key_results[0])
  - A highlight: (E-P0065-baa622fa7f) This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0065#key_results[1])
  - B highlight: (E-P0188-837ee80fad) We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed Li2025Draft (pointer: papers/paper_notes.jsonl:paper_id=P0188#key_results[0])
  - B highlight: (E-P0181-e31a1bbba7) We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. Wu2025Agents (pointer: papers/paper_notes.jsonl:paper_id=P0181#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: MCP; LLM-based; MCP-compliant; LLMs; SA-SWE-32B; AST-based; SWE-Bench; SWE; SkyRL-Agent; SkyRL-train. Sarkar2025Survey Silva2025Agents Cao2025Skyrl Li2025What

## Failures / limitations

- The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems. Sarkar2025Survey
- This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements. Silva2025Agents
- Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains. Papadakis2025Atlas
- To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. Einwiller2025Benevolent

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
