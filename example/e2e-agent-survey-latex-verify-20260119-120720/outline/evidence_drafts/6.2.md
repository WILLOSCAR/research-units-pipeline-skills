# Evidence draft: 6.2 Safety, security, and governance

## Evidence snippets (with provenance)
- (E-P0083-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#method)
- (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
- (E-P0193-a256400826) We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. Lichkovski2025Agent (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0193#limitations[1])
- (E-P0099-e0345118bc) To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. Kale2025Reliable (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0099#key_results[0])
- (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
- (E-P0083-7a6ec4daed) We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Zhang2025Security (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0083#key_results[1])
- (E-P0096-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- (E-P0067-7edb91824f) Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Bonagiri2025Check (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0067#key_results[0])
- (E-P0166-ee6f97d5e5) However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. Wang2025Adversarial (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0166#key_results[1])
- (E-P0166-8512d7eecf) Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. Wang2025Adversarial (provenance: paper_notes | papers/paper_notes.jsonl:paper_id=P0166#key_results[0])

## Definitions / setup

- Setup: Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured? Scope: in-scope: Core topics directly relevant to 'Safety, security, and governance'.. Axes: threat model (prompt / tool injection, exfiltration); defense surface (policy, sandbox, monitoring); security evaluation protocol; mechanism / architecture; data / training setup. Zhou2025Reasoning Gasmi2025Bridging Hadeliya2025When

## Claim candidates

- We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security
- MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP Zhang2025Security
- We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. Lichkovski2025Agent
- To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. Kale2025Reliable
- RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval

## Concrete comparisons

- Axis: threat model (prompt / tool injection, exfiltration); A: Agent frameworks / architectures: `P0012`, `P0021`, `P0036`; B: Safety / security / guardrails: `P0021`, `P0036`, `P0052`. Zhang2025Security Fu2025Eval
  - A highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
- Axis: defense surface (policy, sandbox, monitoring); A: Agent frameworks / architectures: `P0012`, `P0021`, `P0036`; B: Safety / security / guardrails: `P0021`, `P0036`, `P0052`. Zhang2025Security
  - A highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0083-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#method)
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0083-8bcb673a7d) We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#method)
- Axis: security evaluation protocol; A: Agent frameworks / architectures: `P0012`, `P0021`, `P0036`; B: Safety / security / guardrails: `P0021`, `P0036`, `P0052`. Zhang2025Security Fu2025Eval
  - A highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0096-753416ce70) RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[1])
- Axis: mechanism / architecture; A: Agent frameworks / architectures: `P0012`, `P0021`, `P0036`; B: Safety / security / guardrails: `P0021`, `P0036`, `P0052`. Zhang2025Security Fu2025Eval
  - A highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0096-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0096-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
- Axis: data / training setup; A: Agent frameworks / architectures: `P0012`, `P0021`, `P0036`; B: Safety / security / guardrails: `P0021`, `P0036`, `P0052`. Zhang2025Security Fu2025Eval
  - A highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - A highlight: (E-P0096-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])
  - B highlight: (E-P0083-d6095e10e9) MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed Zhang2025Security (pointer: papers/paper_notes.jsonl:paper_id=P0083#key_results[0])
  - B highlight: (E-P0096-2895472ae1) We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Fu2025Eval (pointer: papers/paper_notes.jsonl:paper_id=P0096#key_results[0])

## Evaluation protocol

- Evaluation tokens mentioned in mapped evidence: RSP; GSI; RSV; FEVER; RSP-M; HotpotQA; ReAct; AI-specific; MCP; JSON. Zhou2025Reasoning Gasmi2025Bridging Hadeliya2025When Luo2025Agrail

## Failures / limitations

- While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. Zhou2025Reasoning
- We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. Zhou2025Reasoning
- Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. Gasmi2025Bridging
- This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. Gasmi2025Bridging

## Verify fields (non-blocking)

- named benchmarks/datasets used
- metrics/human-eval protocol
- training data and supervision signal
- baseline choices and ablation evidence
