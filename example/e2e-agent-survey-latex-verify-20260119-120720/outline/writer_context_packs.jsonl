{"sub_id": "3.1", "title": "Agent loop and action spaces", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Agent loop and action spaces highlights a tension around mechanism / architecture and data / training setup, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Agent loop and action spaces, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "decision-first", "opener_hint": "Start with a builder/research decision; state what it depends on; end paragraph 1 with the thesis.", "axes": ["mechanism / architecture", "data / training setup", "evaluation protocol (datasets / metrics / human)", "efficiency / compute", "failure modes / limitations"], "bridge_terms": ["benchmarks/metrics", "compute"], "contrast_hook": "evaluation", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "training signal / supervision", "failure modes / limitations"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Agent loop and action spaces drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Agent frameworks / architectures as baseline", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: mechanism / architecture, data / training setup, evaluation protocol (datasets / metrics / human), efficiency / compute, failure modes / limitations"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Tool-use and function calling", "contrast with Agent frameworks / architectures", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Tool-use and function calling vs Agent frameworks / architectures", "use_clusters": ["Tool-use and function calling"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Tool-use and function calling", "data / training setup", "interface contract", "axes: mechanism / architecture, data / training setup, evaluation protocol (datasets / metrics / human), efficiency / compute, failure modes / limitations"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Tool-use and function calling"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Tool-use and function calling", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Tool-use and function calling"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Tool-use and function calling", "multiple citations in one paragraph", "axes: mechanism / architecture, data / training setup, evaluation protocol (datasets / metrics / human), efficiency / compute, failure modes / limitations"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Agent frameworks / architectures vs Tool-use and function calling along mechanism / architecture, data / training setup, evaluation protocol (datasets / metrics / human), efficiency / compute, failure modes / limitations", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: mechanism / architecture.", "Compare approaches along: data / training setup.", "Compare approaches along: evaluation protocol (datasets / metrics / human).", "Compare approaches along: efficiency / compute.", "Compare approaches along: failure modes / limitations."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Kim2025Bridging", "Zhao2025Achieving", "Liu2025Mcpagentbench", "Li2025Agentswift", "Fumero2025Cybersleuth", "Feng2025Group", "Zhang2026Evoroute", "Yao2022React", "Ghose2025Orfs", "Xi2026Toolgym", "Luo2025Large", "Song2026Envscaler", "Zhang2025Datascibench", "Xu2025Exemplar"], "allowed_bibkeys_mapped": ["Zhao2025Achieving", "Kim2025Bridging", "Li2025Agentswift", "Ghose2025Orfs", "Song2026Envscaler", "Fumero2025Cybersleuth", "Feng2025Group", "Wu2025Meta", "Fang2025Should", "Zhang2025Datascibench", "Xu2025Exemplar", "Zhang2026Evoroute", "Xi2026Toolgym", "Luo2025Large", "Gasmi2025Bridging", "Liu2025Mcpagentbench", "Yang2025Proagent", "Yao2022React"], "allowed_bibkeys_chapter": ["Chen2025Agentguard", "Cheng2025Your", "Dong2025Bench", "Du2024Anytool", "Fang2025Should", "Feng2025Group", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Hao2026From", "Jia2025Autotool", "Kim2025Bridging", "Li2024Personal", "Li2025Agentswift", "Li2025Dissonances", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Lumer2025Memtool", "Luo2025Large", "Mohammadi2025Evaluation", "Shen2024Small", "Song2026Envscaler", "Wu2024Avatar", "Wu2025Meta", "Xi2026Toolgym", "Xu2025Exemplar", "Xuan2026Confidence", "Yang2025Proagent", "Yao2022React", "Zhang2025Datascibench", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0022-9d9d60644a", "E-P0165-1063eee7ce", "E-P0085-f7a14123f9", "E-P0058-904ba35500", "E-P0070-c8c4670812", "E-P0081-4b027dfb27", "E-P0150-60cc0d458f", "E-P0001-ca4a00b5cf", "E-P0090-1d5f67b08e", "E-P0158-895b04aa5c", "E-P0158-8c9597d805", "E-P0008-62cd0c501b", "E-P0090-ddd045953e", "E-P0085-3a4792de2b", "E-P0085-67d5c4342c", "E-P0149-f1a25e82c1", "E-P0189-39281e5083", "E-P0197-be70dd09e9"], "anchor_facts": [{"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "paper_id": "P0090", "evidence_id": "E-P0090-1d5f67b08e", "pointer": "papers/paper_notes.jsonl:paper_id=P0090#key_results[0]"}, {"hook_type": "quant", "text": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["Xi2026Toolgym"], "paper_id": "P0158", "evidence_id": "E-P0158-895b04aa5c", "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0165", "evidence_id": "E-P0165-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0165#key_results[0]"}, {"hook_type": "eval", "text": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "paper_id": "P0085", "evidence_id": "E-P0085-f7a14123f9", "pointer": "papers/paper_notes.jsonl:paper_id=P0085#limitations[1]"}, {"hook_type": "quant", "text": "We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design.", "citations": ["Fumero2025Cybersleuth"], "paper_id": "P0070", "evidence_id": "E-P0070-c8c4670812", "pointer": "papers/paper_notes.jsonl:paper_id=P0070#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop, as well as tool-integrated reasoning on search-augmented QA tasks, using Qwen2.5-1.5B/3B/7B-Instruct.", "citations": ["Feng2025Group"], "paper_id": "P0081", "evidence_id": "E-P0081-4b027dfb27", "pointer": "papers/paper_notes.jsonl:paper_id=P0081#key_results[0]"}], "comparison_cards": [{"axis": "mechanism / architecture", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Zhang2026Evoroute", "Li2025Agentswift", "Ghose2025Orfs", "Xi2026Toolgym"], "A_highlights": [{"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "B_highlights": [{"paper_id": "P0090", "evidence_id": "E-P0090-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0090#key_results[0]"}, {"paper_id": "P0158", "evidence_id": "E-P0158-895b04aa5c", "excerpt": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["Xi2026Toolgym"], "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'mechanism / architecture'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "data / training setup", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Xi2026Toolgym", "Zhang2026Evoroute", "Ghose2025Orfs"], "A_highlights": [{"paper_id": "P0158", "evidence_id": "E-P0158-895b04aa5c", "excerpt": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["Xi2026Toolgym"], "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}, {"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}], "B_highlights": [{"paper_id": "P0158", "evidence_id": "E-P0158-895b04aa5c", "excerpt": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["Xi2026Toolgym"], "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}, {"paper_id": "P0090", "evidence_id": "E-P0090-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0090#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'data / training setup'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "evaluation protocol (datasets / metrics / human)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Li2025Agentswift", "Zhang2026Evoroute", "Liu2025Mcpagentbench", "Ghose2025Orfs"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}], "B_highlights": [{"paper_id": "P0085", "evidence_id": "E-P0085-f7a14123f9", "excerpt": "To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents.", "citations": ["Liu2025Mcpagentbench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0085#limitations[1]"}, {"paper_id": "P0090", "evidence_id": "E-P0090-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0090#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'evaluation protocol (datasets / metrics / human)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "efficiency / compute", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Zhang2026Evoroute", "Li2025Agentswift", "Ghose2025Orfs", "Xi2026Toolgym"], "A_highlights": [{"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}], "B_highlights": [{"paper_id": "P0090", "evidence_id": "E-P0090-1d5f67b08e", "excerpt": "Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations.", "citations": ["Ghose2025Orfs"], "pointer": "papers/paper_notes.jsonl:paper_id=P0090#key_results[0]"}, {"paper_id": "P0158", "evidence_id": "E-P0158-895b04aa5c", "excerpt": "Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness.", "citations": ["Xi2026Toolgym"], "pointer": "papers/paper_notes.jsonl:paper_id=P0158#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'efficiency / compute'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: LLMs; LLM-simulated; SFT; RUC-NLPIR; EnvScaler; SkelBuilder; ScenGenerator; GAIA; EvoRoute; BrowseComp.", "citations": ["Song2026Envscaler", "Zhang2026Evoroute", "Xi2026Toolgym", "Luo2025Large"]}], "limitation_hooks": [{"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service).", "citations": ["Gasmi2025Bridging"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 9, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
{"sub_id": "3.2", "title": "Tool interfaces and orchestration", "section_id": "3", "section_title": "Foundations & Interfaces", "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Tool interfaces and orchestration highlights a tension around tool interface (function calling, schemas, protocols) and tool selection / routing policy, motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Tool interfaces and orchestration, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["tool interface (function calling, schemas, protocols)", "tool selection / routing policy", "sandboxing / permissions / observability", "mechanism / architecture", "data / training setup"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Tool interfaces and orchestration drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Agent frameworks / architectures as baseline", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Tool-use and function calling", "contrast with Agent frameworks / architectures", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Tool-use and function calling vs Agent frameworks / architectures", "use_clusters": ["Tool-use and function calling"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Tool-use and function calling", "data / training setup", "interface contract", "axes: tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Tool-use and function calling"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Tool-use and function calling", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Tool-use and function calling"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Tool-use and function calling", "multiple citations in one paragraph", "axes: tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, mechanism / architecture, data / training setup"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Agent frameworks / architectures vs Tool-use and function calling along tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, mechanism / architecture, data / training setup", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Agent frameworks / architectures", "Tool-use and function calling", "Evaluation / benchmark-focused works"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: mechanism / architecture.", "Compare approaches along: data / training setup.", "Compare approaches along: evaluation protocol (datasets / metrics / human).", "Compare approaches along: efficiency / compute.", "Compare approaches along: failure modes / limitations."], "chapter_key_contrasts": ["evaluation", "tool interfaces"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Dong2025Bench", "Li2025Dissonances", "Du2024Anytool", "Fu2024Imprompter", "Zhou2025Self", "Mohammadi2025Evaluation", "Liu2025Toolscope", "Lumer2025Memtool", "Shen2024Small", "Xuan2026Confidence", "Hao2026From", "Yao2022React", "Ghose2025Orfs", "Wu2024Avatar"], "allowed_bibkeys_mapped": ["Dong2025Bench", "Jia2025Autotool", "Chen2025Agentguard", "Hao2026From", "Cheng2025Your", "Lumer2025Memtool", "Du2024Anytool", "Shen2024Small", "Li2025Dissonances", "Wu2024Avatar", "Fu2024Imprompter", "Ghose2025Orfs", "Xuan2026Confidence", "Zhou2025Self", "Mohammadi2025Evaluation", "Liu2025Toolscope", "Li2024Personal", "Yao2022React"], "allowed_bibkeys_chapter": ["Chen2025Agentguard", "Cheng2025Your", "Dong2025Bench", "Du2024Anytool", "Fang2025Should", "Feng2025Group", "Fu2024Imprompter", "Fumero2025Cybersleuth", "Gasmi2025Bridging", "Ghose2025Orfs", "Hao2026From", "Jia2025Autotool", "Kim2025Bridging", "Li2024Personal", "Li2025Agentswift", "Li2025Dissonances", "Liu2025Mcpagentbench", "Liu2025Toolscope", "Lumer2025Memtool", "Luo2025Large", "Mohammadi2025Evaluation", "Shen2024Small", "Song2026Envscaler", "Wu2024Avatar", "Wu2025Meta", "Xi2026Toolgym", "Xu2025Exemplar", "Xuan2026Confidence", "Yang2025Proagent", "Yao2022React", "Zhang2025Datascibench", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0086-c522619cea", "E-P0220-fae121f81b", "E-P0040-4da9e4ae32", "E-P0136-0ea7b39672", "E-P0032-2e6956a116", "E-P0076-37f9ea924c", "E-P0113-468a77ff1d", "E-P0040-d5c234444e", "E-P0088-35271418ac", "E-P0086-2b935ec0ac", "E-P0086-1d220ae35d", "E-P0142-9640816b42", "E-P0032-17d0e7f9d9", "E-P0157-c2fdc5ad72", "E-P0151-ed4427964c", "E-P0001-ca4a00b5cf", "E-P0090-1d5f67b08e", "E-P0123-3575a7c673"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "quant", "text": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["Liu2025Toolscope"], "paper_id": "P0113", "evidence_id": "E-P0113-468a77ff1d", "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}, {"hook_type": "eval", "text": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Bench"], "paper_id": "P0086", "evidence_id": "E-P0086-c522619cea", "pointer": "papers/paper_notes.jsonl:paper_id=P0086#method"}, {"hook_type": "quant", "text": "Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.", "citations": ["Li2025Dissonances"], "paper_id": "P0220", "evidence_id": "E-P0220-fae121f81b", "pointer": "papers/paper_notes.jsonl:paper_id=P0220#key_results[0]"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0040", "evidence_id": "E-P0040-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#limitations[1]"}, {"hook_type": "quant", "text": "This attack shows a nearly 80% success rate in an end-to-end evaluation.", "citations": ["Fu2024Imprompter"], "paper_id": "P0136", "evidence_id": "E-P0136-0ea7b39672", "pointer": "papers/paper_notes.jsonl:paper_id=P0136#key_results[0]"}], "comparison_cards": [{"axis": "tool interface (function calling, schemas, protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Lumer2025Memtool", "Mohammadi2025Evaluation", "Dong2025Bench"], "A_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}], "B_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0086", "evidence_id": "E-P0086-c522619cea", "excerpt": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'tool interface (function calling, schemas, protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Lumer2025Memtool", "Mohammadi2025Evaluation", "Dong2025Bench"], "A_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}], "B_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0086", "evidence_id": "E-P0086-c522619cea", "excerpt": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop, end-to-end tool orchestration by LLM agents in a hierarchical Model-Context Protocol (MCP) ecosystem.", "citations": ["Dong2025Bench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0086#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Mohammadi2025Evaluation", "Zhou2025Self", "Lumer2025Memtool", "Liu2025Toolscope"], "A_highlights": [{"paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}], "B_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0113", "evidence_id": "E-P0113-468a77ff1d", "excerpt": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["Liu2025Toolscope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "mechanism / architecture", "A_label": "Agent frameworks / architectures", "B_label": "Tool-use and function calling", "citations": ["Mohammadi2025Evaluation", "Zhou2025Self", "Lumer2025Memtool", "Liu2025Toolscope"], "A_highlights": [{"paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}], "B_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0113", "evidence_id": "E-P0113-468a77ff1d", "excerpt": "Evaluations on three state-of-the-art LLMs and three open-source tool-use benchmarks show gains of 8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's effectiveness in enhancing LLM tool use.", "citations": ["Liu2025Toolscope"], "pointer": "papers/paper_notes.jsonl:paper_id=P0113#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Tool-use and function calling along 'mechanism / architecture'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: API; GPT-5; HardGen; LLMs; ReAct; AutoTool; TauBench; TicToc; LLM-based; MCP.", "citations": ["Hao2026From", "Xuan2026Confidence", "Jia2025Autotool", "Zhou2025Self"]}], "limitation_hooks": [{"excerpt": "Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces.", "citations": ["Hao2026From"], "pointer": ""}, {"excerpt": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge.", "citations": ["Xuan2026Confidence"], "pointer": ""}, {"excerpt": "The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks.", "citations": ["Zhou2025Self"], "pointer": ""}, {"excerpt": "However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages.", "citations": ["Cheng2025Your"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 10, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
{"sub_id": "4.1", "title": "Planning and reasoning loops", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?", "thesis": "In Planning and reasoning loops, differences in control loop design (planner / executor, search) and deliberation method (CoT / ToT / MCTS) frequently imply different evaluation setups, so the key is to compare under consistent protocols where possible.", "tension_statement": "In Planning and reasoning loops, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "web/navigation tasks", "metric": "success rate", "constraint": "latency and budget"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["control loop design (planner / executor, search)", "deliberation method (CoT / ToT / MCTS)", "action grounding (tool calls vs environment actions)", "mechanism / architecture", "data / training setup"], "bridge_terms": ["planner/executor", "search", "deliberation", "action grounding"], "contrast_hook": "planning/control loop", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "training signal / supervision"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Planning / reasoning loops"], "rq": "Which design choices in Planning and reasoning loops drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Planning / reasoning loops", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Planning / reasoning loops as baseline", "use_clusters": ["Planning / reasoning loops"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Planning / reasoning loops", "data / training setup", "interface contract", "axes: control loop design (planner / executor, search), deliberation method (CoT / ToT / MCTS), action grounding (tool calls vs environment actions), mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Planning / reasoning loops"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Planning / reasoning loops"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Agent frameworks / architectures", "contrast with Planning / reasoning loops", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Agent frameworks / architectures vs Planning / reasoning loops", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: control loop design (planner / executor, search), deliberation method (CoT / ToT / MCTS), action grounding (tool calls vs environment actions), mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Planning / reasoning loops vs Agent frameworks / architectures", "multiple citations in one paragraph", "axes: control loop design (planner / executor, search), deliberation method (CoT / ToT / MCTS), action grounding (tool calls vs environment actions), mechanism / architecture, data / training setup"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Planning / reasoning loops vs Agent frameworks / architectures along control loop design (planner / executor, search), deliberation method (CoT / ToT / MCTS), action grounding (tool calls vs environment actions), mechanism / architecture, data / training setup", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Control / conditioning interfaces"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Control / conditioning interfaces"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Planning / reasoning loops", "Agent frameworks / architectures", "Control / conditioning interfaces"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: control loop design (planner / executor, search).", "Compare approaches along: deliberation method (CoT / ToT / MCTS).", "Compare approaches along: action grounding (tool calls vs environment actions).", "Compare approaches along: mechanism / architecture.", "Compare approaches along: data / training setup."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "clusters", "allowed_bibkeys_selected": ["Kim2025Bridging", "Shang2024Agentsquare", "Choi2025Reactree", "Yao2022React", "Hu2025Training", "Shi2024Ehragent", "Liu2025Costbench", "Nakano2025Guided", "Mudur2025Feabench", "Silva2025Agents", "Li2025Draft", "Zhou2025Reasoning", "Wu2025Agents"], "allowed_bibkeys_mapped": ["Hong2025Planning", "Hu2025Training", "Silva2025Agents", "Kim2025Bridging", "Zhou2025Reasoning", "Hatalis2025Review", "Nakano2025Guided", "Kiruluta2025Novel", "Liu2025Costbench", "Li2025Draft", "Shang2024Agentsquare", "Shi2024Ehragent", "Li2024Personal", "Mudur2025Feabench", "Choi2025Reactree", "Rosario2025Architecting", "Wu2025Agents", "Yao2022React"], "allowed_bibkeys_chapter": ["Choi2025Reactree", "Hatalis2025Review", "Hong2025Planning", "Hu2025Training", "Huang2025Retrieval", "Kim2025Bridging", "Kiruluta2025Novel", "Li2024Personal", "Li2025Agentswift", "Li2025Draft", "Li2025Graphcodeagent", "Liu2025Costbench", "Lumer2025Memtool", "Mudur2025Feabench", "Nakano2025Guided", "Rosario2025Architecting", "Shang2024Agentsquare", "Shi2024Ehragent", "Shi2025Progent", "Silva2025Agents", "Tao2026Membox", "Tawosi2025Meta", "Verma2026Active", "Wu2025Agents", "Wu2025Meta", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhou2025Reasoning", "Zhu2025Where"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0022-9d9d60644a", "E-P0121-38a26e4777", "E-P0098-c782275d8d", "E-P0001-ca4a00b5cf", "E-P0114-771620f84f", "E-P0129-2a7ea60588", "E-P0186-32d61c8fae", "E-P0023-741891e300", "E-P0023-99359acdd7", "E-P0078-e4ac5005b3", "E-P0065-b35b53de13", "E-P0188-837ee80fad", "E-P0012-e38b4bdff3", "E-P0065-baa622fa7f", "E-P0098-4bcafdb221", "E-P0181-e31a1bbba7", "E-P0186-17133c2d5e", "E-P0078-4334d22ac9"], "anchor_facts": [{"hook_type": "quant", "text": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["Nakano2025Guided"], "paper_id": "P0023", "evidence_id": "E-P0023-99359acdd7", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"hook_type": "quant", "text": "Our best performing strategy generates executable API calls 88% of the time.", "citations": ["Mudur2025Feabench"], "paper_id": "P0078", "evidence_id": "E-P0078-e4ac5005b3", "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}, {"hook_type": "quant", "text": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "paper_id": "P0023", "evidence_id": "E-P0023-741891e300", "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[1]"}, {"hook_type": "quant", "text": "Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs.", "citations": ["Shang2024Agentsquare"], "paper_id": "P0121", "evidence_id": "E-P0121-38a26e4777", "pointer": "papers/paper_notes.jsonl:paper_id=P0121#key_results[0]"}, {"hook_type": "limitation", "text": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks", "citations": ["Hu2025Training"], "paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}], "comparison_cards": [{"axis": "control loop design (planner / executor, search)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Nakano2025Guided", "Choi2025Reactree", "Hu2025Training"], "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-99359acdd7", "excerpt": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "excerpt": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}], "B_highlights": [{"paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "excerpt": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along 'control loop design (planner / executor, search)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "deliberation method (CoT / ToT / MCTS)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Nakano2025Guided", "Choi2025Reactree", "Hu2025Training"], "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-99359acdd7", "excerpt": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "excerpt": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}], "B_highlights": [{"paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "excerpt": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along 'deliberation method (CoT / ToT / MCTS)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "action grounding (tool calls vs environment actions)", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Nakano2025Guided", "Mudur2025Feabench", "Hu2025Training", "Choi2025Reactree"], "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-99359acdd7", "excerpt": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}, {"paper_id": "P0078", "evidence_id": "E-P0078-e4ac5005b3", "excerpt": "Our best performing strategy generates executable API calls 88% of the time.", "citations": ["Mudur2025Feabench"], "pointer": "papers/paper_notes.jsonl:paper_id=P0078#key_results[0]"}], "B_highlights": [{"paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "excerpt": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along 'action grounding (tool calls vs environment actions)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "mechanism / architecture", "A_label": "Planning / reasoning loops", "B_label": "Agent frameworks / architectures", "citations": ["Nakano2025Guided", "Hu2025Training", "Choi2025Reactree"], "A_highlights": [{"paper_id": "P0023", "evidence_id": "E-P0023-741891e300", "excerpt": "To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[1]"}, {"paper_id": "P0023", "evidence_id": "E-P0023-99359acdd7", "excerpt": "Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and required 86.2\\%, 118.7\\%, and 205.9\\% more model queries.", "citations": ["Nakano2025Guided"], "pointer": "papers/paper_notes.jsonl:paper_id=P0023#key_results[0]"}], "B_highlights": [{"paper_id": "P0114", "evidence_id": "E-P0114-771620f84f", "excerpt": "Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning", "citations": ["Hu2025Training"], "pointer": "papers/paper_notes.jsonl:paper_id=P0114#key_results[0]"}, {"paper_id": "P0098", "evidence_id": "E-P0098-c782275d8d", "excerpt": "To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree.", "citations": ["Choi2025Reactree"], "pointer": "papers/paper_notes.jsonl:paper_id=P0098#limitations[1]"}], "write_prompt": "Contrast Planning / reasoning loops vs Agent frameworks / architectures along 'mechanism / architecture'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: RSP; GSI; RSV; FEVER; RSP-M; HotpotQA; ReAct; SCL; CCAM; GPT-4o-powered.", "citations": ["Zhou2025Reasoning", "Kim2025Bridging", "Nakano2025Guided", "Hatalis2025Review"]}], "limitation_hooks": [{"excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures.", "citations": ["Nakano2025Guided"], "pointer": ""}, {"excerpt": "Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making.", "citations": ["Hatalis2025Review"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 10, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
{"sub_id": "4.2", "title": "Memory and retrieval (RAG)", "section_id": "4", "section_title": "Core Components (Planning + Memory)", "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Memory and retrieval (RAG) highlights a tension around memory type (episodic / semantic / scratchpad) and retrieval source + index (docs / web / logs), motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Memory and retrieval (RAG), the core tension is persistence versus freshness: retaining more context helps long-horizon tasks but raises staleness, contamination, and verification challenges.", "evaluation_anchor_minimal": {"task": "web/navigation tasks", "metric": "success rate", "constraint": "latency and budget"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["memory type (episodic / semantic / scratchpad)", "retrieval source + index (docs / web / logs)", "write / update / forgetting policy", "mechanism / architecture", "data / training setup"], "bridge_terms": ["retrieval", "index", "write policy", "long-term memory"], "contrast_hook": "memory/retrieval", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "training signal / supervision"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Memory and retrieval (RAG) drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Agent frameworks / architectures as baseline", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: memory type (episodic / semantic / scratchpad), retrieval source + index (docs / web / logs), write / update / forgetting policy, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Memory / retrieval augmentation", "contrast with Agent frameworks / architectures", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Memory / retrieval augmentation vs Agent frameworks / architectures", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Memory / retrieval augmentation", "data / training setup", "interface contract", "axes: memory type (episodic / semantic / scratchpad), retrieval source + index (docs / web / logs), write / update / forgetting policy, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Memory / retrieval augmentation", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Memory / retrieval augmentation"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Memory / retrieval augmentation", "multiple citations in one paragraph", "axes: memory type (episodic / semantic / scratchpad), retrieval source + index (docs / web / logs), write / update / forgetting policy, mechanism / architecture, data / training setup"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Agent frameworks / architectures vs Memory / retrieval augmentation along memory type (episodic / semantic / scratchpad), retrieval source + index (docs / web / logs), write / update / forgetting policy, mechanism / architecture, data / training setup", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Code agents / software tasks"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Code agents / software tasks"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Agent frameworks / architectures", "Memory / retrieval augmentation", "Code agents / software tasks"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: control loop design (planner / executor, search).", "Compare approaches along: deliberation method (CoT / ToT / MCTS).", "Compare approaches along: action grounding (tool calls vs environment actions).", "Compare approaches along: mechanism / architecture.", "Compare approaches along: data / training setup."], "chapter_key_contrasts": ["planning/control loop", "memory/retrieval"], "chapter_synthesis_mode": "clusters", "allowed_bibkeys_selected": ["Tawosi2025Meta", "Huang2025Retrieval", "Li2025Agentswift", "Yao2022React", "Shi2025Progent", "Lumer2025Memtool", "Verma2026Active", "Zhang2024Large", "Yu2026Agentic", "Zhang2025Large", "Ye2025Taska", "Li2025Graphcodeagent"], "allowed_bibkeys_mapped": ["Yu2026Agentic", "Huang2025Retrieval", "Zhang2025Large", "Wu2025Meta", "Tao2026Membox", "Tawosi2025Meta", "Ye2025Task", "Ye2025Taska", "Xu2025Agentic", "Li2025Graphcodeagent", "Zhang2024Large", "Verma2026Active", "Shi2025Progent", "Zhu2025Where", "Li2025Agentswift", "Lumer2025Memtool", "Hong2025Planning", "Yao2022React"], "allowed_bibkeys_chapter": ["Choi2025Reactree", "Hatalis2025Review", "Hong2025Planning", "Hu2025Training", "Huang2025Retrieval", "Kim2025Bridging", "Kiruluta2025Novel", "Li2024Personal", "Li2025Agentswift", "Li2025Draft", "Li2025Graphcodeagent", "Liu2025Costbench", "Lumer2025Memtool", "Mudur2025Feabench", "Nakano2025Guided", "Rosario2025Architecting", "Shang2024Agentsquare", "Shi2024Ehragent", "Shi2025Progent", "Silva2025Agents", "Tao2026Membox", "Tawosi2025Meta", "Verma2026Active", "Wu2025Agents", "Wu2025Meta", "Xu2025Agentic", "Yao2022React", "Ye2025Task", "Ye2025Taska", "Yu2026Agentic", "Zhang2024Large", "Zhang2025Large", "Zhou2025Reasoning", "Zhu2025Where"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0028-69289755a4", "E-P0028-f36b515991", "E-P0116-c9781caf3b", "E-P0058-904ba35500", "E-P0001-ca4a00b5cf", "E-P0029-68db58914f", "E-P0088-35271418ac", "E-P0116-4af0cf3c02", "E-P0146-9abcf1bf8a", "E-P0044-52fea1d199", "E-P0147-f0f0faaada", "E-P0082-79064ef6b3", "E-P0082-897bcc2f50", "E-P0112-53536132a8", "E-P0205-d568c53e1d", "E-P0028-e3ebb83eb2", "E-P0088-38dc800de9", "E-P0112-3bf4dab38c"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"hook_type": "quant", "text": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"hook_type": "quant", "text": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "paper_id": "P0028", "evidence_id": "E-P0028-f36b515991", "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"hook_type": "quant", "text": "SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability.", "citations": ["Huang2025Retrieval"], "paper_id": "P0116", "evidence_id": "E-P0116-c9781caf3b", "pointer": "papers/paper_notes.jsonl:paper_id=P0116#limitations[1]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed.", "citations": ["Shi2025Progent"], "paper_id": "P0029", "evidence_id": "E-P0029-68db58914f", "pointer": "papers/paper_notes.jsonl:paper_id=P0029#key_results[0]"}, {"hook_type": "quant", "text": "This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation.", "citations": ["Huang2025Retrieval"], "paper_id": "P0116", "evidence_id": "E-P0116-4af0cf3c02", "pointer": "papers/paper_notes.jsonl:paper_id=P0116#key_results[1]"}, {"hook_type": "quant", "text": "Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5.", "citations": ["Verma2026Active"], "paper_id": "P0146", "evidence_id": "E-P0146-9abcf1bf8a", "pointer": "papers/paper_notes.jsonl:paper_id=P0146#key_results[1]"}], "comparison_cards": [{"axis": "memory type (episodic / semantic / scratchpad)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Li2025Agentswift", "Lumer2025Memtool", "Tawosi2025Meta"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}], "B_highlights": [{"paper_id": "P0028", "evidence_id": "E-P0028-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}, {"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'memory type (episodic / semantic / scratchpad)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "retrieval source + index (docs / web / logs)", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Li2025Agentswift", "Lumer2025Memtool", "Tawosi2025Meta"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}], "B_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0028", "evidence_id": "E-P0028-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'retrieval source + index (docs / web / logs)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "write / update / forgetting policy", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Li2025Agentswift", "Lumer2025Memtool", "Tawosi2025Meta"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}], "B_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0028", "evidence_id": "E-P0028-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'write / update / forgetting policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "mechanism / architecture", "A_label": "Agent frameworks / architectures", "B_label": "Memory / retrieval augmentation", "citations": ["Li2025Agentswift", "Lumer2025Memtool", "Tawosi2025Meta"], "A_highlights": [{"paper_id": "P0058", "evidence_id": "E-P0058-904ba35500", "excerpt": "Evaluated across a comprehensive set of seven benchmarks spanning embodied, math, web, tool, and game domains, AgentSwift discovers agents that achieve an average performance gain of 8.34\\% over both existing automated agent search methods and manually designed agents.", "citations": ["Li2025Agentswift"], "pointer": "papers/paper_notes.jsonl:paper_id=P0058#key_results[0]"}, {"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}], "B_highlights": [{"paper_id": "P0088", "evidence_id": "E-P0088-35271418ac", "excerpt": "Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy.", "citations": ["Lumer2025Memtool"], "pointer": "papers/paper_notes.jsonl:paper_id=P0088#key_results[0]"}, {"paper_id": "P0028", "evidence_id": "E-P0028-f36b515991", "excerpt": "Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation.", "citations": ["Tawosi2025Meta"], "pointer": "papers/paper_notes.jsonl:paper_id=P0028#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Memory / retrieval augmentation along 'mechanism / architecture'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: SWE-bench; LTM; STM; GRPO; AgeMem; MEM; LoCoMo; ASB; LLMs; AgentDojo.", "citations": ["Verma2026Active", "Yu2026Agentic", "Tao2026Membox", "Shi2025Progent"]}], "limitation_hooks": [{"excerpt": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical.", "citations": ["Yu2026Agentic"], "pointer": ""}, {"excerpt": "Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments.", "citations": ["Shi2025Progent"], "pointer": ""}, {"excerpt": "Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones.", "citations": ["Shi2025Progent"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 9, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
{"sub_id": "5.1", "title": "Self-improvement and adaptation", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Self-improvement and adaptation methods emphasize training signal (SFT / preference / RL) and data synthesis + evaluator / reward trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "A central tension in Self-improvement and adaptation is the trade-off between training signal (SFT / preference / RL), data synthesis + evaluator / reward and what can be evaluated reliably under realistic constraints.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["training signal (SFT / preference / RL)", "data synthesis + evaluator / reward", "generalization + regression control", "mechanism / architecture", "data / training setup"], "bridge_terms": ["preference", "reward", "feedback", "self-improvement"], "contrast_hook": "learning/feedback", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "training signal / supervision"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Self-improvement and adaptation drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Agent frameworks / architectures as baseline", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: training signal (SFT / preference / RL), data synthesis + evaluator / reward, generalization + regression control, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Planning / reasoning loops", "contrast with Agent frameworks / architectures", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Planning / reasoning loops vs Agent frameworks / architectures", "use_clusters": ["Planning / reasoning loops"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Planning / reasoning loops", "data / training setup", "interface contract", "axes: training signal (SFT / preference / RL), data synthesis + evaluator / reward, generalization + regression control, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Planning / reasoning loops"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Planning / reasoning loops", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Planning / reasoning loops"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Planning / reasoning loops", "multiple citations in one paragraph", "axes: training signal (SFT / preference / RL), data synthesis + evaluator / reward, generalization + regression control, mechanism / architecture, data / training setup"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Agent frameworks / architectures vs Planning / reasoning loops along training signal (SFT / preference / RL), data synthesis + evaluator / reward, generalization + regression control, mechanism / architecture, data / training setup", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Agent frameworks / architectures", "Planning / reasoning loops", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: training signal (SFT / preference / RL).", "Compare approaches along: data synthesis + evaluator / reward.", "Compare approaches along: generalization + regression control.", "Compare approaches along: mechanism / architecture.", "Compare approaches along: data / training setup."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "clusters", "allowed_bibkeys_selected": ["Li2026Autonomous", "Zhao2025Achieving", "Du2024Anytool", "Zhou2025Self", "Yao2022React", "Zhang2026Evoroute", "Xia2025Sand", "Nitin2025Faultline", "Sarukkai2025Context", "Van2025Survey", "Wu2025Evolver"], "allowed_bibkeys_mapped": ["Xia2025Sand", "Sarukkai2025Context", "Zhou2025Self", "Chen2025Grounded", "Belle2025Agents", "Wu2025Evolver", "Zhou2024Archer", "Du2024Anytool", "Van2025Survey", "He2025Enabling", "Tennant2024Moral", "Zhao2025Achieving", "Nitin2025Faultline", "Zhang2025Agents", "Zhang2026Evoroute", "Li2026Autonomous", "Lidayan2025Abbel", "Yao2022React"], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chen2025Grounded", "Chuang2025Debate", "Du2024Anytool", "Einwiller2025Benevolent", "Hao2025Multi", "He2025Enabling", "Li2025Draft", "Li2025What", "Li2026Autonomous", "Lidayan2025Abbel", "Liu2025Aligning", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Shen2024Small", "Silva2025Agents", "Tennant2024Moral", "Van2025Survey", "Wang2023Voyager", "Warnakulasuriya2025Evolution", "Wu2024Federated", "Wu2025Agents", "Wu2025Evolver", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zahedifar2025Agent", "Zhang2025Agents", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0048-67ea29ce26", "E-P0165-1063eee7ce", "E-P0040-4da9e4ae32", "E-P0032-2e6956a116", "E-P0048-9980bf7642", "E-P0001-ca4a00b5cf", "E-P0040-d5c234444e", "E-P0150-60cc0d458f", "E-P0031-6273763a98", "E-P0048-499402e2fb", "E-P0079-74188ef933", "E-P0079-91a368737e", "E-P0210-cfef96abfa", "E-P0210-e7c6cee652", "E-P0032-17d0e7f9d9", "E-P0050-a68f39bc04", "E-P0050-e012f792c6", "E-P0077-1a05555e85"], "anchor_facts": [{"hook_type": "quant", "text": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"hook_type": "quant", "text": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"hook_type": "quant", "text": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}, {"hook_type": "quant", "text": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks.", "citations": ["Li2026Autonomous"], "paper_id": "P0048", "evidence_id": "E-P0048-67ea29ce26", "pointer": "papers/paper_notes.jsonl:paper_id=P0048#method"}, {"hook_type": "quant", "text": "However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation.", "citations": ["Zhao2025Achieving"], "paper_id": "P0165", "evidence_id": "E-P0165-1063eee7ce", "pointer": "papers/paper_notes.jsonl:paper_id=P0165#key_results[0]"}, {"hook_type": "eval", "text": "We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.", "citations": ["Du2024Anytool"], "paper_id": "P0040", "evidence_id": "E-P0040-4da9e4ae32", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#limitations[1]"}, {"hook_type": "quant", "text": "Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential.", "citations": ["Li2026Autonomous"], "paper_id": "P0048", "evidence_id": "E-P0048-9980bf7642", "pointer": "papers/paper_notes.jsonl:paper_id=P0048#key_results[1]"}, {"hook_type": "quant", "text": "Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.", "citations": ["Du2024Anytool"], "paper_id": "P0040", "evidence_id": "E-P0040-d5c234444e", "pointer": "papers/paper_notes.jsonl:paper_id=P0040#key_results[0]"}], "comparison_cards": [{"axis": "training signal (SFT / preference / RL)", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yao2022React"], "A_highlights": [{"paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'training signal (SFT / preference / RL)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "data synthesis + evaluator / reward", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhou2025Self", "Zhang2026Evoroute", "Yao2022React"], "A_highlights": [{"paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}, {"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'data synthesis + evaluator / reward'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "generalization + regression control", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Zhou2025Self", "Yao2022React"], "A_highlights": [{"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'generalization + regression control'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "mechanism / architecture", "A_label": "Agent frameworks / architectures", "B_label": "Planning / reasoning loops", "citations": ["Zhang2026Evoroute", "Zhou2025Self", "Yao2022React"], "A_highlights": [{"paper_id": "P0150", "evidence_id": "E-P0150-60cc0d458f", "excerpt": "Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.", "citations": ["Zhang2026Evoroute"], "pointer": "papers/paper_notes.jsonl:paper_id=P0150#key_results[0]"}, {"paper_id": "P0032", "evidence_id": "E-P0032-2e6956a116", "excerpt": "Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.", "citations": ["Zhou2025Self"], "pointer": "papers/paper_notes.jsonl:paper_id=P0032#key_results[0]"}], "B_highlights": [{"paper_id": "P0001", "evidence_id": "E-P0001-ca4a00b5cf", "excerpt": "On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.", "citations": ["Yao2022React"], "pointer": "papers/paper_notes.jsonl:paper_id=P0001#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Planning / reasoning loops along 'mechanism / architecture'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: DeepSeek-V3; LLMs; GAIA; EvoRoute; BrowseComp; SAND; ReAct-style; TauBench; FMs; MatSci.", "citations": ["Li2026Autonomous", "Zhang2026Evoroute", "Xia2025Sand", "Zhou2025Self"]}], "limitation_hooks": [{"excerpt": "Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.", "citations": ["Li2026Autonomous"], "pointer": ""}, {"excerpt": "We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion.", "citations": ["Zhang2026Evoroute"], "pointer": ""}, {"excerpt": "The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks.", "citations": ["Zhou2025Self"], "pointer": ""}, {"excerpt": "We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion.", "citations": ["Van2025Survey"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 9, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
{"sub_id": "5.2", "title": "Multi-agent coordination", "section_id": "5", "section_title": "Learning, Adaptation & Coordination", "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Multi-agent coordination highlights a tension around communication protocol + role assignment and aggregation (vote / debate / referee), motivating a protocol-aware synthesis rather than per-paper summaries.", "tension_statement": "In Multi-agent coordination, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "agent benchmark tasks", "metric": "success rate", "constraint": "budget/cost model"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["communication protocol + role assignment", "aggregation (vote / debate / referee)", "stability (collusion, mode collapse, incentives)", "mechanism / architecture", "data / training setup"], "bridge_terms": ["roles", "communication", "debate", "aggregation", "stability"], "contrast_hook": "coordination", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "training signal / supervision"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Multi-agent coordination drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Agent frameworks / architectures as baseline", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: communication protocol + role assignment, aggregation (vote / debate / referee), stability (collusion, mode collapse, incentives), mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Multi-agent coordination", "contrast with Agent frameworks / architectures", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Multi-agent coordination vs Agent frameworks / architectures", "use_clusters": ["Multi-agent coordination"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Multi-agent coordination", "data / training setup", "interface contract", "axes: communication protocol + role assignment, aggregation (vote / debate / referee), stability (collusion, mode collapse, incentives), mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Multi-agent coordination"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Multi-agent coordination", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Multi-agent coordination"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Multi-agent coordination", "multiple citations in one paragraph", "axes: communication protocol + role assignment, aggregation (vote / debate / referee), stability (collusion, mode collapse, incentives), mechanism / architecture, data / training setup"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Agent frameworks / architectures vs Multi-agent coordination along communication protocol + role assignment, aggregation (vote / debate / referee), stability (collusion, mode collapse, incentives), mechanism / architecture, data / training setup", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Agent frameworks / architectures", "Multi-agent coordination", "Planning / reasoning loops"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: training signal (SFT / preference / RL).", "Compare approaches along: data synthesis + evaluator / reward.", "Compare approaches along: generalization + regression control.", "Compare approaches along: mechanism / architecture.", "Compare approaches along: data / training setup."], "chapter_key_contrasts": ["learning/feedback", "coordination"], "chapter_synthesis_mode": "clusters", "allowed_bibkeys_selected": ["Chuang2025Debate", "Silva2025Agents", "Shen2024Small", "Wu2025Agents", "Zahedifar2025Agent", "Li2025Draft", "Li2025What", "Wang2023Voyager", "Cao2025Skyrl", "Wu2024Federated", "Zhou2024Archer"], "allowed_bibkeys_mapped": ["Papadakis2025Atlas", "Chuang2025Debate", "Zahedifar2025Agent", "Li2025Draft", "Cao2025Skyrl", "Wu2025Agents", "Shen2024Small", "Hao2025Multi", "Yim2024Evaluating", "Silva2025Agents", "Sarkar2025Survey", "Wu2024Federated", "Zhou2024Archer", "Li2025What", "Liu2025Aligning", "Einwiller2025Benevolent", "Warnakulasuriya2025Evolution", "Wang2023Voyager"], "allowed_bibkeys_chapter": ["Belle2025Agents", "Cao2025Skyrl", "Chen2025Grounded", "Chuang2025Debate", "Du2024Anytool", "Einwiller2025Benevolent", "Hao2025Multi", "He2025Enabling", "Li2025Draft", "Li2025What", "Li2026Autonomous", "Lidayan2025Abbel", "Liu2025Aligning", "Nitin2025Faultline", "Papadakis2025Atlas", "Sarkar2025Survey", "Sarukkai2025Context", "Shen2024Small", "Silva2025Agents", "Tennant2024Moral", "Van2025Survey", "Wang2023Voyager", "Warnakulasuriya2025Evolution", "Wu2024Federated", "Wu2025Agents", "Wu2025Evolver", "Xia2025Sand", "Yao2022React", "Yim2024Evaluating", "Zahedifar2025Agent", "Zhang2025Agents", "Zhang2026Evoroute", "Zhao2025Achieving", "Zhou2024Archer", "Zhou2025Self"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0187-e372fc0508", "E-P0065-baa622fa7f", "E-P0142-c92ed293ba", "E-P0181-e31a1bbba7", "E-P0065-b35b53de13", "E-P0142-9640816b42", "E-P0217-0f34c44fa9", "E-P0142-8158d9909c", "E-P0188-837ee80fad", "E-P0118-ccc7d0cc50", "E-P0005-506120a6cd", "E-P0106-5ed988eb67", "E-P0135-76aa5df0e2", "E-P0187-3acffd03b4", "E-P0005-b2eceb5b30", "E-P0041-faa3d4c9ee", "E-P0106-3af1ce8090", "E-P0217-ebcb5ebf6c"], "anchor_facts": [{"hook_type": "quant", "text": "Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios.", "citations": ["Li2025What"], "paper_id": "P0118", "evidence_id": "E-P0118-ccc7d0cc50", "pointer": "papers/paper_notes.jsonl:paper_id=P0118#key_results[0]"}, {"hook_type": "eval", "text": "To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs.", "citations": ["Chuang2025Debate"], "paper_id": "P0187", "evidence_id": "E-P0187-e372fc0508", "pointer": "papers/paper_notes.jsonl:paper_id=P0187#method"}, {"hook_type": "limitation", "text": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "paper_id": "P0065", "evidence_id": "E-P0065-baa622fa7f", "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[1]"}, {"hook_type": "eval", "text": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "paper_id": "P0181", "evidence_id": "E-P0181-e31a1bbba7", "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}, {"hook_type": "eval", "text": "Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.", "citations": ["Shen2024Small"], "paper_id": "P0142", "evidence_id": "E-P0142-9640816b42", "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[1]"}, {"hook_type": "eval", "text": "First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task.", "citations": ["Shen2024Small"], "paper_id": "P0142", "evidence_id": "E-P0142-8158d9909c", "pointer": "papers/paper_notes.jsonl:paper_id=P0142#key_results[0]"}], "comparison_cards": [{"axis": "communication protocol + role assignment", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Li2025What", "Silva2025Agents", "Li2025Draft", "Wu2025Agents"], "A_highlights": [{"paper_id": "P0118", "evidence_id": "E-P0118-ccc7d0cc50", "excerpt": "Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios.", "citations": ["Li2025What"], "pointer": "papers/paper_notes.jsonl:paper_id=P0118#key_results[0]"}, {"paper_id": "P0065", "evidence_id": "E-P0065-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[1]"}], "B_highlights": [{"paper_id": "P0188", "evidence_id": "E-P0188-837ee80fad", "excerpt": "We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "citations": ["Li2025Draft"], "pointer": "papers/paper_notes.jsonl:paper_id=P0188#key_results[0]"}, {"paper_id": "P0181", "evidence_id": "E-P0181-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'communication protocol + role assignment'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "aggregation (vote / debate / referee)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Li2025What", "Silva2025Agents", "Li2025Draft", "Wu2025Agents"], "A_highlights": [{"paper_id": "P0118", "evidence_id": "E-P0118-ccc7d0cc50", "excerpt": "Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios.", "citations": ["Li2025What"], "pointer": "papers/paper_notes.jsonl:paper_id=P0118#key_results[0]"}, {"paper_id": "P0065", "evidence_id": "E-P0065-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[1]"}], "B_highlights": [{"paper_id": "P0188", "evidence_id": "E-P0188-837ee80fad", "excerpt": "We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "citations": ["Li2025Draft"], "pointer": "papers/paper_notes.jsonl:paper_id=P0188#key_results[0]"}, {"paper_id": "P0181", "evidence_id": "E-P0181-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'aggregation (vote / debate / referee)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "stability (collusion, mode collapse, incentives)", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Li2025What", "Silva2025Agents", "Li2025Draft", "Wu2025Agents"], "A_highlights": [{"paper_id": "P0118", "evidence_id": "E-P0118-ccc7d0cc50", "excerpt": "Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios.", "citations": ["Li2025What"], "pointer": "papers/paper_notes.jsonl:paper_id=P0118#key_results[0]"}, {"paper_id": "P0065", "evidence_id": "E-P0065-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[1]"}], "B_highlights": [{"paper_id": "P0188", "evidence_id": "E-P0188-837ee80fad", "excerpt": "We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "citations": ["Li2025Draft"], "pointer": "papers/paper_notes.jsonl:paper_id=P0188#key_results[0]"}, {"paper_id": "P0181", "evidence_id": "E-P0181-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'stability (collusion, mode collapse, incentives)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "mechanism / architecture", "A_label": "Agent frameworks / architectures", "B_label": "Multi-agent coordination", "citations": ["Li2025What", "Silva2025Agents", "Li2025Draft", "Wu2025Agents"], "A_highlights": [{"paper_id": "P0118", "evidence_id": "E-P0118-ccc7d0cc50", "excerpt": "Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios.", "citations": ["Li2025What"], "pointer": "papers/paper_notes.jsonl:paper_id=P0118#key_results[0]"}, {"paper_id": "P0065", "evidence_id": "E-P0065-baa622fa7f", "excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0065#key_results[1]"}], "B_highlights": [{"paper_id": "P0188", "evidence_id": "E-P0188-837ee80fad", "excerpt": "We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed", "citations": ["Li2025Draft"], "pointer": "papers/paper_notes.jsonl:paper_id=P0188#key_results[0]"}, {"paper_id": "P0181", "evidence_id": "E-P0181-e31a1bbba7", "excerpt": "We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth.", "citations": ["Wu2025Agents"], "pointer": "papers/paper_notes.jsonl:paper_id=P0181#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Multi-agent coordination along 'mechanism / architecture'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: MCP; LLM-based; MCP-compliant; LLMs; SA-SWE-32B; AST-based; SWE-Bench; SWE; SkyRL-Agent; SkyRL-train.", "citations": ["Sarkar2025Survey", "Silva2025Agents", "Cao2025Skyrl", "Li2025What"]}], "limitation_hooks": [{"excerpt": "The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems.", "citations": ["Sarkar2025Survey"], "pointer": ""}, {"excerpt": "This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.", "citations": ["Silva2025Agents"], "pointer": ""}, {"excerpt": "Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.", "citations": ["Papadakis2025Atlas"], "pointer": ""}, {"excerpt": "To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to", "citations": ["Einwiller2025Benevolent"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 6, "considered": 6, "kept": 6, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
{"sub_id": "6.1", "title": "Benchmarks and evaluation protocols", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Benchmarks and evaluation protocols methods emphasize tool interface (function calling, schemas, protocols) and tool selection / routing policy trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Benchmarks and evaluation protocols, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "code tasks", "metric": "test pass rate / success", "constraint": "sandbox and budget"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "tension-first", "opener_hint": "Start with the subsectionâ€™s central tension/trade-off; end paragraph 1 with the thesis.", "axes": ["tool interface (function calling, schemas, protocols)", "tool selection / routing policy", "sandboxing / permissions / observability", "task suites (web / code / embodied / tools)", "metrics (success, cost, reliability, safety)"], "bridge_terms": ["function calling", "tool schema", "routing", "sandbox", "observability", "benchmarks"], "contrast_hook": "tool interfaces", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "compute / cost (train/infer)", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Benchmarks and evaluation protocols drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Agent frameworks / architectures as baseline", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, task suites (web / code / embodied / tools), metrics (success, cost, reliability, safety)"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Evaluation / benchmark-focused works", "contrast with Agent frameworks / architectures", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Evaluation / benchmark-focused works vs Agent frameworks / architectures", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Evaluation / benchmark-focused works", "data / training setup", "interface contract", "axes: tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, task suites (web / code / embodied / tools), metrics (success, cost, reliability, safety)"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Evaluation / benchmark-focused works", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Evaluation / benchmark-focused works"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Evaluation / benchmark-focused works", "multiple citations in one paragraph", "axes: tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, task suites (web / code / embodied / tools), metrics (success, cost, reliability, safety)"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Agent frameworks / architectures vs Evaluation / benchmark-focused works along tool interface (function calling, schemas, protocols), tool selection / routing policy, sandboxing / permissions / observability, task suites (web / code / embodied / tools), metrics (success, cost, reliability, safety)", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Safety / security / guardrails"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Safety / security / guardrails"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Agent frameworks / architectures", "Evaluation / benchmark-focused works", "Safety / security / guardrails"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: tool interface (function calling, schemas, protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability.", "Compare approaches along: task suites (web / code / embodied / tools).", "Compare approaches along: metrics (success, cost, reliability, safety)."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhang2025Security", "Plaat2025Agentic", "Fu2025Eval", "Kale2025Reliable", "Mohammadi2025Evaluation", "Yang2025Contextagent", "Kim2026Beyond", "Zhang2025Agents", "Van2025Survey", "Zhang2025Datascibench", "Chen2025Towards", "Zhang2024Agent", "Liu2026Agents"], "allowed_bibkeys_mapped": ["Mohammadi2025Evaluation", "Chen2025Towards", "Fu2025Eval", "Kim2026Beyond", "Ji2025Taxonomy", "Zhang2025Buildbench", "Van2025Survey", "Zhang2025Security", "Kale2025Reliable", "Zhang2024Agent", "Zhang2025Datascibench", "Zhang2025Agents", "Liu2026Agents", "Plaat2025Agentic", "Hadeliya2025When", "Agrawal2025Language", "Yang2025Contextagent", "Wang2023Survey"], "allowed_bibkeys_chapter": ["Agrawal2025Language", "An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Fang2025Should", "Fu2025Eval", "Gasmi2025Bridging", "Hadeliya2025When", "Hahm2025Enhancing", "Ji2025Taxonomy", "Kale2025Reliable", "Kamath2025Enforcing", "Kim2026Beyond", "Lichkovski2025Agent", "Liu2026Agents", "Luo2025Agrail", "Mohammadi2025Evaluation", "Plaat2025Agentic", "Sha2025Agent", "Van2025Survey", "Wang2023Survey", "Wang2025Adversarial", "Yang2025Contextagent", "Zhang2024Agent", "Zhang2025Agents", "Zhang2025Buildbench", "Zhang2025Datascibench", "Zhang2025Security", "Zhong2025Impossiblebench", "Zhou2025Reasoning"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0083-8bcb673a7d", "E-P0083-d6095e10e9", "E-P0006-e688e4a34e", "E-P0096-753416ce70", "E-P0099-e0345118bc", "E-P0083-7a6ec4daed", "E-P0076-37f9ea924c", "E-P0096-2895472ae1", "E-P0069-0fc82f8849", "E-P0013-79f88927fa", "E-P0214-cec7a6da6c", "E-P0050-a68f39bc04", "E-P0050-e012f792c6", "E-P0189-39281e5083", "E-P0035-4fc221fdea", "E-P0120-e26328e18c", "E-P0154-8b56718f74", "E-P0120-51b52c66a1"], "anchor_facts": [{"hook_type": "quant", "text": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "paper_id": "P0013", "evidence_id": "E-P0013-79f88927fa", "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#method"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "limitation", "text": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "paper_id": "P0006", "evidence_id": "E-P0006-e688e4a34e", "pointer": "papers/paper_notes.jsonl:paper_id=P0006#limitations[1]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["Kale2025Reliable"], "paper_id": "P0099", "evidence_id": "E-P0099-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[0]"}], "comparison_cards": [{"axis": "tool interface (function calling, schemas, protocols)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Kim2026Beyond", "Plaat2025Agentic", "Fu2025Eval", "Zhang2025Security"], "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"paper_id": "P0006", "evidence_id": "E-P0006-e688e4a34e", "excerpt": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0006#limitations[1]"}], "B_highlights": [{"paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}, {"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'tool interface (function calling, schemas, protocols)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "tool selection / routing policy", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Kim2026Beyond", "Plaat2025Agentic", "Fu2025Eval", "Zhang2025Security"], "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"paper_id": "P0006", "evidence_id": "E-P0006-e688e4a34e", "excerpt": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0006#limitations[1]"}], "B_highlights": [{"paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}, {"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'tool selection / routing policy'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "sandboxing / permissions / observability", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Plaat2025Agentic", "Kim2026Beyond", "Zhang2025Security", "Mohammadi2025Evaluation"], "A_highlights": [{"paper_id": "P0006", "evidence_id": "E-P0006-e688e4a34e", "excerpt": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0006#limitations[1]"}, {"paper_id": "P0013", "evidence_id": "E-P0013-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0076", "evidence_id": "E-P0076-37f9ea924c", "excerpt": "This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and", "citations": ["Mohammadi2025Evaluation"], "pointer": "papers/paper_notes.jsonl:paper_id=P0076#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'sandboxing / permissions / observability'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "task suites (web / code / embodied / tools)", "A_label": "Agent frameworks / architectures", "B_label": "Evaluation / benchmark-focused works", "citations": ["Kim2026Beyond", "Plaat2025Agentic", "Fu2025Eval", "Zhang2025Security"], "A_highlights": [{"paper_id": "P0013", "evidence_id": "E-P0013-79f88927fa", "excerpt": "Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1.", "citations": ["Kim2026Beyond"], "pointer": "papers/paper_notes.jsonl:paper_id=P0013#key_results[0]"}, {"paper_id": "P0006", "evidence_id": "E-P0006-e688e4a34e", "excerpt": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "pointer": "papers/paper_notes.jsonl:paper_id=P0006#limitations[1]"}], "B_highlights": [{"paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}, {"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Evaluation / benchmark-focused works along 'task suites (web / code / embodied / tools)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: API; LLMs; WildAGTEval; IPI; IPI-centric; RPA; RPAs; LLM-based; GPT-4; FMs.", "citations": ["Kim2026Beyond", "Liu2026Agents", "Plaat2025Agentic", "Ji2025Taxonomy"]}], "limitation_hooks": [{"excerpt": "Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability.", "citations": ["Liu2026Agents"], "pointer": ""}, {"excerpt": "We note that there is risk associated with LLM assistants taking action in the real world-safety, liability and security are open problems-while agentic LLMs are also likely to benefit society.", "citations": ["Plaat2025Agentic"], "pointer": ""}, {"excerpt": "We then thoroughly assess the security and usability of representative defense frameworks.", "citations": ["Ji2025Taxonomy"], "pointer": ""}, {"excerpt": "Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses.", "citations": ["Ji2025Taxonomy"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 12, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
{"sub_id": "6.2", "title": "Safety, security, and governance", "section_id": "6", "section_title": "Evaluation & Risks", "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?", "thesis": "Safety, security, and governance methods emphasize threat model (prompt / tool injection, exfiltration) and defense surface (policy, sandbox, monitoring) trade-offs, but synthesis is clearest when claims are tied to explicit evaluation settings and reporting conventions.", "tension_statement": "In Safety, security, and governance, a practical tension is expressivity versus control: richer interfaces expand capability but make behavior harder to constrain and verify.", "evaluation_anchor_minimal": {"task": "attack/defense evaluation", "metric": "attack success rate", "constraint": "policy/sandbox setting"}, "paper_voice_palette": {"opener_archetypes": {"tension-first": ["A key tension is", "The central trade-off is", "A recurring constraint is"], "decision-first": ["For system builders, the crux is", "A practical decision is", "One design choice is"], "lens-first": ["Seen through the lens of", "From the perspective of", "Under an interface contract,"]}, "synthesis_stems": ["Across these studies,", "Collectively,", "In summary,", "The evidence suggests that", "A consistent theme is that"], "rewrite_rules": [{"avoid_stem": "This subsection surveys", "prefer_stem": "A key tension is"}, {"avoid_stem": "In this subsection", "prefer_stem": "We focus on"}, {"avoid_stem": "Next, we move", "prefer_stem": "Having established"}, {"avoid_stem": "We now turn", "prefer_stem": "We then examine"}, {"avoid_stem": "survey comparisons should", "prefer_stem": "Across protocols, we observe"}]}, "opener_mode": "lens-first", "opener_hint": "Start by naming the lens (interface/protocol/threat model); state what it reveals; end paragraph 1 with the thesis.", "axes": ["threat model (prompt / tool injection, exfiltration)", "defense surface (policy, sandbox, monitoring)", "security evaluation protocol", "mechanism / architecture", "data / training setup"], "bridge_terms": ["threat model", "prompt/tool injection", "monitoring", "guardrails"], "contrast_hook": "security", "required_evidence_fields": ["benchmarks/datasets", "metrics / human-eval protocol", "training signal / supervision", "threat model", "defense surface"], "paragraph_plan": [{"para": 1, "argument_role": "setup_thesis", "intent": "Define scope, setup, and the subsection thesis (no pipeline jargon).", "focus": ["scope boundary", "key definitions", "thesis vs neighboring subsections"], "connector_to_prev": "", "connector_phrase": "", "use_clusters": ["Agent frameworks / architectures"], "rq": "Which design choices in Safety, security, and governance drive the major trade-offs, and how are those trade-offs measured?"}, {"para": 2, "argument_role": "mechanism_cluster_A", "intent": "Explain cluster A: core mechanism/architecture and what decision it makes in the agent loop.", "focus": ["cluster: Agent frameworks / architectures", "mechanism / architecture", "assumptions"], "connector_to_prev": "grounding", "connector_phrase": "Grounding: Agent frameworks / architectures as baseline", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 3, "argument_role": "implementation_cluster_A", "intent": "Cluster A implementation details: data/training signals and interface contract (tools/memory) that constrain behavior.", "focus": ["cluster: Agent frameworks / architectures", "data / training setup", "interface contract", "axes: threat model (prompt / tool injection, exfiltration), defense surface (policy, sandbox, monitoring), security evaluation protocol, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Elaboration: interface + training assumptions", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 4, "argument_role": "evaluation_cluster_A", "intent": "Cluster A evaluation/trade-offs: where it works, costs (compute/latency), and typical failure modes.", "focus": ["cluster: Agent frameworks / architectures", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation lens: protocol + failure modes", "use_clusters": ["Agent frameworks / architectures"]}, {"para": 5, "argument_role": "contrast_cluster_B", "intent": "Explain cluster B (contrast with A): core mechanism/architecture and what it optimizes for.", "focus": ["cluster: Safety / security / guardrails", "contrast with Agent frameworks / architectures", "mechanism / architecture"], "connector_to_prev": "contrast", "connector_phrase": "Contrast: Safety / security / guardrails vs Agent frameworks / architectures", "use_clusters": ["Safety / security / guardrails"]}, {"para": 6, "argument_role": "implementation_cluster_B", "intent": "Cluster B implementation details: data/training and interface assumptions (mirror A for comparability).", "focus": ["cluster: Safety / security / guardrails", "data / training setup", "interface contract", "axes: threat model (prompt / tool injection, exfiltration), defense surface (policy, sandbox, monitoring), security evaluation protocol, mechanism / architecture, data / training setup"], "connector_to_prev": "elaboration", "connector_phrase": "Implementation contrast: assumptions shift", "use_clusters": ["Safety / security / guardrails"]}, {"para": 7, "argument_role": "evaluation_cluster_B", "intent": "Cluster B evaluation/trade-offs: where it works, costs, and failure modes (mirror A).", "focus": ["cluster: Safety / security / guardrails", "evaluation anchor", "efficiency", "failure modes"], "connector_to_prev": "evaluation", "connector_phrase": "Evaluation contrast: apples-to-apples where possible", "use_clusters": ["Safety / security / guardrails"]}, {"para": 8, "argument_role": "cross_paper_synthesis", "intent": "Cross-paper synthesis: compare clusters along the main axes (include >=2 citations in one paragraph).", "focus": ["compare Agent frameworks / architectures vs Safety / security / guardrails", "multiple citations in one paragraph", "axes: threat model (prompt / tool injection, exfiltration), defense surface (policy, sandbox, monitoring), security evaluation protocol, mechanism / architecture, data / training setup"], "connector_to_prev": "synthesis", "connector_phrase": "Synthesis: Agent frameworks / architectures vs Safety / security / guardrails along threat model (prompt / tool injection, exfiltration), defense surface (policy, sandbox, monitoring), security evaluation protocol, mechanism / architecture, data / training setup", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 9, "argument_role": "decision_guidance", "intent": "Decision guidance: when to choose which route (criteria + evaluation signals + engineering constraints).", "focus": ["decision checklist", "evaluation protocol", "practical constraints"], "connector_to_prev": "consequence", "connector_phrase": "Implication: decision checklist", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"]}, {"para": 10, "argument_role": "limitations_open_questions", "intent": "Limitations + verification targets; end with a concrete open question to hand off.", "focus": ["limitations", "evidence mode: provisional", "what needs verification", "open question"], "connector_to_prev": "limitations", "connector_phrase": "Limitations: verification targets", "use_clusters": ["Agent frameworks / architectures", "Safety / security / guardrails", "Tool-use and function calling"], "policy": "Use conservative language; avoid strong conclusions; prefer questions-to-answer + explicit evidence gaps list."}], "chapter_throughline": ["Pin scope to goal: agent survey (LLM agents) LaTeX survey.", "Compare approaches along: tool interface (function calling, schemas, protocols).", "Compare approaches along: tool selection / routing policy.", "Compare approaches along: sandboxing / permissions / observability.", "Compare approaches along: task suites (web / code / embodied / tools).", "Compare approaches along: metrics (success, cost, reliability, safety)."], "chapter_key_contrasts": ["tool interfaces", "security"], "chapter_synthesis_mode": "tradeoff_matrix", "allowed_bibkeys_selected": ["Zhang2025Security", "Lichkovski2025Agent", "Kale2025Reliable", "Fu2025Eval", "Bonagiri2025Check", "Wang2025Adversarial", "Gasmi2025Bridging", "Zhang2024Agent", "Kamath2025Enforcing", "Zhong2025Impossiblebench", "Fang2025Should"], "allowed_bibkeys_mapped": ["Bonagiri2025Check", "Gasmi2025Bridging", "Zhang2025Security", "Fang2025Should", "Zhang2024Agent", "Wang2025Adversarial", "Luo2025Agrail", "Sha2025Agent", "An2025Ipiguard", "Hadeliya2025When", "Fu2025Eval", "Hahm2025Enhancing", "Kale2025Reliable", "Lichkovski2025Agent", "Kamath2025Enforcing", "Zhong2025Impossiblebench", "Zhou2025Reasoning", "Wang2023Survey"], "allowed_bibkeys_chapter": ["Agrawal2025Language", "An2025Ipiguard", "Bonagiri2025Check", "Chen2025Towards", "Fang2025Should", "Fu2025Eval", "Gasmi2025Bridging", "Hadeliya2025When", "Hahm2025Enhancing", "Ji2025Taxonomy", "Kale2025Reliable", "Kamath2025Enforcing", "Kim2026Beyond", "Lichkovski2025Agent", "Liu2026Agents", "Luo2025Agrail", "Mohammadi2025Evaluation", "Plaat2025Agentic", "Sha2025Agent", "Van2025Survey", "Wang2023Survey", "Wang2025Adversarial", "Yang2025Contextagent", "Zhang2024Agent", "Zhang2025Agents", "Zhang2025Buildbench", "Zhang2025Datascibench", "Zhang2025Security", "Zhong2025Impossiblebench", "Zhou2025Reasoning"], "allowed_bibkeys_global": ["Yao2022React"], "evidence_ids": ["E-P0083-8bcb673a7d", "E-P0083-d6095e10e9", "E-P0193-a256400826", "E-P0099-e0345118bc", "E-P0096-753416ce70", "E-P0083-7a6ec4daed", "E-P0096-2895472ae1", "E-P0067-7edb91824f", "E-P0166-ee6f97d5e5", "E-P0166-8512d7eecf", "E-P0021-8e34a29629", "E-P0120-e26328e18c", "E-P0120-51b52c66a1", "E-P0194-2718dbc45e", "E-P0208-60c18f38e8", "E-P0021-8a2c2e2291", "E-P0117-3cd6217549", "E-P0117-dcb97bbb10"], "anchor_facts": [{"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}, {"hook_type": "quant", "text": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "paper_id": "P0096", "evidence_id": "E-P0096-2895472ae1", "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}, {"hook_type": "eval", "text": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-8bcb673a7d", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#method"}, {"hook_type": "quant", "text": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed a", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"hook_type": "quant", "text": "To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Ar", "citations": ["Kale2025Reliable"], "paper_id": "P0099", "evidence_id": "E-P0099-e0345118bc", "pointer": "papers/paper_notes.jsonl:paper_id=P0099#key_results[0]"}, {"hook_type": "quant", "text": "We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances.", "citations": ["Zhang2025Security"], "paper_id": "P0083", "evidence_id": "E-P0083-7a6ec4daed", "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[1]"}, {"hook_type": "quant", "text": "Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs.", "citations": ["Bonagiri2025Check"], "paper_id": "P0067", "evidence_id": "E-P0067-7edb91824f", "pointer": "papers/paper_notes.jsonl:paper_id=P0067#key_results[0]"}], "comparison_cards": [{"axis": "threat model (prompt / tool injection, exfiltration)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security", "Fu2025Eval"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'threat model (prompt / tool injection, exfiltration)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "defense surface (policy, sandbox, monitoring)", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0083", "evidence_id": "E-P0083-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#method"}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0083", "evidence_id": "E-P0083-8bcb673a7d", "excerpt": "We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling.", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#method"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'defense surface (policy, sandbox, monitoring)'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "security evaluation protocol", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security", "Fu2025Eval"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-753416ce70", "excerpt": "RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[1]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'security evaluation protocol'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}, {"axis": "mechanism / architecture", "A_label": "Agent frameworks / architectures", "B_label": "Safety / security / guardrails", "citations": ["Zhang2025Security", "Fu2025Eval"], "A_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-2895472ae1", "excerpt": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}], "B_highlights": [{"paper_id": "P0083", "evidence_id": "E-P0083-d6095e10e9", "excerpt": "MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed", "citations": ["Zhang2025Security"], "pointer": "papers/paper_notes.jsonl:paper_id=P0083#key_results[0]"}, {"paper_id": "P0096", "evidence_id": "E-P0096-2895472ae1", "excerpt": "We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings.", "citations": ["Fu2025Eval"], "pointer": "papers/paper_notes.jsonl:paper_id=P0096#key_results[0]"}], "write_prompt": "Contrast Agent frameworks / architectures vs Safety / security / guardrails along 'mechanism / architecture'. Ground A and B using the highlight snippets; do not introduce new claims beyond the cited evidence."}], "evaluation_protocol": [{"bullet": "Evaluation tokens mentioned in mapped evidence: RSP; GSI; RSV; FEVER; RSP-M; HotpotQA; ReAct; AI-specific; MCP; JSON.", "citations": ["Zhou2025Reasoning", "Gasmi2025Bridging", "Hadeliya2025When", "Luo2025Agrail"]}], "limitation_hooks": [{"excerpt": "While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers.", "citations": ["Zhou2025Reasoning"], "pointer": ""}, {"excerpt": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately.", "citations": ["Gasmi2025Bridging"], "pointer": ""}, {"excerpt": "This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework.", "citations": ["Gasmi2025Bridging"], "pointer": ""}], "must_use": {"min_anchor_facts": 2, "min_comparison_cards": 2, "min_limitation_hooks": 1, "require_cited_numeric_if_available": true, "require_multi_cite_synthesis_paragraph": true, "thesis_required": true}, "do_not_repeat_phrases": ["This subsection surveys", "This subsection argues", "In this subsection", "Next, we move from", "We now turn to", "this run", "this run is", "Method note (evidence policy):", "A few representative references include", "Notable lines of work include", "Concrete examples include", "Concrete examples in", "Concrete examples for", "Work in this area includes", "Recent systems for", "Examples that illustrate", "Representative systems for", "Concrete systems include", "Recent systems include", "Examples include", "Representative systems include", "Therefore, survey synthesis should", "Therefore, survey comparisons should", "As a result, survey comparisons should", "survey synthesis should", "survey comparisons should", "Taken together,", "claims remain provisional under abstract-only evidence", "abstract-only evidence", "Key takeaway:", "Main takeaway:"], "pack_warnings": [], "pack_stats": {"anchors": {"raw": 8, "considered": 8, "kept": 8, "dropped_no_cites": 0}, "comparisons": {"raw": 5, "considered": 4, "kept": 4, "dropped_no_highlights": 0, "highlights_dropped_no_cites": 0}, "evaluation_protocol": {"raw": 1, "considered": 1, "kept": 1, "dropped_no_cites": 0}, "limitation_hooks": {"raw": 4, "considered": 4, "kept": 4, "dropped_no_cites": 0}, "trim_policy": {"default": 400, "anchor_fact": 420, "highlight_excerpt": 280, "comparison_write_prompt": 420, "eval_bullet": 320, "limitation_excerpt": 320}}, "generated_at": "2026-01-19T12:09:29"}
